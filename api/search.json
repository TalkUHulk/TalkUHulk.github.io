[{"id":"2d84f3892209cec11720cffbf464a897","title":"CV面试基础总结","content":"1. 评测指标\n1.1 基本概念\n\nTP TN FP FN\n\n\nT-Ture;F-False 表示预测结果的正确性，T表示预测正确，F表示预测错误； P-positive;N-negative 表示预测的正负性，P表示预测为正样本，N表示预测为负样本；\n\n\n\n---\n---\n\n\n\n\nTP——True Positive\n真正例，表示样本为正，预测值为正——预测正确T\n\n\nFP——False Positive\n假正例，表示样本为负，预测值为正——预测错误F\n\n\nFN——False Negative\n假负例，表示样本为正，预测值为负——预测错误F\n\n\nTN——True Negative\n真负例，表示样本为负，预测值为负——预测正确T\n\n\n\n\nAccuracy Precision Recall\n\n\nAccuracy:\n\n用于判定预测结果的准确程度，即预测正确的总数/样本总数。 预测正确又分为两种情况：样本为正、预测为正和样本为负、预测为负，即TP+TN。\n\n\nPrecision:\n\n精确率用于描述在所有预测为正的样本中，预测正确的比例,在有的翻译中，Precision也被称为查准率。 查准率针对的是预测，顾名思义，查准率主要用来判断“查的准不准”，其依据是查出的正例占所有正例预测的比率。\n\n\nRecall:\n\n和精确率不同，Precison针对的是预测，Recall针对的是样本。召回率表示在所有正例的样本中，有多少被预测/检测了出来，所以在有的翻译中，Recall也被称为查全率。 查全率针对的是样本，即对样本而言，有多少比例的正例样本在预测中被检出。\n\n\n\n\n\n\n\n\n\n\n最优目标是希望查准率和查全率都接近100%，但通常这二者的关系是负相关.\n\nF1-score\n\nF1 score是围绕Recall和Precision衍生出来的一个参考值，公式 = Precision和Recall的调和平均值\n\n\n\n\n\n\n\n\n\n\n其数值大小通常接近二者中的较小数、当recall = precision时，F1 = recall = precision，如果F1值较高，说明recall和precision都较高,我们希望取到较高的F1值。\n\nPR曲线\n\nPrecision-Recall curve，即PR曲线，是在Precision查准率和Recall查全率概念上衍生出的曲线，X轴为Recall，Y轴为Precision。如下图A,B,C三个模型，绘制出的PR曲线：\n\n\n\n\n\n\n\n\n\n\n我们可以通过PR曲线，找到最优化的模型。比较上图A和C模型，很容易比较出A模型更优，对于模型的PR曲线来说，如果这条曲线能包住另一条模型曲线，则可以肯定这条曲线下的模型更优秀（因为其完全覆&gt; 盖了模型C,故可以轻易取到PR值都接近理想话（1,1）的点）但当模型的PR图有交叉时，就不太容易通过肉眼比较了，譬如这里A和B。 对于模型PR曲线有交叉的情况，我们可以考察其F1值，F1值越大，模型越优！（因为通常F1值越高，反应到图像上，表明此模型曲线围绕X轴覆盖的面积更大，而面积越接近1，表明F和P两者都很高）上图A和B模型，模型A的F1&gt;模型B的F1值，故模型A更优秀。\n\nROC曲线\n\nReceiver Operating Characteristic curve，即ROC曲线，译为：受试者工作特征曲线。ROC曲线起源于“二战”，是一个用于敌机检测的雷达信号分析技术，后来应用到了医疗检测、心理学等领域，1989年被引入机器学习领域。在ROC曲线中，X轴为FPR（假正例率）,Y轴为TPR（真正例率）。其中FPR是False Positive Rate缩写，即FP率（假正例率），TPR是True Positive Rate的缩写，即TP率（真正例率）。结合图示更清晰：\nTPR分子：TP,即预测为正例，实际为正理例（预测正确）； TPR分母：所有正例样本 = TP + FN\nFPR分子：FP,即预测为正例，实际为负例（预测错误）； FPR分母：所有负例样本数 = FP + TN\n\n\n\n\n\n\n\n\n\n可以看出，TPR实际上就 = Recall查全率,FPR实际上就 = Precision查准率，只是两者X轴和Y轴次序相反\n\n\n\n\n\n\n\n\n\n\n和PR曲线类似，如果一个模型曲线将另一条模型曲线“完全包住”，则可以断定此模型性能更优！否则，还是需要通过ROC曲线围X轴的面积来确定模型优劣，此面积即AUC(Area Under ROC Curve)\n\nAUC\n\nAUC，即Area Under ROC Curve，表示ROC曲线中，曲线和X轴围成的面积，通常比较两个模型的AUC大小，AUC大者，表明其面积大，更接近1,模型的TPR和FPR两者都相对较高，模型更优。\n\nAP,mAP\n\nAverage Precision平均精准率 = （某一个类别）每个样本的精确率求和/样本总数N。 看一个例子，假设VOC数据集中，分别有person,train,cat,dog...总计20个分类类别，测试集有1000张图片，则针对其中某一类，譬如cat类，我们计算其平均精准率AP即可用：,为什么是1000张图片的Precision累加？因为每一张图都是潜在的待分类图片，可能包含20个类别中的1～多个分类，且可能包含这些分类下的1～n个bounding box，故对于某个分类，单独一张图片有需要独立计算其精准度。\nMean Average Precision均值平均精准率 = 所有类别的AP值累加求和/类别数\n\n\nIS(inception score)\n\n\n\n\n\n\n\n\n\n\n衡量图片的生成质量是一个比较难的问题，一直以来也没有一个特别好的度量方式，inception score的思想，是通过将生成模型的评估问题，通过映射到分类器上，以此来简化评估的难易程度，是一个非常好的创新。当然这种映射必然存在的问题就是信息的丢失。真实图片的inception_score是肯定比较高的，但是inception score高并不能代表生成的质量就好。(最简单的例子就是通过adversarial samples,可以通过简单的生成每个类的特征图谱，看似噪声的图像，但是其inception score会很高。)\n\n\n\n\n\n\n\n\n\nIS值越高，图片质量和多样性则好\nIS用来衡量GAN网络的两个指标： * 生成图片的质量 * 多样性\n8.1 图片的质量 熵（entropy）可以用来描述随机性：如果一个随机变量是高度可预测的，那么它就有较低的熵；相反，如果它是高度不可预测，那么它就用较高的熵。 如下图，我们有两个概率分布，p2比p1有更高的熵值，因为p2是一个均匀分布，我们很难预测x的值。\n\n在GAN中，我们希望条件概率｜可以被高度预测x表示给定的图片，y表示这个图片包含的主要物体，看到后面你会更加清楚这个概率是什么意思），也就是希望它的熵值较低。例如，给定一个图片，我们很容易的知道其中包含什么物体。 因此，我们使用inception network（可以理解这是一个固定的分类网络）来对生成的图像进行分类。（这里都是针对ImageNet数据集而言）然后预测 ｜， 这里y就是标签。用这个概率来反应图片的质量。 简单来说，假如inception network能够以较高的概率预测图片中包含的物体，也就是有很高的把握对其进行正确分类，这就说明图片质量较高。相反，比如我们人眼并无法看出这张图片是什么，就说明这个图片质量不高。\n\n\n\n\n\n\n\n\n\n综上，我们知道概率｜代表了图片的质量，概率越大，质量则越高。\n8.2 多样性 如果生成的图像多样化很好，那么预测的标签y的分布则会有较高的熵，因为数量多了，我们就更难准确预测 y。\n结合以上两个指标来说，我们的目标应该就是这样的： 1. 图片质量：针对每一张生成的图片，已知的分类器应该很确信的知道它属于哪一类。而这可以用条件概率｜来表示，它越大越好。而｜熵应该是越小越好。\n2. 图片的多样性：我们这时候考虑的是标签的分布情况，我们希望标签分布均与，而不希望模型生成的都是某一类图片。这时候我们考虑的不是条件概率了，而是边缘概率，也就是P(y),展开来写应该是 ，这里的n就是原训练数据的类数。我们希望，从熵的角度来说，我们希望 P(y)熵越大越好。\n\n\n\n\n\n\n\n\n\n1.最大化H(y);也就是对于输入的样本，通过inception_v3模型后的类别要均衡，衡量模式坍塌。 2.最小化H(y|x);说明对于输入的样本，通过inception_v3模型后预测某类别的置信度要高，衡量图片生成的质量。\nIS缺点：当只产生一种物体的图像时，我们仍会认为这是均匀分布，而导致评价不正确。当模型坍塌时，结果就可能产生同样的图片。\n\nFID\n\n在计算FID中我们也同样使用inception network网络。我们还是先来简单回顾一下什么是inception network，它就是一个特征提取的深度网络，最后一层是一个pooling层，然后可以输出一张图像的类别。在计算FID时，我们去掉这个最后一层pooling层，得到的是一个2048维的高层特征，以下简称n维特征。我们继续简化一下，那么这个n维特征是一个向量。则有：对于我们已经拥有的真实图像，这个向量是服从一个分布的，（我们可以假设它是服从一个高斯分布）；对于那些用GAN来生成的n维特征它也是一个分布；我们应该立马能够知道了，GAN的目标就是使得两个分布尽量相同。假如两个分布相同，那么生成图像的真实性和多样性就和训练数据相同了。 于是，现在的问题就是，怎么计算两个分布之间的距离呢？我们需要注意到这两个分布是多变量的，也就是前面提到的n维特征。也就是说我们计算的是两个多维变量分布之间的距离，数学上可以用Wasserstein-2 distance或者Frechet distance来进行计算。以下简单介绍一下如何计算这个距离。\n\n\n\n\n\n\n\n\n\n假如一个随机变量服从高斯分布，这个分布可以用一个均值和方差来确定。那么两个分布只要均值和方差相同，则两个分布相同。我们就利用这个均值和方差来计算这两个单变量高斯分布之间的距离。但我们这里是多维的分布，我们知道协方差矩阵可以用来衡量两个维度之间的相关性。所以，我们使用均值和协方差矩阵来计算两个分布之间的距离。均值的维度就是前面n维特征的维度，也就是n维；协方差矩阵则是n*n的矩阵。\nFID公式： \n公式中，Tr表示矩阵对角线上元素的总和，矩阵论中俗称“迹”（trace）。均值为μ,协方差为Σ 。此外x表示真实的图片，g是生成的图片。 较低的FID意味着两个分布之间更接近，也就意味着生成图片的质量较高、多样性较好。 FID对模型坍塌更加敏感。相比较IS来说，FID对噪声有更好的鲁棒性。因为假如只有一种图片时，FID这个距离将会相当的高。因此，FID更适合描述GAN网络的多样性。\n\n\n\n\n\n\n\n\n\n同样的，FID和IS都是基于特征提取，也就是依赖于某些特征的出现或者不出现。但是他们都无法描述这些特征的空间关系。如下图,这里我们我们人不会认为这是一张好的人脸图片。但是根据FID和IS，他们就是一个很好的人脸图片。因为它有人脸必要的特征，虽然这些特征的空间关系不好。\n\n1.2 实际问题\n\n样本不均衡的情况下Accuracy、ROC和PR曲线的差别和表现\n\n\n\n兼顾正例和负例的权衡。因为TPR聚焦于正例，FPR聚焦于与负例，使其成为一个比较均衡的评估方法。\nROC曲线选用的两个指标,都不依赖于具体的类别分布。\n\n注意TPR用到的TP和FN同属P列，FPR用到的FP和TN同属N列，所以即使P或N的整体数量发生了改变，也不会影响到另一列。也就是说，**即使正例与负例的比例发生了很大变化，ROC曲线也不会产生大的变化，而像Precision使用的TP和FP就分属两列，则易受类别分布改变的影响***。\n上面提到ROC曲线的优点是不会随着类别分布的改变而改变，但这在某种程度上也是其缺点。因为负例N增加了很多，而曲线却没变，这等于产生了大量FP。像信息检索中如果主要关心正例的预测准确性的话，这就不可接受了。在类别不平衡的背景下，负例的数目众多致使FPR的增长不明显，导致ROC曲线呈现一个过分乐观的效果估计。\n\n\n\n\n\n\n\n\n\nTPR考虑的是第一行，实际都是正例，FPR考虑的是第二行，实际都是负例。因此，在正负样本数量不均衡的时候，比如负样本的数量增加到原来的10倍，那TPR不受影响，FPR的各项也是成比例的增加，并不会有太大的变化。因此，在样本不均衡的情况下，同样ROC曲线仍然能较好地评价分类器的性能，这是ROC的一个优良特性，也是为什么一般ROC曲线使用更多的原因。\n\n\n\n\n\n\n\n\n\nROC曲线和PR曲线的区别 1.当正负原本数量比较均衡的时候，两者差别不大，当数量比例失衡时，ROC曲线不如PR曲线能更好的反映出分类器的真实性能，通过PR曲线的查准率指标反映出来。 2.PR曲线比ROC曲线更关注正样本，ROC曲线兼顾了两者。\n\n\n\n\n\n\n\n\n\n对于正负样本数量极不均衡的情况，只通过准确率（Accuracy）往往难以反映系统的真实性能。 举例说明，对于一个地震预测系统，假设所有样本中，1000天中有1天发生地震，记：0：不地震，1：地震，分类器不假思索的将所有样本分类为0，即可得到99.99%的准确率，当地震真正来临时，并不能成功预测，这种结果是我们不能接受的。\n\n一句话说明AUC的本质和计算规则\n\n本质：一个正例，一个负例，预测为正例的概率值大于预测为负的概率值的可能性； 计算规则：ROC曲线下的面积：\n\nAUC高可以理解为精确率高吗？\n\n不可以，精确率是基于某个阈值进行计算的，AUC是基于所有可能的阈值进行计算的，具有更高的健壮性。AUC不关注某个阈值下的表现如何，综合所有阈值的预测性能，所以精确率高，AUC不一定大，反之亦然。\n\n同时增大或减小样本的预测概率，会对AUC产生什么影响？ 没影响，因为是同时增大，AUC本质是预测为正例的概率值大于预测为负的概率值的可能性。\n\n2. CNN基础知识\n2.1 卷积\n\n\n\n\n\n\n\n\n\n所谓两个函数的卷积，本质上就是先将一个函数翻转，然后进行滑动叠加。\n对卷积的意义的理解： 1. 从“积”的过程可以看到，我们得到的叠加值，是个全局的概念。以信号分析为例，卷积的结果是不仅跟当前时刻输入信号的响应值有关，也跟过去所有时刻输入信号的响应都有关系，考虑了对过去的所有输入的效果的累积。在图像处理的中，卷积处理的结果，其实就是把每个像素周边的，甚至是整个图像的像素都考虑进来，对当前像素进行某种加权处理。所以说，“积”是全局概念，或者说是一种“混合”，把两个函数在时间或者空间上进行混合。 2. 那为什么要进行“卷”？直接相乘不好吗？我的理解，进行“卷”（翻转）的目的其实是施加一种约束，它指定了在“积”的时候以什么为参照。在信号分析的场景，它指定了在哪个特定时间点的前后进行“积”，在空间分析的场景，它指定了在哪个位置的周边进行累积处理。\n卷积时间复杂度 =  k是kenerl的尺寸，m是输出的下一层的feature map的尺寸.\n底层实现: 主要用的就是矩阵相乘，也就是GEMM。先用一个叫im2col的函数把图像转成矩阵，然后和卷积核做矩阵乘法。参考\n感受野计算: \n\n增大感受野的方法: pooling 池化: 池化主要任务是对数据降维，减小网络参数，提升网络的计算效率，同时，池化也是增加感受野的方法之一，但在增加感受野的同时，伴随着分辨率的降低，图像细节损失 dilated conv空洞卷积: 空洞卷积的出现为了解决pooling层增大感受野之后进行上采样（增加图像的分辨率）过程中，图像信息缺失问题。\n1x3、3x1代替3x3卷积: &gt; 各向同性的是可以的，参考高斯滤波，但是深度学习里面的卷积核不太可能各向同性\nif a 2D kernel has a rank of one, the operation can be equivalently transformed into a series of 1D convolutions\nBut 1. However, as the learned kernels in deep networks have distributed eigenvalues, their intrinsic rank is higher than one in practice, thus applying the transformation directly to the kernels results in signifificant information loss。 2. However, the authors found out that such replacement is not equivalent as it did not work well on the low-level layers\n参考论文\n加速:\nWinograd快速卷积算法\n2.2 反卷积/转置卷积\n解释\n转置卷积的计算:\n输入层：\n超参数：\n过滤器个数：k 过滤器中卷积核维度：w*h 滑动步长（Stride）：s 填充值（Padding）：p 输出层：\n（为简化计算，设，则记\n其中输出层和输入层之间的参数关系分为两种情况：\n情况一：(in-1) * s -2p + k = out\n\n情况二：(in-1) * s -2p + k != out\n\n棋盘效应\n知乎：Redflashing 在使用转置卷积时观察到一个棘手的现象（尤其是深色部分常出现）就是\"棋盘格子状伪影\"，被命名为棋盘效应（Checkboard artifacts）。\n\n棋盘效应是由于转置卷积的“不均匀重叠”（Uneven overlap）的结果。使图像中某个部位的颜色比其他部位更深。尤其是当卷积核（Kernel）的大小不能被步长（Stride）整除时，反卷积就会不均匀重叠。虽然原则上网络可以通过训练调整权重来避免这种情况，但在实践中神经网络很难完全避免这种不均匀重叠。 ​下面通过一个详细的例子，更为直观展示棋盘效应。下图的顶部部分是输入层，底部部分为转置卷积输出结果。结果转置卷积操作，小尺寸的输入映射到较大尺寸的输出（体现在长和宽维度）。 在（a）中，步长为1，卷积核为2*2。如红色部分所展示，输入第一个像素映射到输出上第一个和第二个像素。而正如绿色部分，输入的第二个像素映射到输出上的第二个和第三个像素。则输出上的第二个像素从输入上的第一个和第二个像素接收信息。总而言之，输出中间部分的像素从输入中接收的信息存在重叠区域。在示例（b）中的卷积核大小增加到3时，输出所接收到的大多数信息的中心部分将收缩。但这并不是最大的问题，因为重叠仍然是均匀的。\n\n如果将步幅改为2，在卷积核大小为2的示例中，输出上的所有像素从输入中接收相同数量的信息。由下图（a）可见，此时描以转置卷积的重叠。若将卷积核大小改为4（下图（b）），则均匀重叠区域将收缩，与此同时因为重叠是均匀的，故仍然为有效输出。但如果将卷积核大小改为3，步长为2（下图（c）），以及将卷积核大小改为5，步长为2（下图（d）），问题就出现了，对于这两种情况输出上的每个像素接收的信息量与相邻像素不同。在输出上找不到连续且均匀重叠区域。\n\n在二维情况下棋盘效应更为严重，下图直观地展示了在二维空间内的棋盘效应。\n\n如何避免棋盘效应\n\n采取可以被步长整除的卷积核长度 该方法较好地应对了棋盘效应问题，但仍然不够圆满，因为一旦我们的卷积核学习不均匀，依旧会产生棋盘效应（如下图所示）\n\n\n\n插值 可以直接进行插值Resize操作，然后再进行卷积操作。该方式在超分辨率的相关论文中比较常见。例如我们可以用常见的图形学中常用的双线性插值和近邻插值以及样条插值来进行上采样。\n\n\n2.3 空洞卷积/扩张卷积/膨胀卷积\n知乎：代辰 空洞卷积（Atrous Convolution），向卷积层引入了一个称为“扩张率(dilation rate)”的新参数，该参数定义了卷积核处理数据时各值的间距。换句话说，相比原来的标准卷积，扩张卷积多了一个超参数称之为dilation rate（扩张率），指的是kernel各点之间的间隔数量，正常的卷积核的扩张率为1。\n\n上图是一个扩张率为2，尺寸为 3×3 的空洞卷积，感受野与5×5的卷积核相同，而且仅需要9个参数。在相同的计算条件下，空洞卷积提供了更大的感受野。当网络层需要较大的感受野，但计算资源有限而无法提高卷积核数量或大小时，可以考虑空洞卷积。\n\n\n\n\n\n\n\n\n\n空洞卷积主要有两方面的作用： 1.扩大感受野在深度学习网络中为了增加感受野且降低计算量，总要进行降采样(pooling或conv)，这样虽然可以增加感受野，但空间分辨率降低了。为了能不丢失分辨率，且仍然扩大感受野，可以使用空洞卷积。这在检测，分割任务中十分有用。一方面感受野大了可以检测分割大目标，另一方面分辨率高了可以精确定位目标。 2.捕获多尺度上下文信息空洞卷积有一个参数可以设置dilation rate，具体含义就是在卷积核中填充dilation rate-1个0，因此，当设置不同dilation rate时，感受野就会不一样，也即获取了多尺度信息。多尺度信息在视觉任务中相当重要啊。DeepLab中的ASPP模块 从这里可以看出，空洞卷积可以任意扩大感受野，且不需要引入额外参数，但如果把分辨率增加了，算法整体计算量肯定会增加。\n\n\n\n\n\n\n\n\n\n空洞卷积虽然有这么多优点，但在实际中不好优化，速度会大大折扣。\n空洞卷积存在的问题:\n空洞卷积是存在理论问题的，论文中称为gridding，其实就是网格效应/棋盘问题。因为空洞卷积得到的某一层的结果中，邻近的像素是从相互独立的子集中卷积得到的，相互之间缺少依赖。\n\n局部信息丢失： 由于空洞卷积的计算方式类似于棋盘格式，某一层得到的卷积结果，来自上一层的独立的集合，没有相互依赖，因此该层的卷积结果之间没有相关性，即局部信息丢失。\n远距离获取的信息没有相关性： 由于空洞卷积稀疏的采样输入信号，使得远距离卷积得到的信息之间没有相关性，影响分类结果。\n\n(解决方法：)[https://zhuanlan.zhihu.com/p/50369448] 1. Removing max pooling：由于maxpool会引入更高频的激活，这样的激活会随着卷积层往后传播，使得grid问题更明显。 2. Adding layers：在网络最后增加更小空洞率的残参block, 有点类似于HDC。 3. Removing residual connections：去掉残参连接，防止之前层的高频信号往后传播。\n\n\n\n\n\n\n\n\n\n总结：简单来说，就是空洞卷积虽然在参数不变的情况下保证了更大的感受野，但是对于一些很小的物体，本身就不要那么大的感受野来说，这是不友好的。\n空洞卷积感受野如何计算\n\n\n\n\n\n\n\n\n\n和标准卷积是一致的 空洞卷积实际卷积核大小：K=k+(k-1)(r-1)，k为原始卷积核大小，r为空洞卷积参数空洞率；\n以三个r=2的3*3/s1空洞卷积为例计算感受野：\nK=k+(k-1)(r-1)=3+2*1=5\nR=1+4+4+4=13\n\n\n\n\n\n\n\n\n\n相当于三个kernel size=5的卷积核串联，如果步长设置为1的话，每两层的尺寸关系为X0-5+1=X1,也就是X0-4=X1,因此可以得到通项公式，X0=Xi+i*4。因此对于三个卷积核有四个层，对应i=3，X3=1,所以是13\n2.4 depthwise卷积\n\n\n常规卷积(来源见水印)\n\nDepthwise Convolution的一个卷积核负责一个通道，一个通道只被一个卷积核卷积。常规卷积每个卷积核是同时操作输入图片的每个通道。\n同样是对于一张5×5像素、三通道彩色输入图片（shape为5×5×3），Depthwise Convolution首先经过第一次卷积运算，不同于上面的常规卷积，DW完全是在二维平面内进行。卷积核的数量与上一层的通道数相同（通道和卷积核一一对应）。所以一个三通道的图像经过运算后生成了3个Feature map(如果有same padding则尺寸与输入层相同为5×5)，如下图所示。\n\n\ndepthwise卷积(来源见水印)\n\n其中一个Filter只包含一个大小为3×3的Kernel，卷积部分的参数个数计算如下： N_depthwise = 3 × 3 × 3 = 27\n\n\n\n\n\n\n\n\n\nDepthwise Convolution完成后的Feature map数量与输入层的通道数相同，无法扩展Feature map。而且这种运算对输入层的每个通道独立进行卷积运算，没有有效的利用不同通道在相同空间位置上的feature信息。因此需要Pointwise Convolution来将这些Feature map进行组合生成新的Feature map。\n2.5 pointwise卷积\nPointwise Convolution的运算与常规卷积运算非常相似，它的卷积核的尺寸为 1×1×M，M为上一层的通道数。所以这里的卷积运算会将上一步的map在深度方向上进行加权组合，生成新的Feature map。有几个卷积核就有几个输出Feature map。如下图所示。\n\n\npointwise卷积(来源见水印)\n\n由于采用的是1×1卷积的方式，此步中卷积涉及到的参数个数可以计算为： N_pointwise = 1 × 1 × 3 × 4 = 12\n经过Pointwise Convolution之后，同样输出了4张Feature map，与常规卷积的输出维度相同。\n\n\n\n\n\n\n\n\n\n参数对比 回顾一下，常规卷积的参数个数为： N_std = 4 × 3 × 3 × 3 = 108 Separable Convolution的参数由两部分相加得到： N_depthwise = 3 × 3 × 3 = 27 N_pointwise = 1 × 1 × 3 × 4 = 12 N_separable = N_depthwise + N_pointwise = 39 相同的输入，同样是得到4张Feature map，Separable Convolution的参数个数是常规卷积的约1/3。因此，在参数量相同的前提下，采用Separable Convolution的神经网络层数可以做的更深。\n2.6 conv，depthwise和pointwise 计算量\n假设输入特征图大小为  ×  × M， 输出特征图大小为  ×  × N， 卷积核大小为  × \n参数量：     标准卷积参数量为：          ×  × M × N\n    深度可分离卷积参数量为(depthwise+pointwise)：          ×  × M + M × N\n计算量：     标准卷积计算量为：          ×  × M ×  ×  × N\n    深度可分离卷积计算量为：          ×  × M ×  ×  + M ×  ×  × N\n\n可见参数数量和乘加操作的运算量均下降为原来的 我们通常所使用的是3×3的卷积核，也就是会下降到原来的九分之一到八分之一。\n2.7 分组卷积\n\n第一张图代表标准卷积操作。若输入特征图尺寸为 ，卷积核尺寸为 ，输出特征图尺寸为，标准卷积层的参数量为：*c_2。（一个滤波器在输入特征图 大小的区域内操作，输出结果为1个数值，所以需要 个滤波器。）\n第二张图代表分组卷积操作。将输入特征图按照通道数分成g组，则每组输入特征图的尺寸为 ，对应的卷积核尺寸为，每组输出特征图尺寸为。将g组结果拼接(concat)，得到最终尺寸为 的输出特征图。分组卷积层的参数量为 。\n深入思考一下，常规卷积输出的特征图上，每一个点是由输入特征图个点计算得到的；而分组卷积输出的特征图上，每一个点是由输入特征图 个点计算得到的。自然，分组卷积的参数量是标准卷积的 。\n2.8 PixelShuffle\nPixelShuffle(像素重组)的主要功能是将低分辨的特征图，通过卷积和多通道间的重组得到高分辨率的特征图。这一方法最初是为了解决图像超分辨率问题而提出的，这种称为Sub-Pixel Convolutional Neural Network的方法成为了上采样的有效手段。\n\npixelshuffle的主要功能就是将这个通道的特征图组合为新的w*r, h*r的上采样结果。具体来说，就是将原来一个低分辨的像素划分为r各更小的格子，利用r个特征图对应位置的值按照一定的规则来填充这些小格子。按照同样的规则将每个低分辨像素划分出的小格子填满就完成了重组过程。在这一过程中模型可以调整r*r个shuffle通道权重不断优化生成的结果。 主要实现了这样的功能：N*(C*r*r)*W*H----&gt;&gt;N*C*(H*r)*(W*r)\n\n\n\n\n\n\n\n\n\n在图像超分辨和图像增强的算法中需要对特征图进行上下采样的过程，可以解决插值和解卷积的一些人工痕迹问题。\n2.9 Pooling\n(知乎)[https://www.zhihu.com/question/49376084/answer/172483833]\nPooling 的本质，其实是采样。Pooling 对于输入的 Feature Map，选择某种方式对其进行压缩。如Max-Pooling,还有Mean-Pooling，Stochastic-Pooling 等。它们的具体实现如名称所示，具体选择哪一个则取决于具体的任务。 Pooling 的意义，主要有两点： 1. 减少参数。通过对 Feature Map 降维，有效减少后续层需要的参数 2. Translation Invariance(平移不变性)。它表示对于 Input，当其中像素在邻域发生微小位移时，Pooling Layer 的输出是不变的。这就使网络的鲁棒性增强了，有一定抗扰动的作用\n2.10 maxpooling/averagepooling怎么传递导数？(反池化)\n(参考)[https://zhuanlan.zhihu.com/p/258604402]\n池化层在反向传播时，它是不可导的，因为它是对特征图进行下采样会导致特征图变小，比如一个2x2的池化，在L+1层输出的特征图是16个神经元，那么对应L层就会有64个神经元，两层之间经过池化操作后，特征图尺寸改变，无法一一对应，这使得梯度无法按位传播。那么如何解决池化层不可导但却要参与反向传播呢？\n在反向传播时，梯度是按位传播的，那么，一个解决方法，就是如何构造按位的问题，但一定要遵守传播梯度总和保持不变的原则。\n对于平均池化，其前向传播是取某特征区域的平均值进行输出，这个区域的每一个神经元都是有参与前向传播了的，因此，在反向传播时，框架需要将梯度平均分配给每一个神经元再进行反向传播，相关的操作示意如下图所示。\n\n对于最大池化，其前向传播是取某特征区域的最大值进行输出，这个区域仅有最大值神经元参与了前向传播，因此，在反向传播时，框架仅需要将该区域的梯度直接分配到最大值神经元即可，其他神经元的梯度被分配为0且是被舍弃不参与反向传播的，但如何确认最大值神经元，这个还得框架在进行前向传播时记录下最大值神经元的Max ID位置，这是最大池化与平均池化差异的地方，相关的操作示意如下图所示。\n\n其中，上述表格中，前向传播时，每个单元格表示特征图神经元值，而在反向传播时，每个单元格表示的是分配给对应神经元的梯度值。\n2.11 (BN IN LN GN ADAIN)[https://zhuanlan.zhihu.com/p/289384231?utm_source=wechat_session]\n\nBN计算均值和标准差时，固定channel(在一个channel内)，对HW和batch作平均； LN计算均值和标准差时，固定batch(在一个batch内)，对HW和channel作平均； IN计算均值和标准差时，同时固定channel和batch(在一个batch内中的一个channel内)，对HW作平均； GN计算均值和标准差时，固定batch且对channel作分组(在一个batch内对channel作分组)，在分组内对HW作平均。\n2.11.1 BN\n在网络层数加深的时候，会影响我们每一层输出的数据分布。而之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（以Sigmoid函数为例），所以这导致后向传播时低层神经网络的梯度很小甚至消失，这是训练深层神经网络收敛越来越慢的本质原因，而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，所以就可以让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。\nBN核心公式： \n\nBN的具体操作为：先计算B的均值和方差，之后将B集合的均值、方差变换为0、1，最后将B中每个元素乘以γ再加β，输出。 γ、β是可训练参数，参与整个网络的BP；\n归一化的目的：将数据规整到统一区间，减少数据的发散程度，降低网络的学习难度。BN的精髓在于归一之后，使用γ、β作为还原参数，在一定程度上保留原数据的分布。\n\n训练与推理时BN中的均值、方差分别是什么？ 训练时，均值、方差分别是该批次内数据相应维度的均值与方差； 推理时，均值、方差是基于所有批次的期望计算所得，公式如下：  其中，E(x)表示x的期望.\n代码实现技巧：\n\n在代码实现中有一个技巧，如果训练几百万个Batch，那么是不是要将其均值方差全部储存，最后再计算推理时所用的均值和方差？这样显然太过笨拙，占用内存随着训练次数不断上升。为了避免该问题，实际代码（如tf）使用了滑动平均，储存固定个数Batch的均值和方差，不断迭代更新推理时需要的E(x)与Var(x)。 注意到代码中： 1. beta、gamma在训练状态下，是可训练参数，在推理状态下，直接加载训练好的数值。 2. moving_mean、moving_var在训练、推理中都是不可训练参数，只根据滑动平均计算公式更新数值，不会随着网络的训练BP而改变数值；在推理时，直接加载储存计算好的滑动平均之后的数值，作为推理时的均值和方差。\nBN层的本质: 平滑了优化空间\n对比白化操作:\n白化是机器学习里面 常用的一种规范化数据分布的方法，主要是PCA白化与ZCA白化。白化是对输入数据分布进行变换，进而达到以下两个目的： （1）使得输入特征分布具有相同的均值与方差。 （2）去除特征之间的相关性。 但是白化也存在两个问题： （1）计算成本太大，像PCA白化还要算协方差矩阵等。 （2）白化过程由于改变了网络每一层的分布，因而改变了网络层中本身数据的表达能力。底层网络学习到的参数信息会被白化操作丢失掉。而BN正是针对了白化的这两个缺点进行了改善\nBN的缺陷:\n\n需要较大的 batchsize 才能合理估训练数据的均值和方差(横向计算)，这导致内存很可能不够用;\n很难应用在训练数据长度不同的 RNN 模型上.\n\nBN和卷积层融合:\n\n2.11.2 IN\nIN是针对于不同的batch, 不同的chennel进行归一化。还是把图像的尺寸表示为[N, C, H, W]的话，IN则是针对于[H,W]进行归一化。这种方式通常会用在风格迁移的训练中。\n\n\n\n\n\n\n\n\n\nIN 操作也在单个样本内部进行，不依赖 batch。\n2.11.3 LN\nLN的方法是对于每一个sample中的多个feature(也就是channel)进行归一化操作。把图像的尺寸表示为[N, C, H, W]的话，LN则是对于[C,H,W]进行归一化。相对于BN中所表示的同一个feature在不同的batch之间拥有同样的均值和方差。LN中所表示的则是在同一个sample中，不同的feature上有着相同的均值和方差。\n与BN相比，LN也不依赖于mini-batch size的大小。这种操作通常运用在RNN的网络中。\n\n\n\n\n\n\n\n\n\nLayer Normalization (LN) 的一个优势是不需要批训练，在单条数据内部就能归一化。LN 对每个样本的 C、H、W 维度上的数据求均值和标准差，保留 N 维度。\n2.11.4 GN\nGN是介乎于instance normal 和 layer normal 之间的一种归一化方式。也就是说当我们把所有的channel都放到同一个group中的时候就变成了layer normal， 如果我们把每个channel都归为一个不同的group，则变成了instance normal.\nGN同样可以针对于mini batch size较小的情况。因为它有不受batch size的约束。\n可以看到与BN不同，LN/IN和GN都没有对batch作平均，所以当batch变化时，网络的错误率不会有明显变化。但论文的实验显示：LN和IN 在时间序列模型(RNN/LSTM)和生成模型(GAN)上有很好的效果，而GN在视觉模型上表现更好。\n\n\n\n\n\n\n\n\n\nGroup Normalization (GN) 适用于占用显存比较大的任务，例如图像分割,对这类任务，可能 batchsize 只能是个位数，再大显存就不够用了。而当 batchsize 是个位数时，BN 的表现很差，因为没办法通过几个样本的数据量，来近似总体的均值和标准差。GN 也是独立于 batch 的，它是 LN 和 IN 的折中\n2.11.5 映射参数γ和β的区别\n对于 BN，IN，GN， 其γ和β都是维度等于通道数 C 的向量。而对于 LN，其γ和β都是维度等于 normalized_shape 的矩阵。 最后，BN和IN 可以设置参数：momentum 和 track_running_stats来获得在全局数据上更准确的 running mean 和 running std。 而 LN 和 GN 只能计算当前 batch 内数据的真实均值和标准差。\n2.11.6 dropout\n在训练时，每个神经单元以概率𝑝被保留(Dropout丢弃率为1−𝑝)； 在预测阶段（测试阶段），每个神经单元都是存在的，权重参数𝑤要乘以𝑝，输出是：𝑝𝑤。\n预测阶段需要乘上𝑝的原因： 　　前一层隐藏层的一个神经元在𝑑𝑟𝑜𝑝𝑜𝑢𝑡之前的输出是𝑥，训练时𝑑𝑟𝑜𝑝𝑜𝑢𝑡之后的期望值是𝐸=𝑝𝑥+(1−𝑝)0˙; 在预测阶段该层神经元总是激活，为了保持同样的输出期望值并使下一层也得到同样的结果，需要调整𝑥−&gt;𝑝𝑥. 其中𝑝是Bernoulli分布（0-1分布）中值为1的概率。\n为什么说Dropout可以解决过拟合？ 1. 取平均的作用： 先回到标准的模型即没有dropout，我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。例如3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。 2. 减少神经元之间复杂的共适应关系： 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。 3. Dropout类似于性别在生物进化中的角色：物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝。\ndropout会改变数据分布, 导致训练测试样本分布不一致，怎么解决\n实践发现droput之后改变了数据的标准差（令标准差变大，若数据均值非0时，甚至均值也会产生改变）。 如果同时又使用了BN归一化，由于BN在训练时保存了训练集的均值与标准差。dropout影响了所保存的均值与标准差的准确性（不能适应未来预测数据的需要），那么将影响网络的准确性。\ndropout 有两种实现方式，Vanilla Dropout 和 Inverted Dropout。 主流框架使用Inverted Dropout： 知乎：神经网络Dropout层中为什么dropout后还需要进行rescale？\n\n当模型使用了dropout layer，训练的时候只有占比为p的隐藏层单元参与训练，那么在预测的时候，如果所有的隐藏层单元都需要参与进来，则得到的结果相比训练时平均要大，为了避免这种情况，就需要测试的时候将输出结果乘以p使下一层的输入规模保持不变。而利用inverted dropout，我们可以在训练的时候直接将dropout后留下的权重扩大倍，这样就可以使结果的scale保持不变，而在预测的时候也不用做额外的操作了，更方便一些。\n\n\n\n\n\n\n\n\n\n每个神经元的丢弃概率p遵循概率p的伯努利分布;伯努利实验是 “Single trial with only two outcomes\",二项实验是 “Repeating a Bernoulli experiment for n times”. 二项分布就是，将伯努利实验重复n次后的概率分布。\n2.11.7 CNN的平移不变性\n出处\nCNN的平移不变性。简单来说，平移不变性（translation invariant）指的是CNN对于同一张图及其平移后的版本，都能输出同样的结果。这对于图像分类（image classification）问题来说肯定是最理想的，因为对于一个物体的平移并不应该改变它的类别。而对于其它问题，比如物体检测（detection）、物体分割（segmentation）来说，这个性质则不应该有，原因是当输入发生平移时，输出也应该相应地进行平移。这种性质又称为平移等价性（translation equivalence）。\n作者：Hengkai Guo 链接：https://www.zhihu.com/question/301522740/answer/531606623 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n3. 激活函数相关\n\n\n\n\n\n\n\n\n\n无论是全连接层还是卷积层的计算都是简单的乘加运算，也称之为线性运算，这样我们可以作为线性层。但是，线性层的特征表达能力是有限的，所以在这些线性计算之后又引入了非线性计算，增强模型特征的表达能力，也就是大家熟知的激活层，也称为非线性层。 参考\n激活函数可以分为两大类 ： 饱和激活函数： sigmoid、 tanh 非饱和激活函数: ReLU 、Leaky Relu 、ELU【指数线性单元】、PReLU【参数化的ReLU 】、RReLU【随机ReLU】 相对于饱和激活函数，使用“非饱和激活函数”的优势在于两点： 1. 首先，“非饱和激活函数”能解决深度神经网络【层数非常多！！】的“梯度消失”问题，浅层网络【三五层那种】才用sigmoid 作为激活函数。 2. 其次，它能加快收敛速度。\n3.1 Sigmoid\nsigmoid函数也称为Logistic函数, sigmod函数的取值范围在（0, 1）之间，可以将网络的输出映射在这一范围，方便分析。\n3.1.1 公式及导数\n\n\n3.1.2 曲线\n\n\n(来源见水印)\n\n3.1.3 特点\n优点： 平滑、易于求导。\n缺点： 1. 激活函数计算量大（在正向传播和反向传播中都包含幂运算和除法）； 2. 反向传播求误差梯度时，求导涉及除法； 3. Sigmoid导数取值范围是[0, 0.25]，由于神经网络反向传播时的“链式反应”，很容易就会出现梯度消失的情况。例如对于一个10层的网络， 根据，第10层的误差相对第一层卷积的参数的梯度将是一个非常小的值，这就是所谓的“梯度消失”。 4. Sigmoid的输出不是0均值（即zero-centered）；这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入，随着网络的加深，会改变数据的原始分布。\n3.2 Softmax\n\n\n\n\n\n\n\n\n\n通过Softmax函数就可以将多分类的输出值转换为范围在[0, 1]和为1的概率分布。\n3.1.1 公式及导数\n 其中  为第i个节点的输出值，C为输出节点的个数，即分类的类别个数。\n用表示softmax:\n\n3.1.2 引入指数形式\n优点: 1. 指数函数曲线呈现递增趋势，最重要的是斜率逐渐增大，也就是说在x轴上一个很小的变化，可以导致y轴上很大的变化。这种函数曲线能够将输出的数值拉开距离。 2. 在深度学习中通常使用反向传播求解梯度进而使用梯度下降进行参数更新的过程，而指数函数在求导的时候比较方便。如: \n缺点(上溢和下溢问题): 指数函数的曲线斜率逐渐增大虽然能够将输出值拉开距离，但是也带来了缺点，当值非常大的话，计算得到的数值也会变的非常大，数值可能会溢出。同理当输入为负数且绝对值也很大的时候，会分子、分母会变得极小，有可能四舍五入为0，导致下溢出。\n\n\n\n\n\n\n\n\n\n有一个方法计算上溢的情况，也就是每个变量都减去最大值，然后做softmax；还有一种方法是直接用log softmax\n\n3.1.3 Sigmoid 和 Softmax 区别\nsigmoid将一个real value映射到（0,1）的区间，用来做二分类。而 softmax 把一个 k 维的real value向量（a1,a2,a3,a4….）映射成一个（b1,b2,b3,b4….）其中 bi 是一个 0～1 的常数，输出神经元之和为 1.0，所以相当于概率值，然后可以根据 bi 的概率大小来进行多分类的任务。\n二分类问题时 sigmoid 和 softmax 是一样的，求的都是 cross entropy loss，而 softmax 可以用于多分类问题多个logistic回归通过叠加也同样可以实现多分类的效果，但是 softmax回归进行的多分类，类与类之间是互斥的，即一个输入只能被归为一类；多个logistic回归进行多分类，输出的类别并不是互斥的，即\"苹果\"这个词语既属于\"水果\"类也属于\"3C\"类别。\n\n\n\n\n\n\n\n\n\n使用softmax回归或者多个logistics回归解决多分类问题时：使用哪一个主要根据类别之间是否互斥。对于选择softmax分类器还是k个logistics分类器，取决于所有类别之间是否互斥。所有类别之间明显互斥用softmax；所有类别之间不互斥有交叉的情况下最好用k个logistics分类器。\n\n3.1.4 为什么soft?\n知乎: hardmax最大的特点就是只选出其中一个最大的值，即非黑即白。但是往往在实际中这种方式是不合情理的，比如对于文本分类来说，一篇文章或多或少包含着各种主题信息，我们更期望得到文章对于每个可能的文本类别的概率值（置信度），可以简单理解成属于对应类别的可信度。所以此时用到了soft的概念，Softmax的含义就在于不再唯一的确定某一个最大值，而是为每个输出分类的结果都赋予一个概率值，表示属于每个类别的可能性。\n3.3 ReLU, ReLU6\n\n\n\n\n\n\n\n\n\nrelu和sigmoid比，1.不会梯度弥散; 2.稀疏参数; 3计算简单\nRelu(Rectified Linear Unit)——修正线性单元函数：该函数形式比较简单， 公式：relu=max(0, x)\n\n\n(来源见水印)\n\n从上图可知，ReLU的有效导数是常数1，解决了深层网络中出现的梯度消失问题，也就使得深层网络可训练。同时ReLU又是非线性函数，所谓非线性，就是一阶导数不为常数；对ReLU求导，在输入值分别为正和为负的情况下，导数是不同的，即ReLU的导数不是常数，所以ReLU是非线性的（只是不同于Sigmoid和tanh，relu的非线性不是光滑的）。\nReLU在x&gt;0下，导数为常数1的特点： 导数为常数1的好处就是在“链式反应”中不会出现梯度消失，但梯度下降的强度就完全取决于权值的乘积，这样就可能会出现梯度爆炸问题。解决这类问题：一是控制权值，让它们在（0，1）范围内；二是做梯度裁剪，控制梯度下降强度，如ReLU(x)=min(6, max(0,x))\nReLU在x&lt;0下，输出置为0的特点：\n描述该特征前，需要明确深度学习的目标：深度学习是根据大批量样本数据，从错综复杂的数据关系中，找到关键信息（关键特征）。换句话说，就是把密集矩阵转化为稀疏矩阵，保留数据的关键信息，去除噪音，这样的模型就有了鲁棒性。ReLU将x&lt;0的输出置为0，就是一个去噪音，稀疏矩阵的过程。而且在训练过程中，这种稀疏性是动态调节的，网络会自动调整稀疏比例，保证矩阵有最优的有效特征。 但是ReLU 强制将x&lt;0部分的输出置为0（置为0就是屏蔽该特征），可能会导致模型无法学习到有效特征，所以如果学习率设置的太大，就可能会导致网络的大部分神经元处于‘dead’状态，所以使用ReLU的网络，学习率不能设置太大。\nReLU作为激活函数的特点： 1. 相比Sigmoid和tanh，ReLU摒弃了复杂的计算，提高了运算速度。 2. 解决了梯度消失问题，收敛速度快于Sigmoid和tanh函数，但要防范ReLU的梯度爆炸 3. 容易得到更好的模型，但也要防止训练中出现模型‘Dead’情况。\nrelu小于0会导致梯度消失，怎么办？ &gt; 在小于的时候，激活函数梯度为零，梯度消失，神经元不更新，变成了死亡节点。出现这个原因可能是因为学习率太大，导致w更新巨大，使得输入数据在经过这个神经元的时候，输出值小于0，从而经过激活函数的时候为0，从此不再更新。所以relu为激活函数，学习率不能太大.\nRelu6:\nReLU6(x)=min(max(0,x),6) \nMobileNetV1 中使用 ReLU6。ReLU6 就是普通的ReLU，但是限制最大输出为6。这是为了在移动端设备 float16/int8 的低精度的时候也能 有很好的数值分辨率。如果对 ReLU 的激活范围不加限制，输出范围为 0 到正无穷，如果激活值非常大，分布在一个很大的范围内，则低精度的 float16/int8 无法很好地精确描述如此大范围的数值，带来精度损失。\nrelu6的好处： 1. 可以让模型更早地学到稀疏特征。并没有可信的理论推导，作者试验出来的结果是这样的。注意，relu6只是比relu更早的学习到稀疏特征，而不是说relu学不到稀疏特征。 2. 可以防止数值爆炸。 3. 增强浮点数的小数位表达能力。因为整数位最大是6，所以只占3个bit，其他bit全部用来表达小数位。\n3.4 Leaky ReLU, PReLU（Parametric Relu）, RReLU（Random ReLU）\n为了防止模型的‘Dead’情况，后人将x&lt;0部分并没有直接置为0，而是给了一个很小的负数梯度值α。\nLeaky ReLU中的α为常数，一般设置 0.01。这个函数通常比 Relu 激活函数效果要好，但是效果不是很稳定，所以在实际中 Leaky ReLu 使用的并不多。\nPRelu（参数化修正线性单元）中的α作为一个可学习的参数，会在训练的过程中进行更新。\nRReLU（随机纠正线性单元）也是Leaky ReLU的一个变体。在RReLU中，负值的斜率在训练中是随机的，在之后的测试中就变成了固定的了。RReLU的亮点在于，在训练环节中，是从一个均匀的分布U(I,u)中随机抽取的数值。\n\n\n(来源见水印)\n\n3.5 tanh\ntanh为双曲正切函数。tanh和 sigmoid 相似，都属于饱和激活函数，区别在于输出值范围由 (0,1) 变为了 (-1,1)，可以把 tanh 函数看做是 sigmoid 向下平移和拉伸后的结果。\n3.4.1 公式及导数\n 公式1  公式2  公式3\n从公式2中，可以更加清晰看出tanh与sigmoid函数的关系（平移+拉伸）\n3.4.2 曲线\n\n\n(来源见水印)\n\n3.4.3 特点\n相比Sigmoid函数， 1. tanh的输出范围时(-1, 1)，解决了Sigmoid函数的不是zero-centered输出问题； 2. 幂运算的问题仍然存在； 3. tanh导数范围在(0, 1)之间，相比sigmoid的(0, 0.25)，梯度消失（gradient vanishing）问题会得到缓解，但仍然还会存在。\n3.6 softplus\nSoftplus函数是Logistic-Sigmoid函数原函数,  加了1是为了保证非负性。Softplus可以看作是平滑版的relu。红色的即为ReLU。\n\n3.7 Swish, hard-Swish\nSwish:β是个常数或者可以训练的参数。 和 ReLU 一样，Swish 无上界有下界。与 ReLU 不同的是，Swish 是平滑且非单调的函数。事实上，Swish 的非单调特性把它与大多数常见的激活函数区别开来。\nf(x) = x*sigmoid(βx) f'(x) = sigmoid(βx) + x(β*sigmoid(βx))\n\n\n\n\n\n\n\n\n\n\nSwish函数可以看做是介于线性函数与ReLU函数之间的平滑函数.\nhard-Swish(MobileNetV3): 虽然这种Swish非线性提高了精度，但是在嵌入式环境中，他的成本是非零的，因为在移动设备上计算sigmoid函数代价要大得多。 因此作者使用hard-Swish和hard-Sigmoid替换了ReLU6和SE-block中的Sigmoid层，但是只是在网络的后半段才将ReLU6替换为h-Swish，因为作者发现Swish函数只有在更深的网络层使用才能体现其优势。 首先是肯定了Swish的重要性，然后指出在量化模式下，Sigmoid函数比ReLU6的计算代价大的多，所以才有了这个ReLU6版本的h-Swish。\n\n\n\n\n\n\n\n\n\n\n\nwhy hard？relu6有上界。如果对relu6再除6再向左平移三个单位，就会得到一条类似于sigmoid函数 \n3.8 Mish\nMish: y=*tanh(ln(1+exp(x)))\nMish和Swish中参数=1的曲线对比：（第一张是原始函数，第二张是导数）\n\n\n\n\n\n\n\n\n\n\n以上无边界(即正值可以达到任何高度)避免了由于封顶而导致的饱和。理论上对负值的轻微允许允许更好的梯度流，而不是像ReLU中那样的硬零边界。最后，可能也是最重要的，目前的想法是，平滑的激活函数允许更好的信息深入神经网络，从而得到更好的准确性和泛化。要区别可能是Mish函数在曲线上几乎所有点上的平滑度.\n3.9 神经网络中的激活函数为什么都是平滑或近似平滑的？\n\n激活值不存在像感知机那样的阶跃现象，比较容易收敛\n很多平滑函数的引入，使得模型有了非线性因素，因此可以识别更加复杂的模式\n平滑函数是可导的，这便于梯度的计算与更新。而对像感知机这样的激活函数，梯度的更新非常困难；如下图所示，两个激活值相同的点，从同一个点到另外一个点，似乎是没有很好的策略来更新梯度\n可导，因此可以指定更加灵活的梯度更新规则，加速模型训练\n\n4. 正则化相关\n4.1 正则化的本质\n知乎回答\n重点记录下： 如何去防止过拟合，首先想到的就是控制N的数量吧，即让N最小化吧，而让N最小化，其实就是让W向量中项的个数最小化，其中。 so,如何求解“让W向量中项的个数最小化\"? 没错，这就是0范数的概念！什么是范数，这里只是给出个0-2范数定义: 0范数，向量中非零元素的个数。 1范数，为绝对值之和。 2范数，就是通常意义上的模。\n\n\n\n\n\n\n\n\n\n求解“让W向量中项的个数最小化”吗？怎么与0范数的定义有点不一样，一句话，向量中0元素，对应的x样本中的项我们是不需要考虑的，可以砍掉。因为0*没有啥意义，说明项没有任何权重\n所以为了防止过拟合，除了需要前面的相加项最小，即最小，我们还需要让最小，所以，为了同时满足两项都最小化，咱们可以求解让和r(d)之和最小，这样不就同时满足两者了吗？如果r(d) 过大，再小也没用；相反r(d)再小，太大也失去了问题的意义。说到这里已经回答了，那就是为什么需要有个r(d)项，为什么r(d)能够防止过拟合原因了。\n\n\n\n\n\n\n\n\n\n1范数和0范数可以实现稀疏，1因具有比L0更好的优化求解特性而被广泛应用。然后L2范数，是下面这么理解的，我就直接查别人给的解释好了，反正简单，就不自己动脑子解释了： L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的正则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这里是有很大的区别的哦；所以大家比起1范数，更钟爱2范数。\n4.2 L1和L2正则化\nL1正则化与L2正则化\n\n \n降低过拟合程度：\n正则化之所以能够降低过拟合的原因在于，正则化是结构风险最小化的一种策略实现。\n给loss function加上正则化项，能使得新得到的优化目标函数h = f+normal，需要在f和normal中做一个权衡（trade-off），如果还像原来只优化f的情况下，那可能得到一组解比较复杂，使得正则项normal比较大，那么h就不是最优的，因此可以看出加正则项能让解更加简单，符合奥卡姆剃刀理论，同时也比较符合在偏差和方差（方差表示模型的复杂度）分析中，通过降低模型复杂度，得到更小的泛化误差，降低过拟合程度。\nL1正则化和L2正则化：\nL1正则化就是在loss function后边所加正则项为L1范数，加上L1范数容易得到稀疏解（0比较多）。 L2正则化就是loss function后边所加正则项为L2范数的平方，加上L2正则相比于L1正则来说，得到的解比较平滑（不是稀疏），但是同样能够保证解中接近于0（但不是等于0，所以相对平滑）的维度比较多，降低模型的复杂度。\nl1 相比于 l2 为什么容易获得稀疏解？\n两种 regularization 能不能把最优的 x 变成 0，取决于原先的费用函数在 0 点处的导数。如果本来导数不为 0，那么施加 L2 regularization 后导数依然不为 0，最优的 x 也不会变成 0。而施加 L1 regularization 时，只要 regularization 项的系数 C 大于原先费用函数在 0 点处的导数的绝对值，x = 0 就会变成一个极小值点。上面只分析了一个参数 x。事实上 L1 regularization 会使得许多参数的最优值变成 0，这样模型就稀疏了。\nL1和L2正则化的区别 1. 从求解效率上看： L2损失函数是可导的，L2正则化也是可导的，所以L2正则化是有解析解的，求解的效率高。但是L1正则化在零点处是不可导的，所以它是没有解析解的，如果问题是一个稀疏问题（简单地说就是很多特征的系数为0），那么可以采用稀疏算法求解，如果问题不是稀疏的，那求解的效率就很低了。 2. 从解的角度看：L2正则化得到不会是稀疏性结果，但是L1正则化可能会得到稀疏结果。 3. L1正则化的优点在于它可以进行特征选择（由于其结果是稀疏的，即很多变量前的系数为0，那么这些系数为0的变量，就是被淘汰的变量）；但是L2正则化则不行。（因为L2正则化的结果不是稀疏的）。所以可以认为L1正则化是一种嵌入式的特征选择方法，其特征选择过程与模型的建立过程融为一体，同时完成。\nL1范数符合拉普拉斯分布，L2范数符合高斯分布，怎么理解？\n5 网络梯度优化下降方法\n5.1 SGD\nSGD一般都指mini-batch gradient descent。 SGD就是每一次迭代计算mini-batch的梯度，然后对参数进行更新，是最常见的优化方法了。即： \n其中，，是梯度 SGD完全依赖于当前batch的梯度，所以\n缺点:\n\n选择合适的learning rate比较困难 - 对所有的参数更新使用同样的learning rate。对于稀疏数据或者特征，有时我们可能想更新快一些对于不经常出现的特征，对于常出现的特征更新慢一些，这时候SGD就不太能满足要求了\nSGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点;\n\n5.2 Momentum\nmomentum是模拟物理里动量的概念，积累之前的动量来替代真正的梯度。公式如下：\n\n特点： 下降初期时，使用上一次参数更新，下降方向一致，乘上较大的μ能够进行很好的加速 下降中后期时，在局部最小值来回震荡的时候，gradient-&gt;0，μ使得更新幅度增大，跳出陷阱 在梯度改变方向的时候，μ能够减少更新 总而言之，momentum项能够在相关方向加速SGD，抑制振荡，从而加快收敛\n5.3 Adam\nAdam(Adaptive Moment Estimation)本质上是带有动量项的RMSprop，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。公式如下：\n\n特点：\n结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点 对内存需求较小 为不同的参数计算不同的自适应学习率 也适用于大多非凸优化 - 适用于大数据集和高维空间\n5.4 经验之谈\n对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值 SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠 如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。 Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。 在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果\n5.5 一些问题\nL2正则=Weight Decay？并不是这样\n使用Adam优化带L2正则的损失并不有效。如果引入L2正则项，在计算梯度的时候会加上对正则项求梯度的结果。那么如果本身比较大的一些权重对应的梯度也会比较大，由于Adam计算步骤中减去项会有除以梯度平方的累积，使得减去项偏小。按常理说，越大的权重应该惩罚越大，但是在Adam并不是这样。而权重衰减对所有的权重都是采用相同的系数进行更新，越大的权重显然惩罚越大。在常见的深度学习库中只提供了L2正则，并没有提供权重衰减的实现。这可能就是导致Adam跑出来的很多效果相对SGD with Momentum偏差的一个原因。\n牛顿法和梯度下降法有什么不同？\n6. 模型相关计算\n输入L* L，卷积核k* k，步长s，padding p，输出尺寸，Flops计算\nL1 = (L-k+2*p)/s + 1 flops = k * k * c1 * c2 * L1 * L1\n网络大小指标：参数量，flops，乘加数\n参考 * FLOPS: 注意全部大写 是floating point of per second的缩写，意指每秒浮点运算次数。可以理解为计算速度，用来衡量硬件的性能。 * FLOPs: 是floating point of operations的缩写，是浮点运算次数，理解为计算量，可以用来衡量算法/模型复杂度。\n卷积层:\n +1表示bias  表示一次卷积操作中的加法运算量，+ 1 表示bias， 上面是乘运算和加运算的总和，将一次乘运算或加运算都视作一次浮点运算。 在计算机视觉论文中，常常将一个'乘-加'组合视为一次浮点运算，英文表述为'Multi-Add'，运算量正好是上面的算法减半，此时的运算量为： \n全连接层: params=(I+1)*O=I*O+O 每一层神经元(O这一层)的权重数为I×O，bias数量为O。 FLOPs=[I+(I-1)+1]*O=2*I*O 第一个I表示乘法运算量， I-1表示加法运算量，+1表示bias， *O表示计算O个神经元的值。\n7. Loss相关\n7.1 Cross Entropy &amp;&amp; MSE\n\n均方差损失函数（MSE） 简单来说，均方误差（MSE）的含义是求一个batch中n个样本的n个输出与期望输出的差的平方的平均值、\nCross-entropy（交叉熵损失函数) 交叉熵是用来评估当前训练得到的概率分布与真实分布的差异情况。 它刻画的是实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近。\n\n\n\n来源\n\n7.1.2 分类为何不用MSE\n分类问题，最后必须是 one hot 形式算出各 label 的概率， 然后通过 argmax 选出最终的分类。 在计算各个 label 概率的时候，用的是 softmax 函数。如果用 MSE 计算 loss， 输出的曲线是波动的，有很多局部的极值点。 即，非凸优化问题 (non-convex) cross entropy 计算 loss，则依旧是一个凸优化问题，用梯度下降求解时，凸优化问题有很好的收敛特性。 &gt; 当梯度很小的时候，应该减小步长（否则容易在最优解附近产生来回震荡），但是如果采用 MSE ，当梯度很小的时候，无法知道是离目标很远还是已经在目标附近了。（离目标很近和离目标很远，其梯度都很小）----知乎\n7.1.3 交叉熵为什么有log项\n不带log，对p的求导处处为1； log(p)，当p接近1时（接近目标），导数小，接近0时，导数大 带log可以使得优化时更偏重于离目标远的那些p，而非同等对待。\n7.1.4 熵、交叉熵、KL散度的关系\n交叉熵其实只是KL散度的一部分\n\n其中p代表label或者叫groundtruth，q代表预测值.在机器学习中，我们需要评估label和predicts之间的差距，使用KL散度刚刚好，即： \n由于KL散度中的前一部分恰巧就是p的熵，p代表label或者叫groundtruth，故−H(p(x))不变，故在优化过程中，只需要关注DKL()的后一部分：交叉熵(H(p, q)) 就可以了。\n7.2 L1 loss, L2 loss, smooth L1 loss\n出处 &gt; 对于大多数CNN网络，我们一般是使用L2-loss而不是L1-loss，因为L2-loss的收敛速度要比L1-loss要快得多\n对于边框预测回归问题，通常也可以选择平方损失函数（L2损失），但L2范数的缺点是当存在离群点（outliers)的时候，这些点会占loss的主要组成部分。比如说真实值为1，预测10次，有一次预测值为1000，其余次的预测值为1左右，显然loss值主要由1000主宰。所以FastRCNN采用稍微缓和一点绝对损失函数（smooth L1损失），它是随着误差线性增长，而不是平方增长。\n\n\n\n\n\n\n\n\n\n注意：smooth L1和L1-loss函数的区别在于，L1-loss在0点处导数不唯一，可能影响收敛。smooth L1的解决办法是在0点附近使用平方函数使得它更加平滑。\n\n\n\n\n\n\n\n\n\nsmooth L1 loss并不是为了解决L1-loss在0点处导数不唯一，毕竟直接定义L1 loss在0处的导数为0就行了。更像是，当预测值与标签之差小于1时，smooth l1的导数更小，使得loss收敛的更加稳定，更容易收敛到局部最优，而不会跳出局部最优。\n公式： \n\n\n\n\n来源\n\n\n\n\n\n\n\n\n\n\nsmooth L1 loss让loss对于离群点更加鲁棒，即：相比于L2损失函数，其对离群点、异常值（outlier）不敏感，梯度变化相对更小，训练时不容易跑飞。\ncos距离和l2什么情况下相等?\nL2归一化后欧拉距离的平方与cosine相似度的关系为; &gt; L2归一化就是对向量的每一个值都除以向量的平方和的开方\n\n7.3 focal loss\n来源知乎\nFocal Loss的引入主要是为了解决难易样本数量不平衡（注意，有区别于正负样本数量不平衡）的问题，实际可以使用的范围非常广泛，为了方便解释，还是拿目标检测的应用场景来说明： 单阶段的目标检测器通常会产生高达100k的候选目标，只有极少数是正样本，正负样本数量非常不平衡。我们在计算分类的时候常用的损失——交叉熵的公式如下： \n为了解决正负样本不平衡的问题，我们通常会在交叉熵损失的前面加上一个参数，即： 公式（2） \n但这并不能解决全部问题。根据正、负、难、易，样本一共可以分为以下四类：\n\n\n\n\n难\n易\n\n\n\n\n正\n正难\n正易\n\n\n负\n负难\n负易\n\n\n\n尽管平衡了正负样本，但对难易样本的不平衡没有任何帮助。而实际上，目标检测中大量的候选目标都是易分样本。 这些样本的损失很低，但是由于数量极不平衡，易分样本的数量相对来讲太多，最终主导了总的损失。而本文的作者认为，易分样本（即，置信度高的样本）对模型的提升效果非常小，模型应该主要关注与那些难分样本（这个假设是有问题的，是GHM的主要改进对象）\n这时候，Focal Loss就上场了！ 一个简单的思想：把高置信度(p)样本的损失再降低一些不就好了吗！ 公式（3） \n举个例， γ取2时，如果p=0.968, (1-0.968)^2=0.001，损失衰减了1000倍！\nFocal Loss的最终形式结合了上面的公式（2）. 这很好理解，公式(3)解决了难易样本的不平衡，公式(2)解决了正负样本的不平衡，将公式（2）与（3）结合使用，同时解决正负难易2个问题！\n最终的Focal Loss形式如下：\n\n\n\n\n\n\n\n\n\n\n实验表明α=0.25, γ=2的时候效果最佳。注意在作者的任务中，正样本是属于少数样本，也就是说，本来正样本难以“匹敌”负样本，但经过 (1−ŷ)^γ 和 ŷ^γ 的“操控”后，也许形势还逆转了，还要对正样本降权。\n7.4 人脸识别loss\n出处\n7.4.1 Softmax Loss\n见3.2节\n7.4.2 Center Loss\n对MNIST数据集进行分类，若损失函数采用上述介绍的Softmax Loss(因为Softmax Loss能够使特征可分)，那么最后每个类别数字学出来的特征分布下图，我们可以看出类间距离还是比较小，类内距离比较大的，虽然效果很好：\n\n如果损失函数采用Center Loss，那么特征分布如下图，我们可以看出相比于Softmax Loss, 类间距离变大了，类内距离变小了：\n\n所以我们可以看出Center Loss能够最小化类内距离的同时保证特征可分，来提高特征之间的可判别性！简单地说，给每一类(lable)定义一个类中心(Center)，同一类的数据向类中心靠近，离得远要惩罚！于是Center Loss就出现了。\n\n其中 表示这个样本所对应的第类别的特征中心， m表示每一个batch大小。上述公式的意义是：希望batch中的每个样本特征距离特征中心的距离的平方和越小越好，也就是负责类内差距。\n那么上述的每一batch怎么确定的呢？理想情况下，需要随着学习到的feature进行实时更新，也就是在每一次迭代的时候用整个数据集的feature来计算每个类的中心。但是这样时间复杂度高，于是：用 batch来更新enter，每一轮计算一下当前batch数据与center的距离，然后这个距离以梯度的形式叠加到center上。 我们下面对求导：\n\n这里因为每个 batch的数量m太小，那么每次更新center可能会引起抖动。那么梯度上面加个限制，这个值在0-1之间：\n△\n为了最小化类内，最大化类间，即满足特征可分和特征可判别，论文中将Softmax Loss和Center Loss结合。\n\n7.4.3 A-Softmax Loss\n7.4.4 L-Softmax Loss\n7.4.5 CosFace Loss\n7.4.6 AM-Softmax Loss\n7.4.7 ArcFace/Insight Face\n7.4.8 Triplet Loss\n7.5 Gan loss\n8. 模型结构\n8.1 为什么resnet效果会那么好\n出处 1. 使网络更容易在某些层学到恒等变换（identity mapping）。在某些层执行恒等变换是一种构造性解，使更深的模型的性能至少不低于较浅的模型。这也是作者原始论文指出的动机。Deep Residual Learning for Image Recognition 2. 残差网络是很多浅层网络的集成（ensemble），层数的指数级那么多。主要的实验证据是：把 ResNet 中的某些层直接删掉，模型的性能几乎不下降。 Residual Networks Behave Like Ensembles of Relatively Shallow Networks 3. 残差网络使信息更容易在各层之间流动，包括在前向传播时提供特征重用，在反向传播时缓解梯度信号消失。原作者在一篇后续文章中给出了讨论。Identity Mappings in Deep Residual Networks\n\n\n\n\n\n\n\n\n\nhigh way(当门为1的时候，全部输出原x，不用激活。) 比较: https://blog.csdn.net/qq_27009517/article/details/84028568\nmobilenet shuffle net\n胶囊网络\nSE模块\n9. 实际训练相关\n9.1 学习过程中学习曲线产生振荡，可能原因？\n\n训练的batch_size太小\n\nbatch数太小，而类别又比较多的时候，可能会导致loss函数震荡而不收敛，尤其是在网络比较复杂的时候。\n随着batchsize增大，处理相同的数据量的速度越快。\n随着batchsize增大，达到相同精度所需要的epoch数量越来越多。\n由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。\n过大的batchsize的结果是网络很容易收敛到一些不好的局部最优点。同样太小的batch也存在一些问题，比如训练速度很慢，训练不容易收敛等。\n体的batch size的选取和训练集的样本数目相关\n\n\n\n\n\n\n\n\n\n\n\n一定范围内，batchsize越大，其确定的下降方向就越准，引起训练震荡越小。 batchsize增大到一定的程度，其确定的下降方向已经基本不再变化。一味地增加batch size就好，太大的batch size 容易陷入sharp minima，泛化性不好\n\n学习率：学习率太大，一步前进的路程太长，会出现来回震荡的情况，但是学习率太小，收敛速度会比较慢。\n是否找到合适的loss函数：在深度学习里面，不同的loss针对的任务是有不同的，有些loss函数比较通用例如L1/L2等，而如perceptual loss则比较适合在图像恢复/生成领域的任务上。当loss出现问题的适合，想一想，是不是loss设置的有问题，别人在此领域的任务的方法是否也使用和你一样的loss。\n是否使用合适的激活函数：一般来说，都几乎使用RELU作为全局激活函数，尽可能少的使用sigmoid激活函数（激活范围太小），容易造成梯度弥散、消失\n是否选择合适的优化算法：一般来说，我都使用Adam作为优化器（默认参数）。如果经过仔细调整的SGD算法性能可能更好，但是时间上不太允许这样做。\n\n9.2 loss nan原因\n\n如果在迭代的100轮以内，出现NaN，一般情况下的原因是因为你的学习率过高，需要降低学习率。可以不断降低学习率直至不出现NaN为止，一般来说低于现有学习率1-10倍即可。\n如果当前的网络是类似于RNN的循环神经网络的话，出现NaN可能是因为梯度爆炸的原因，一个有效的方式是增加“gradient clipping”（梯度截断来解决）\n可能用0作为了除数;\n可能0或者负数作为自然对数\n需要计算loss的数组越界（尤其是自己，自定义了一个新的网络，可能出现这种情况）\n在某些涉及指数计算，可能最后算得值为INF（无穷）（比如不做其他处理的softmax中分子分母需要计算exp（x），值过大，最后可能为INF/INF，得到NaN，此时你要确认你使用的softmax中在计算exp（x）做了相关处理（比如减去最大值等等））\n\n9.3 loss不降/网络不收敛怎么办，valid loss不降（过拟合）怎么办\n训练集loss不下降： 出处 1. 模型结构和特征工程存在问题 2. 权重初始化方案有问题 &gt; 建议无脑xaiver normal初始化或者 he normal 3. 正则化过度 L1 L2和Dropout是防止过拟合用的，当训练集loss下不来时，就要考虑一下是不是正则化过度，导致模型欠拟合了。 4. 选择合适的激活函数、损失函数 5. 选择合适的优化器和学习速率 6. 模型训练遇到瓶颈 这里的瓶颈一般包括：梯度消失、大量神经元失活、梯度爆炸和弥散、学习率过大或过小等。 7. batch size过大 batch size过小，会导致模型后期摇摆不定，迟迟难以收敛，而过大时，模型前期由于梯度的平均，导致收敛速度过慢。 8. 数据集未打乱，数据集问题，未归一化\nvalid loss不降（过拟合）：\n\nRegularization(正则化) 权重衰减(Weight Decay)。到训练的后期，通过衰减因子使权重的梯度下降地越来越缓。\n\n\nBatch Normalization\nDropout\nL1 , L2\n\n\n调整网络结构 过拟合很重要的一个原因也是模型的复杂度太高，除了正则化手段以外，适当减小模型的规模也是很重要的，尽量让神经网络结构的假设空间与预期目标模型需要存储的信息量相匹配。\n增大训练数据量 这是终极解决方案，深度学习就是在有大量数据的基础上发展起来的。深度学习的三件套：数据、模型和硬件。模型可以直接拿来用，硬件可以花钱买，但是数据需要一点一点去收集，而且很多问题的解决就依赖于大量的数据，没数据就没有一切。\nEarly Stopping(早停法) (详细解释)早停法将数据分成训练集和验证集，训练集用来计算梯度、更新权重和阈值，验证集用来估计误差，若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值。\n\n9.4 判断过拟合，欠拟合\n出处\n\n\n\n\n\n\n\n\n\n学习曲线就是通过画出不同训练集大小时训练集和交叉验证的准确率，可以看到模型在新数据上的表现，进而来判断模型是否方差偏高或偏差过高，以及增大训练集是否可以减小过拟合。\n\n欠拟合: 当训练集和测试集的误差收敛但却很高时，为高偏差。 左上角的偏差很高，训练集和验证集的准确率都很低，很可能是欠拟合。 我们可以增加模型参数，比如，构建更多的特征，减小正则项。 此时通过增加数据量是不起作用的。\n过拟合: 当训练集和测试集的误差之间有大的差距时，为高方差。 当训练集的准确率比其他独立数据集上的测试结果的准确率要高时，一般都是过拟合。 右上角方差很高，训练集和验证集的准确率相差太多，应该是过拟合。 我们可以增大训练集，降低模型复杂度，增大正则项，或者通过特征选择减少特征数。\n\n\n\n\n\n\n\n\n\n理想情况是是找到偏差和方差都很小的情况，即收敛且误差较小。\n9.5 梯度消失和梯度爆炸，哪些函数会导致梯度消失\n\n\n\n\n\n\n\n\n\n梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。\n其实梯度爆炸和梯度消失问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。对于更普遍的梯度消失问题，可以考虑用ReLU激活函数取代sigmoid激活函数。另外，LSTM的结构设计也可以改善RNN中的梯度消失问题。\n\n\n\n\n\n\n\n\n\n哪些函数会导致梯度消失? sigmoid\n解决方法： 1. 好的参数初始化方式，如He初始化 2. 非饱和的激活函数（如 ReLU） 3. 批量规范化（Batch Normalization） 4. 梯度截断（Gradient Clipping） 5. 更快的优化器 6. LSTM\n9.6 cnn中网络权重初始化为0有什么问题，为什么不能，全部初始化为常数呢\n\n\n\n\n\n\n\n\n\n结论：在训练神经网络的时候，权重初始化要谨慎，不能初始化为0\n参考 如果神经元的权重被初始化为0， 在第一次更新的时候，除了输出之外，所有的中间层的节点的值都为零。一般神经网络拥有对称的结构，那么在进行第一次误差反向传播时，更新后的网络参数将会相同，在下一次更新时，相同的网络参数学习提取不到有用的特征，因此深度学习模型都不会使用0初始化所有参数。\n\n模型所有权重w初始化为0，所有偏置b初始化为0 简单来说，就会出现同一隐藏层所有神经元的输出都一致，对于后期不同的batch，每一隐藏层的权重都能得到更新，但是存在每一隐藏层的隐藏神经元权重都是一致的，多个隐藏神经元的作用就如同1个神经元。\n模型所有权重w初始化为0，所有偏置b随机初始化 这种方式存在更新较慢、梯度消失、梯度爆炸等问题，在实践中，通常不会选择此方式。\n模型所有的权重w随机初始化，所有偏置b初始化为0 在反向传播的过程中所有权重的导数都不相同，所以权重和偏置b都能得到更新。\n\n9.7 初始化方法，He解决了啥\n参考\nXavier Xavier初始化将一个层的权重设置为从一个有界的随机均匀分布中选择的值\n\n其中，是传入网络连接的数量叫“扇入”，是从那层出去的网络连接的数量，也被称为“扇出”。\nHe\n\n\n\n\n\n\n\n\n\n探索如何用类relu的激活函数在网络中最好地初始化权重是kobjective He等人，提出他们自己的初始化方案的动机，这是为使用这些非对称、非线性激活的深层神经网络量身定制的。\n方法： 1. 为给定层上的权值矩阵创建一个张量，并用从标准正态分布中随机选择的数字填充它。 2. 将每个随机选择的数字乘以√2/√n，其中n是从上一层的输出(也称为“扇入”)进入给定层的连接数。 3. 偏置张量初始化为零。\n9.8 反向传播时，同一个mini-batch在共享卷积层的末端是否需要除以batch-size？\n个人觉得不需要。\n由于目前主流深度学习框架处理mini-batch的反向传播时，默认都是先将每个mini-batch中每个instance得到的loss平均化之后再反求梯度，也就是说每次反向传播的梯度是对mini-batch中每个instance的梯度平均之后的结果。\n9.9 偏置和方差区别，模型比较复杂时，偏置和方差变化：偏置变小，方差变大\n\n\n\n\n\n\n\n\n\n模型复杂度增加时，模型预测的方差会增大，偏差会减小\n出处\n\n偏差(Bias)：在不同训练集上训练得到的所有模型的平均性能和最优模型的差异，可以用来衡量模型的拟合能力。\n方差(Variance)：在不同的训练集上训练得到的模型之间的性能差异，表示数据扰动对模型性能的影响，可以用来衡量模型是否容易过拟合，即模型的泛化能力。\n\n所以，当模型的复杂度增加时，模型的拟合能力得到增强，偏差便会减小，但很有可能会由于拟合“过度”，从而对数据扰动更加敏感，导致方差增大。从模型评价上来看，模型复杂度增加后，出现验证集效果提升，但是测试集效果下降的现象。\n9.10 多卡训练，梯度为什么平均\n出处 单GPU情况下，也要对一个 batch 中 每个 data sample 的gradient 求平均。现在回到多卡的情况，相当于把一个 batch 分到多个卡上去跑，仍然希望在这个 batch 内求平均。现在假设我们一个 batch 有 20 的 data sample，本来单卡我们可以直接对这 20 个 data sample 求平均，现在加入我们有 5 个卡，那么每个卡跑 4 个 data sample，那么我们可以先 4 个 4 个的求平均，然后把得到的 5 个均值求平均，得到的结果和直接对 20 个 data sample 求平均是一样的。\n9.11 蒸馏\n详见\n9.12 怎么做模型压缩？\n详见\n9.1x int8量化和卷积加速的方式\nfft，winograd，im2col+sgemm 详见\n10. 分类\n细粒度分类怎么训练，陷入局部最优值怎么办，bilinear CNN\n零样本分类问题，如果测试出现一个图片是训练时没有的，怎么做\nhttps://blog.csdn.net/tianguiyuyu/article/details/81948700 不用那么麻烦，我之前做ins用户分类的时候也遇到过类似的问题不过我用的是向量化文本数据之后上lightgbm测试，训练集大概有十几个类别包括厨师，宠物，画家，摄影师等，测试的时候发现了测试集中出现了“黑人说唱歌手”这个群体训练集中没有的群体，但是因为模型已经训练出了十几个分类的结构了所以最终黑人说唱歌手被预测为宠物，后来想了一个办法就是把多分类问题转化为多个二分类问题，然后设定预测概率超过比如0.8的时候判定为某个类别否则判定为其它类，这样就可以解决问题了，黑人说唱歌手最终因为没有达到预先设定的阈值所以没有被判定为任何类别。\nattention怎么做，self-attention怎么做，原理是什么，为什么有效，multi attention（多头），attention里面的QKV是啥\nsoft attention 和 hard attention\nencoder-decoder中，如果decoder基于attention应该怎么做\n手写代码\n卷积 maxpooling nms，softnms iou，各种滤波，softmax，实现one-got特征， mAP\nPCA\n高斯卷积核如何优化\n积分图均值滤波\n亚像素卷积，一般上采样会遇到哪些问题\n3. 目标检测\nOHEM，正负样本比例\nroi pooling和roi align区别，怎么做插值，线性和spline，插值公式\nresnet101不适合做检测\n检测类别冲突\nnms耗时，时间复杂度，会有多少框\n类别不均衡\nhttps://blog.csdn.net/weixin_35653315/article/details/78327408#commentsedit #### 类别不均衡用resnet还说inception好，细粒度分类呢 #### cascade rcnn #### 目标检测有一个类别ap很低 #### CBAM实际分类中和senet表现差不多，可能原因？ #### one stage 和 two stage，为什么慢，为什么精度高，one stage在哪些方面精度低 ROI + default box的理解 #### one stage对负样本太多的解决办法，focal loss等等，anchor正负比例 #### one stage中的anchor预测出的位置是有偏移量的，而类别置信度是针对原始位置的特征，并不是偏移后的，有什么解决办法 #### 什么场景适合检测，什么场景适合分隔 #### Faster RCNN，anchor，bbox对应，roi pooling align，正负样本怎么选择，rpn，rpn能不能多分类 #### maskrcnn #### 检测出现一半的物体处理方式 #### 小目标 #### 如何解决类内检测 #### 检测的框偏移45度，怎么处理 https://zhuanlan.zhihu.com/p/150780620 #### 几种iou #### roi pooling和roi align区别，怎么做插值，线性和spline，插值公式 #### SPP，FPN，PAN，FPN特征融合为什么是相加，FPN多尺度层中的box如何进行roi pooling\n","slug":"CV面试基础总结","date":"2021-10-12T07:57:22.000Z","categories_index":"","tags_index":"面试基础","author_index":"Hulk Wang"},{"id":"06a101cf6ba6fad0463c3458164de1c3","title":"Faster RCNN 记录","content":"\n\n\n\n\n\n\n\n\n参考来源： https://zhuanlan.zhihu.com/p/31426458 https://zhuanlan.zhihu.com/p/86403390\n\n\nFaster RCNN基本结构\n\nFaster RCNN其实可以分为4个主要内容：\n\n特征提取：Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。\nRPN：RPN网络用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。\nRoi Pooling：该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。\nClassification：利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。\n\n\n\nFaster RCNN(VGG16)网络结构\n\n特征提取\n没啥可说的，特征提取，输入MxN，VGG16为例，4个pooling层，得到的feature map分辨率为：（M/16）x（N/16）\nRPN\n\n\nRPN结构\n\nRPN网络实际分为2条线，上面一条通过softmax分类anchors获得positive和negative分类，下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合positive anchors和对应bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。\nanchors\n以tf代码为例： def _anchor_component(self):  #获得锚的数量和位置\n    with tf.variable_scope('ANCHOR_' + 'default'):\n        # just to get the shape right  只是为了让形状正确\n        height = tf.to_int32(tf.ceil(self._im_info[0, 0] / np.float32(self._feat_stride[0]))) #高度为图片高/16,，就是特征图的高，tf.ceil向上取整\n        width = tf.to_int32(tf.ceil(self._im_info[0, 1] / np.float32(self._feat_stride[0]))) #宽度为图片宽/16，为特征图的宽\n        anchors, anchor_length = tf.py_func(generate_anchors_pre,\n                                            [height, width,                                        self._feat_stride,self._anchor_scales, self._anchor_ratios],\n                                            [tf.float32, tf.int32], name=\"generate_anchors\")  #构建生成锚的py函数，这个锚有9*（50*38）个，anchor_length是锚的个数\n        anchors.set_shape([None, 4])  #锚定义为4列\n        anchor_length.set_shape([]) #行向量，length为锚的个数\n        self._anchors = anchors\n        self._anchor_length = anchor_length  #length为特征图面积*9 函数generate_anchors_pre来生成anchor：\ndef generate_anchors_pre(height, width, feat_stride, anchor_scales=(8, 16, 32), anchor_ratios=(0.5, 1, 2)):\n    \"\"\" A wrapper function to generate anchors given different scales\n      Also return the number of anchors in variable 'length' 给定不同比例生成锚点的包装函数也返回可变“长度”的锚点数量\n    \"\"\"\n    anchors = generate_anchors(ratios=np.array(anchor_ratios), scales=np.array(anchor_scales))\n    A = anchors.shape[0] #anchor的数量，为9\n    shift_x = np.arange(0, width) * feat_stride  #将特征图的宽度进行16倍延伸至原图，以width=4为例子，则shfit_x=[0,16,32,48]\n    shift_y = np.arange(0, height) * feat_stride  #将特征图的高度进行16倍衍生至原图\n    shift_x, shift_y = np.meshgrid(shift_x, shift_y)  #生成原图的网格点\n    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel())).transpose()  #若width=50，height=38，生成（50*38）*4的数组\n    #如 [[0,0,0,0],[16,0,16,0],[32,0,32,0].......]，shift中的前两个坐标和后两个一样（保持右下和左上的坐标一样），是从左到右，从上到下的坐标点（映射到原图）\n    K = shifts.shape[0]  #k=50*38\n    # width changes faster, so here it is H, W, C\n    anchors = anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2))  #列数相同的list相加就是简单的添加，而数组不一样，1*9*4和（50*38）*1*4进行相加，生成了（50*38）*9*4的数组\n    #其实意思就是右下角坐标和左上角的左边都加上同一个变换坐标\n    anchors = anchors.reshape((K * A, 4)).astype(np.float32, copy=False) #三维变两维，（50*38*9，4），此处就是将特征层的anchor坐标转到原图上的区域\n    length = np.int32(anchors.shape[0])  #length=50*38*9\n\n    return anchors, length\n观察第一行代码，即 anchors = generate_anchors(ratios=np.array(anchor_ratios),scales=np.array(anchor_scales)) 这里是3x3=9个基础anchor的生成，我们进入函数generate_anchors中，发现其实生成就是一个数组，这个数组是： # array([[ -83.,  -39.,  100.,   56.],\n#       [-175.,  -87.,  192.,  104.],\n#       [-359., -183.,  376.,  200.],\n#       [ -55.,  -55.,   72.,   72.],\n#       [-119., -119.,  136.,  136.],\n#       [-247., -247.,  264.,  264.],\n#       [ -35.,  -79.,   52.,   96.],\n#       [ -79., -167.,   96.,  184.],\n#       [-167., -343.,  184.,  360.]]) 其中每行的4个值（x1，y1, x2, y2）表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为{1:1, 1:2, 2:1}三种，实际上通过anchors就引入了检测中常用到的多尺度方法。\n生成这个数组的代码是： def generate_anchors(base_size=16, ratios=[0.5, 1, 2],\n                     scales=2 ** np.arange(3, 6)):\n    \"\"\"\n    Generate anchor (reference) windows by enumerating aspect ratios X\n    scales wrt a reference (0, 0, 15, 15) window.  通过枚举参考(0，0，15，15)窗口的长宽比来生成锚(参考)窗口。\n    \"\"\"\n\n    base_anchor = np.array([1, 1, base_size, base_size]) - 1  #生成一个base_anchor = [0, 0, 15, 15]，其中(0, 0)是anchor左上点的坐标\n    # (15, 15)是anchor右下点的坐标，那么这个anchor的中心点的坐标是(7.5, 7.5)\n    ratio_anchors = _ratio_enum(base_anchor, ratios)#然后产生ratio_anchors，就是将base_anchor和ratios[0.5, 1, 2],ratio_anchors生成三个anchors\n    # 传入到_ratio_enum()函数，ratios代表的是三种宽高比。\n    anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)  #在刚刚3个anchor基础上继续生成anchor\n                         for i in range(ratio_anchors.shape[0])])\n    return anchors 我们发现这个数组中的每行数据（如第一行：[ -83., -39., 100., 56.] )，它们的中心位置都为（7.5，7.5），即（0，0，15，15）的中心（因为通过代码也可以得知，这9个基础框的生成也是以（0，0，15，15）为基础的，代码中base_anchor就是（0，0，15，15））。\n通过3种anchor ratio和3种anchor scale生成的9个数组，这9个数组在坐标图上如图2所示。 \n\nanchor\n\n\n\n\n\n\n\n\n\n\n为什么这样设计呢？我们知道，其实一张图片通过特征提取网路VGG16后，长宽比都缩小了16倍得到了特征图。比如原先的800*600的原图通过VGG16后得到了50*38的特征图（以上先不考虑通道数），我们就假设，特征图上的每一个点（大小为1*1），和原图16*16区域对应（这里记住，对应不是指代感受野，只是便于理解).这里，使用对应的好处，就是特征图上的每个点（大小为1*1）负责由原图对应区域(大小为16*16）中心生成的9个anchor的训练和学习。所以Faster RCNN共产生50x38x9=17100个anchor，基本覆盖了全图各个区域。\n看generate_anchors_pre函数，shifts就是对（shift_x, shift_y）进行组合，其中shift_x是对x坐标进行移动，shift_y是对y坐标进行移动，综合起来就是将基础的中心为（7.5，7.5）的9个anchor平移到全图上，覆盖所有可能的区域。 anchors = anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)) #列数相同的list相加就是简单的添加，而数组不一样，1*9*4和（50*38）*1*4进行相加，生成了（50*38）*9*4的数组\n#其实意思就是右下角坐标和左上角的左边都加上同一个变换坐标\nanchors = anchors.reshape((K * A, 4)).astype(np.float32, copy=False) #三维变两维，（50*38*9，4），此处就是将特征层的anchor坐标转到原图上的区域\nlength = np.int32(anchors.shape[0]) #length=50*38*9 上述代码就是完成了9个base anchor 的移动，输出结果就是50389个anchor。那么到此，所有的anchor都生成了，当然了，所有的anchor也和特征图产生了一一对应的关系了。\n\n\n\n\n\n\n\n\n\nanchor是辅助模型进行训练的，能让模型对物体的大小和形状有个大致的认知，也算是人为添加的先验知识了。\nRPN最终就是在原图尺度上，设置了密密麻麻的候选Anchor。然后用cnn去判断哪些Anchor是里面有目标的positive anchor，哪些是没目标的negative anchor。所以，仅仅是个二分类而已！\n\n解释一下上面这张图的数字。\n\n在原文中使用的是ZF model中，其Conv Layers中最后的conv5层num_output=256，对应生成256张特征图，所以相当于feature map每个点都是256-dimensions\n在conv5之后，做了rpn_conv/3x3卷积且num_output=256，相当于每个点又融合了周围3x3的空间信息\n假设在conv5 feature map中每个点上有k个anchor（默认k=9），而每个anhcor要分positive和negative，所以每个点由256d feature转化为cls=2•k scores；而每个anchor都有(x, y, w, h)对应4个偏移量，所以reg=4•k coordinates\n全部anchors拿去训练太多了，训练程序会在合适的anchors中随机选取128个postive anchors+128个negative anchors进行训练\n\nrpn中的二分类\n一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设 W=M/16，H=N/16。在进入reshape与softmax之前，先做了1x1卷积（此时anchor已经生成完毕），如图9：\n\n可以看到其num_output=18，也就是经过该卷积的输出图像为WxHx18大小,这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是positive和negative，所有这些信息都保存WxHx(9*2)大小的矩阵。\n\n\n\n\n\n\n\n\n\n综上所述，RPN网络中利用anchors和softmax初步提取出positive anchors作为候选区域（另外也有实现用sigmoid代替softmax，输出[1, 1, 9xH, W]后接sigmoid进行positive/negative二分类，原理一样）。\nbounding box regression\n\n\n\n\n\n\n\n\n\n在RPN网络中，进行bbox regression得到的是每个anchor的偏移量。再与anchor的坐标进行调整以后，得到proposal的坐标，经过一系列后处理，比如NMS，top-K操作以后，得到得分最高的前N个proposal传入分类网络。\n如图所示绿色框为飞机的Ground Truth(GT)，红色为提取的positive anchors，即便红色的框被分类器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机。所以我们希望采用一种方法对红色的框进行微调，使得positive anchors和GT更加接近。\n\n对于窗口一般使用四维向量 (x, y, w, h)表示，分别表示窗口的中心点坐标和宽高。对于下图，红色的框A代表原始的positive Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G'，即：\n\n\n给定anchor A=(, , , )和GT=[, , , ]\n寻找一种变换F，使得：F(, , , )= (, , , ),其中(, , , )≈(, , , )\n\n比较简单的思路就是:\n\n观察上面4个公式发现，需要学习的是,,, 这四个变换。当输入的anchor A与GT相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调（注意，只有当anchors A和GT比较接近时，才能使用线性回归模型，否则就是复杂的非线性问题了）。\n接下来的问题就是如何通过线性回归获,,, 了。线性回归就是给定输入的特征向量X, 学习一组参数W, 使得经过线性回归后的值跟真实值Y非常接近，即Y=WX.\n对于该问题，输入X是cnn feature map，定义为Φ；同时还有训练传入A与GT之间的变换量，即,,,。输出是,,,四个变换。那么目标函数可以表示为:\n\n所以在RPN中，bounding box regression通过第二条线完成：\n\n\nRPN中的bbox reg\n\n可以看到经过该卷积输出图像为WxHx36，在caffe blob存储为[1, 4x9, H, W]，这里相当于feature maps每个点都有9个anchors，每个anchors又都有4个用于回归的\nVGG输出(M/16)*(N/16)*256的特征，对应设置 (M/16)*(N/16)*k 个anchors，而RPN输出：\n\n大小为(M/16)*(N/16)*2k的positive/negative softmax分类特征矩阵\n大小为(M/16)*(N/16)*4k的regression坐标回归特征矩阵\n\n恰好满足RPN完成positive/negative分类+bounding box regression坐标回归.\nProposal Layer\nProposal Layer负责综合所有 ,,, 变换量和positive anchors，计算出精准的proposal，送入后续RoI Pooling Layer。\nProposal Layer有3个输入： 1. 分类器结果（positive vs negative anchorsrpn_cls_prob_reshape) 2. bbox reg的 ,,,变换量rpn_bbox_pred 3. im_info； 4. 另外还有参数feat_stride=16，这和图4是对应的。\n首先解释im_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定MxN，im_info=[M, N, scale_factor]则保存了此次缩放的所有信息。然后经过Conv Layers，经过4次pooling变为WxH=(M/16)x(N/16)大小，其中feature_stride=16则保存了该信息，用于计算anchor偏移量。\nProposal Layer forward按照以下顺序依次处理： 1.生成anchors，利用,,, 对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致） 2. 按照输入的positive softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的positive anchors 3. 限定超出图像边界的positive anchors为图像边界，防止后续roi pooling时proposal超出图像边界 4. 剔除尺寸非常小的positive anchors 5. 对剩余的positive anchors进行NMS（nonmaximum suppression） 6. Proposal Layer有3个输入：positive和negative anchors分类器结果rpn_cls_prob_reshape，对应的bbox reg的(e.g. 300)结果作为proposal输出\n之后输出proposal=[x1, y1, x2, y2]，注意，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应MxN输入图像尺度的，这点在后续网络中有用。\n\n\n\n\n\n\n\n\n\nRPN网络结构就介绍到这里，总结起来就是： 生成anchors -&gt; softmax分类器提取positvie anchors -&gt; bbox reg回归positive anchors -&gt; Proposal Layer生成proposals\nRPN生成RoIs\nRPN在自身训练的同时，还会提供RoIs（region of interests）给Fast RCNN（RoIHead）作为训练样本。RPN生成RoIs的过程(ProposalCreator)如下：\n\n对于每张图片，利用它的feature map， 计算 (H/16)× (W/16)×9（大概20000）个anchor属于前景的概率，以及对应的位置参数。\n选取概率较大的12000个anchor\n利用回归的位置参数，修正这12000个anchor的位置，得到RoIs\n利用非极大值（(Non-maximum suppression, NMS）抑制，选出概率最大的2000个RoIs\n\n注意：在inference的时候，为了提高处理速度，12000和2000分别变为6000和300.\n注意：这部分的操作不需要进行反向传播，因此可以利用numpy/tensor实现。\nROI pooling\nRoI Pooling层则负责收集proposal，并计算出proposal feature maps，送入后续网络。从图2中可以看到Rol pooling层有2个输入：\n\n原始的feature maps\nRPN输出的proposal boxes（大小各不相同）\n\n为何需要RoI Pooling\n当网络训练好后输入的图像尺寸必须是固定值，同时网络输出也是固定大小的vector or matrix。如果输入图像大小不定，这个问题就变得比较麻烦。有2种解决办法：\n\n从图像中crop一部分传入网络\n将图像warp成需要的大小后传入网络\n\n无论采取那种办法都不好，要么crop后破坏了图像的完整结构，要么warp破坏了图像原始形状信息。\nRPN网络生成的proposals的方法：对positive anchors进行bounding box regression，那么这样获得的proposals也是大小形状各不相同，即也存在上述问题。所以Faster R-CNN中提出了RoI Pooling解决这个问题.\nRoI Pooling layer forward\n\n由于proposal是对应MxN尺度的，所以首先使用spatial_scale参数将其映射回(M/16)x(N/16)大小的feature map尺度；\n再将每个proposal对应的feature map区域水平分为 pooled_w*_h的网格；\n对网格的每一份都进行max pooling处理。\n\n这样处理后，即使大小不同的proposal输出结果都是pooled_w*_h固定大小，实现了固定长度输出。\n为什么要pooling成7×7的尺度？\n是为了能够共享权重。在之前讲过，除了用到VGG前几层的卷积之外，最后的全连接层也可以继续利用。当所有的RoIs都被pooling成（512×7×7）的feature map后，将它reshape 成一个一维的向量，就可以利用VGG16预训练的权重，初始化前两层全连接。最后再接两个全连接层，分别是：\nFC 21 用来分类，预测RoIs属于哪个类别（20个类+背景） FC 84 用来回归位置（21个类，每个类都有4个位置参数）\nClassification\nClassification部分利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出cls_prob概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量bbox_pred，用于回归更加精确的目标检测框。Classification部分网络结构如图:\n\n\nClassification部分网络结构\n\n从RoI Pooling获取到7x7=49大小的proposal feature maps后，送入后续网络，可以看到做了如下2件事：\n\n通过全连接和softmax对proposals进行分类，这实际上已经是识别的范畴了\n再次对proposals进行bounding box regression，获取更高精度的rect box\n\nAnchor到底与网络输出如何对应\nVGG输出 50*38*512的特征，对应设置 50*38*k个anchors，而RPN输出50*38*2k的分类特征矩阵和50*38*4k的坐标回归特征矩阵。\n\n其实在实现过程中，每个点的2k个分类特征与 4k回归特征，与 k个anchor逐个对应即可，这实际是一种“人为设置的逻辑映射”。当然，也可以不这样设置，但是无论如何都需要保证在训练和测试过程中映射方式必须一致。\nLoss\n\n上述公式中 i 表示anchors index， 表示positive softmax probability，代表对应的GT predict概率（即当第i个anchor与GT间IoU&gt;0.7，认为是该anchor是positive，=1 ；反之IoU&lt;0.3时，认为是该anchor是negative，=0 ；至于那些0.3&lt;IoU&lt;0.7的anchor则不参与训练）；t 代表predict bounding box，代表对应positive anchor对应的GT box。可以看到，整个Loss分为2部分：\n\n在训练Faster RCNN的时候有四个损失：\n\nRPN 分类损失：anchor是否为前景（二分类）\nRPN位置回归损失：anchor位置微调\nRoI 分类损失：RoI所属类别（21分类，多了一个类作为背景）\nRoI位置回归损失：继续对RoI位置微调\n\n四个损失相加作为最后的损失，反向传播，更新参数。\n### 注意\n1\n\n在RPN的时候，已经对anchor做了一遍NMS，在RCNN测试的时候，还要再做一遍\n在RPN的时候，已经对anchor的位置做了回归调整，在RCNN阶段还要对RoI再做一遍\n在RPN阶段分类是二分类，而Fast RCNN阶段是21分类(voc)\n\n2\nRPN会产生大约2000个RoIs，这2000个RoIs不是都拿去训练，而是选择128个RoIs用以训练。选择的规则如下：\n\nRoIs和gt_bboxes 的IoU大于0.5的，选择一些（比如32个）\n选择 RoIs和gt_bboxes的IoU小于等于0（或者0.1）的选择一些（比如 128-32=96个）作为负样本\n\n为了便于训练，对选择出的128个RoIs，还对他们的gt_roi_loc 进行标准化处理（减去均值除以标准差）\n","slug":"Faster-RCNN-记录","date":"2021-10-11T07:47:29.000Z","categories_index":"","tags_index":"detection","author_index":"Hulk Wang"},{"id":"7f399d762217f0410e81da356daa491f","title":"FCOS学习笔记","content":"FCOS学习笔记\n\n\n\n\n\n\n\n\n\n笔记来源：https://blog.csdn.net/WZZ18191171661/article/details/89258086 https://zhuanlan.zhihu.com/p/339023466\nFCOS是一个基于FCN的per-pixel、anchor free的one-stage目标检测算法，论文全:《FCOS: Fully Convolutional One-Stage Object Detection》\nAnchor-based不足：\n\nanchor会引入很多需要优化的超参数， 比如anchor number、anchor size、anchor ratio等；\n为了保证算法效果，需要很多的anchors，存在正负样本类别不均衡问题；\n在训练的时候，需要计算所有anchor box同ground truth boxes的IoU，计算量较大；\n\nFCOS优势： 1. 因为输出是pixel-based预测，所以可以复用semantic segmentation方向的相关tricks； 2. 可以修改FCOS的输出分支，用于解决instance segmentation和keypoint detection任务；\n\n\nFCOS网络结构\n\n实现细节\n与Anchor Base对比\n对于基于anchors的目标检测算法而言，我们将输入的图片送入backbone网络之后，会获得最终的feature_map，比如说是17x17x256；然后我们会在该feature_map上的每一位置上使用预先定义好的anchors。而FCOS的改动点就在这里，它是直接在feature_map上的每一点进行回归操作。\n具体的实施思路如下所示： 1. 我们可以将feature_map中的每一个点(x,y)映射回原始的输入图片中: (⌊s/2⌋ + xs, ⌊s/2⌋ + ys) 其中: s为步长，(x,y)为改点对应feature map上的坐标.\n\n如果这个映射回原始输入的点在相应的GT的bbox范围之内，而且类别标签对应，我们将其作为训练的正样本块，否则将其作为正样本块；\n回归的目标是(l,t,r,b)，即中心点做bbox的left、top、right和bottom之间的距离，具体如下图所示：\n\n如果一个位置在多个bbox的内部的话，如右图，针对这样样本文中采样的方法是直接选择择面积最小的边界框作为其回归目标。由于网络中FPN的存在，导致这样的模糊样本的数量大大减少。\n如果这个位置(x,y)和一个bbox关联的话，该位置处的训练回归目标可制定为:其中(x1,y1)和(x2,y2)分别表示bbox的左上角和右下角坐标值。\n\n由于FCOS可以通过这样方式获得很多正样本块，使用这样的正样本块进行回归操作，因此获得了比较好的性能提升，而原始的基于anchor的算法需要通过计算预设的anchor和对应的GT之间的IOU值，当该IOU值大于设定的阈值时才将其看做正样本块。\n\nLoss\n\nloss函数如上图所示，包含两部分，Lcls表示分类loss，本文使用的是Focal_loss；Lreg表示回归loss，本文使用的是IOU loss。\ncenter-ness分支\nCenter-ness表示的是(x,y)距目标中心的标准化后的距离，为了制止过多的低质量离目标中心远的检测框而设计。\n\n\n如上图，红色到蓝色表示center-ness从1到0，因为center-ness是在0-1之间，所以用的BCE loss，这个loss会一起加到上面我们提到的loss function中。在测试时，检测框的排序分数由center-ness乘上分类的分数。如果还有低质量的框，最后可用NMS来剔除。\n","slug":"FCOS学习笔记","date":"2021-09-13T08:19:35.000Z","categories_index":"","tags_index":"detection","author_index":"Hulk Wang"},{"id":"57d3aa7cdcc56700d35ee7aa4e8c6558","title":"YOLO-V5学习笔记","content":"YOLO-V5学习笔记\n\n\n\n\n\n\n\n\n\n知识点来源于网络，仅记录学习 来源：https://zhuanlan.zhihu.com/p/172121380\n网络结构\n\n\nYolov5s网络结构(来源见水印)\n\n如上图为yolov5的整体网络结构，跟yolov4一样，分别按input、backbone、Neck以及Prediction四部分来理解。\n\n\n\n\n\n\n\n\n\nYolov5官方代码中，一共有4个版本，分别是Yolov5s、Yolov5m、Yolov5l、Yolov5x四个模型。Yolov5s是Yolov5系列中深度最小，特征图的宽度最小的网络。后面的3种都是在此基础上不断加深，不断加宽。\n输入端\nMosaic数据增强\n与v4一样，采用Mosaic数据增强；\n自适应锚框计算\n将anchor初始计算(聚类)集成到训练代码中；\n自适应图片缩放\n针对inference阶段的优化\n在常用的目标检测算法中，不同的图片长宽都不相同，因此常用的方式是将原始图片统一缩放到一个标准尺寸，再送入检测网络中。\n\n\n传统方法(来源见水印)\n\n但Yolov5代码中对此进行了改进，也是Yolov5推理速度能够很快的一个不错的trick。\n作者认为，在项目实际使用时，很多图片的长宽比不同，因此缩放填充后，两端的黑边大小都不同，而如果填充的比较多，则存在信息冗余，影响推理速度。\n因此在Yolov5的代码中datasets.py的letterbox函数中进行了修改，对原始图像自适应的添加最少的黑边。\n\n\nyolov5(来源见水印)\n\n举例说明填充方法： 原始：800x600 目标：416\n\n选择小的缩放系数，短边:min(416/800, 416/600);\n得到新的尺寸(即长边resize到目标尺寸,短边按原始长宽比变换）: (416,312);\n计算pad大小(找到大于312且能被32整除的最小整数): (416 - 312) mod 32 = 8 所以pad值为8/2=4\n\nBackbone\nFocus结构\n\n\nfocus(来源见水印)\n\nyolov5中，Focus模块位于backbone前。具体操作是在一张图片中每隔一个像素拿到一个值，类似于邻近下采样，这样就拿到了四张近似下采样的图片，但是没有信息丢失。相当于w,h变为1/2，输入通道扩充了4倍，最后将得到的新图片再经过卷积操作，最终得到了没有信息丢失情况下的二倍下采样特征图。\n以yolov5s为例，原始的640 × 640 × 3的图像输入Focus结构，采用切片操作，先变成320 × 320 × 12的特征图，再经过一次卷积操作，最终变成320 × 320 × 32的特征图。\n具体代码实现：\n\n目的和作用： Focus是为了提速，和mAP无关，减少了计算量和参数量。\nThe YOLOv5 Focus layer replaces the first 3 YOLOv3 layers with a single layer:\n\n详见作者解答\nCSP结构\nYolov5与Yolov4不同点在于，Yolov4中只有主干网络使用了CSP结构。\n而Yolov5中设计了两种CSP结构，以Yolov5s网络为例，CSP1_X结构应用于Backbone主干网络，另一种CSP2_X结构则应用于Neck中。\n\nNeck\n\nYolov5现在的Neck和Yolov4中一样，都采用FPN+PAN的结构.\n如CSP结构中讲到，Yolov5和Yolov4的不同点在于，Yolov4的Neck结构中，采用的都是普通的卷积操作。而Yolov5的Neck结构中，采用借鉴CSPnet设计的CSP2结构，加强网络特征融合的能力。\n\n\nPrediction\nBounding box损失函数\nYolov5: GIOU_Loss Yolov4: CIOU_Loss\nnms非极大值抑制\nYolov4: DIOU_nms Yolov5: 加权nms\n\n\n\n\n\n\n\n\n\nWeighted NMS出现于ICME Workshop 2017《Inception Single Shot MultiBox Detector for object detection》一文中。论文认为Traditional NMS每次迭代所选出的最大得分框未必是精确定位的，冗余框也有可能是定位良好的。那么与直接剔除机制不同，Weighted NMS顾名思义是对坐标加权平均，加权平均的对象包括M自身以及IoU≥NMS阈值的相邻框。  加权的权重为 :   ，表示得分与IoU的乘积。 优点： Weighted NMS通常能够获得更高的Precision和Recall，只要NMS阈值选取得当，Weighted NMS均能稳定提高AP与AR，无论是AP50还是AP75，也不论所使用的检测模型是什么。 缺点： 顺序处理模式，且运算效率比Traditional NMS更低。加权因子是IoU与得分，前者只考虑两个框的重叠面积，这对描述box重叠关系或许不够全面；而后者受到定位与得分不一致问题的限制。\n","slug":"YOLO-V5学习笔记","date":"2021-09-09T07:42:37.000Z","categories_index":"","tags_index":"detection","author_index":"Hulk Wang"},{"id":"f45a749a54e27d0a1f22b1f23eadc80a","title":"Pixel-Level Domain Transfer论文复现","content":"Pixel-Level Domain Transfer论文复现\n博客迁移，原文链接\n\n\n\n\n\n\n\n\n\nAbstract.：We present an image-conditional image generation model. The model transfers an input domain to a target domain in semantic level, and generates the target image in pixel level. To generate realistic target images, we employ the real/fake-discriminator as in Generative Adversarial Nets, but also introduce a novel domain-discriminator to make the generated image relevant to the input image. We verify our model through a challenging task of generating a piece of clothing from an input image of a dressed person. We present a high quality clothing dataset containing the two domains, and succeed in demonstrating decent results.\n论文简述\n整篇论文比较容易懂，主要内容就是把输入domain转换到目标domain，输入一张模特图片，得到上衣图片，如下：\n\n文章主要贡献主要在两个方面：\nLookBook数据集\n下载地址（uj3j）\n\n基于Gan的转换框架\n网络结构如下：\n\n生成网络是encoder-decoder结构，判别网络有两个：Dr和Da。\nDr就是一个基本的Gan的判别网络，判别fake或real；Da主要用来判断生成图像与输入是否配对，所以Dr输入是生成网络的输入和输出的concat.\n整个过程很容易懂，细节看原文即可.\n论文复现\nGenerator：\n输入64x64x3图像，输出64x64x3生成图像\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n \n        def conv_block(in_channels, out_channels, kernel_size, stride&#x3D;1,\n                 padding&#x3D;0, bn&#x3D;True, a_func&#x3D;&#39;lrelu&#39;):\n \n            block &#x3D; nn.ModuleList()\n            block.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func &#x3D;&#x3D; &#39;lrelu&#39;:\n                block.append(nn.LeakyReLU(0.2))\n            elif a_func &#x3D;&#x3D; &#39;relu&#39;:\n                block.append(nn.ReLU())\n            else:\n                pass\n \n            return block\n \n        def convTranspose_block(in_channels, out_channels, kernel_size, stride&#x3D;2,\n                 padding&#x3D;0, output_padding&#x3D;0, bn&#x3D;True, a_func&#x3D;&#39;relu&#39;):\n            &#39;&#39;&#39;\n            H_out &#x3D; (H_in - 1) * stride - 2 * padding + kernel_size + output_padding\n            :param in_channels:\n            :param out_channels:\n            :param kernel_size:\n            :param stride:\n            :param padding:\n            :param output_padding:\n            :param bn:\n            :param a_func:\n            :return:\n            &#39;&#39;&#39;\n            block &#x3D; nn.ModuleList()\n            block.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride,\n                 padding, output_padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func &#x3D;&#x3D; &#39;lrelu&#39;:\n                block.append(nn.LeakyReLU(0.2))\n            elif a_func &#x3D;&#x3D; &#39;relu&#39;:\n                block.append(nn.ReLU())\n            else:\n                pass\n \n            return block\n \n \n        def encoder():\n            conv_layer &#x3D; nn.ModuleList()\n            conv_layer +&#x3D; conv_block(3, 128, 5, 2, 2, False)    # 32x32x128\n            conv_layer +&#x3D; conv_block(128, 256, 5, 2, 2)        # 16x16x256\n            conv_layer +&#x3D; conv_block(256, 512, 5, 2, 2)         # 8x8x512\n            conv_layer +&#x3D; conv_block(512, 1024, 5, 2, 2)       # 4x4x1024\n            conv_layer +&#x3D; conv_block(1024, 64, 4, 1)          # 1x1x64\n            return conv_layer\n \n        def decoder():\n            conv_layer &#x3D; nn.ModuleList()\n            conv_layer +&#x3D; conv_block(64, 4 * 4 * 1024, 1, a_func&#x3D;&#39;relu&#39;)\n            conv_layer.append(Reshape((1024, 4, 4)))                            # 4x4x1024\n            conv_layer +&#x3D; convTranspose_block(1024, 512, 4, 2, 1)               # 8x8x512\n            conv_layer +&#x3D; convTranspose_block(512, 256, 4, 2, 1)                # 16x16x256\n            conv_layer +&#x3D; convTranspose_block(256, 128, 4, 2, 1)                # 32x32x128\n            conv_layer +&#x3D; convTranspose_block(128, 3, 4, 2, 1, bn&#x3D;False, a_func&#x3D;&#39;&#39;)     # 64x64x3\n            conv_layer.append(nn.Tanh())\n            return conv_layer\n \n        self.net &#x3D; nn.Sequential(\n            *encoder(),\n            *decoder(),\n        )\n \n    def forward(self, input):\n        out &#x3D; self.net(input)\n        return out\nDiscriminatorR\n输入64x64x3图像，输出real or fake；\nclass DiscriminatorR(nn.Module):\n    def __init__(self):\n        super(DiscriminatorR, self).__init__()\n \n        def conv_block(in_channels, out_channels, kernel_size, stride&#x3D;1,\n                       padding&#x3D;0, bn&#x3D;True, a_func&#x3D;True):\n \n            block &#x3D; nn.ModuleList()\n            block.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func:\n                block.append(nn.LeakyReLU(0.2))\n \n            return block\n \n \n        self.net &#x3D; nn.Sequential(\n            *conv_block(3, 128, 5, 2, 2, False),                            # 32x32x128\n            *conv_block(128, 256, 5, 2, 2),                                 # 16x16x256\n            *conv_block(256, 512, 5, 2, 2),                                 # 8x8x512\n            *conv_block(512, 1024, 5, 2, 2),                                # 4x4x1024\n            *conv_block(1024, 1, 4, bn&#x3D;False, a_func&#x3D;False),                # 1x1x1\n            nn.Sigmoid(),\n        )\n \n    def forward(self, img):\n        out &#x3D; self.net(img)\n        return out\nDiscriminatorA\n输入64x64x6的concat图像，输出real or fake；\nclass DiscriminatorA(nn.Module):\n    def __init__(self):\n        super(DiscriminatorA, self).__init__()\n \n        def conv_block(in_channels, out_channels, kernel_size, stride&#x3D;1,\n                       padding&#x3D;0, bn&#x3D;True, a_func&#x3D;True):\n \n            block &#x3D; nn.ModuleList()\n            block.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func:\n                block.append(nn.LeakyReLU(0.2))\n \n            return block\n \n        self.net &#x3D; nn.Sequential(\n            *conv_block(6, 128, 5, 2, 2, False),                # 32x32x128\n            *conv_block(128, 256, 5, 2, 2),                     # 16x16x256\n            *conv_block(256, 512, 5, 2, 2),                     # 8x8x512\n            *conv_block(512, 1024, 5, 2, 2),                    # 4x4x1024\n            *conv_block(1024, 1, 4, bn&#x3D;False, a_func&#x3D;False),    # 1x1x1\n            nn.Sigmoid(),\n        )\n \n    def forward(self, img):\n        out &#x3D; self.net(img)\n        return out\nloss\n与原文不同，在生成损失上加了mse\ngen_loss_d &#x3D; self.adversarial_loss(torch.squeeze(gen_output), real_label)\ngen_loss_a &#x3D; self.adversarial_loss(torch.squeeze(gen_output_a), real_label)\nmse_loss &#x3D; self.mse_loss(gen_target_batch, target_batch)\n完整训练测试代码：GitHub\n结果\ntensorboard\n\n训练过程可视化\n\n验证集\n\n","slug":"Pixel-Level-Domain-Transfer论文复现","date":"2021-09-06T11:51:46.000Z","categories_index":"","tags_index":"csdn迁移","author_index":"Hulk Wang"},{"id":"260edd383abab8c5258536a42ddb3e2a","title":"YOLO-V4学习笔记","content":"YOLO-V4学习笔记\n\n\n\n\n\n\n\n\n\n知识点来源于网络，仅记录学习 来源：https://zhuanlan.zhihu.com/p/143747206\n网络结构\n\n\n网络结构(来源见水印)\n\n五个基本组件: 1. CBM：Yolov4网络结构中的最小组件，由Conv+Bn+Mish激活函数三者组成。 2. CBL：由Conv+Bn+Leaky_relu激活函数三者组成。 3. Res unit：借鉴Resnet网络中的残差结构，让网络可以构建的更深。 4. CSPX：借鉴CSPNet网络结构，由卷积层和X个Res unint模块Concate组成。 5. SPP：采用1×1，5×5，9×9，13×13的最大池化的方式，进行多尺度融合。\n\n\nObject detector\n\n如上图，大致分为四个阶段理解yolov4，分别为输入端、backbone、Neck以及Prediction。\n输入端\nMosaic数据增强\nYolov4中使用的Mosaic是参考2019年底提出的CutMix数据增强的方式，但CutMix只使用了两张图片进行拼接，而Mosaic数据增强则采用了4张图片，随机缩放、随机裁剪、随机排布的方式进行拼接。\n\n为什么要进行Mosaic数据增强?\n\n\n\n\n\n\n\n\n\n在平时项目训练时，小目标的AP一般比中目标和大目标低很多。而Coco数据集中也包含大量的小目标，但比较麻烦的是小目标的分布并不均匀。\n首先看下小、中、大目标的定义： 2019年发布的论文《Augmentation for small object detection》对此进行了区分:\n\n\n\n\n最小矩形区域面积\n最大矩形区域面积\n\n\n\n\n小目标\n0 * 0\n32 * 32\n\n\n中目标\n32 * 32\n96 * 96\n\n\n大目标\n96 * 96\n∞ * ∞\n\n\n\n\n\n\n\n\n\n\n\n\n可以看到小目标的定义是目标框的长宽0×0~32×32之间的物体。\n\n\n\n\n小\n中\n大\n\n\n\n\n数据集中小目标占比\n41.4%\n34.3%\n24.3%\n\n\n数据集图片包含占比\n52.3%\n70.7%\n83.0%\n\n\n\n但在整体的数据集中，小、中、大目标的占比并不均衡。 如上表所示，Coco数据集中小目标占比达到41.4%，数量比中目标和大目标都要多。\n但在所有的训练集图片中，只有52.3%的图片有小目标，而中目标和大目标的分布相对来说更加均匀一些。\n针对这种状况，Yolov4的作者采用了Mosaic数据增强的方式。\n主要有几个优点：\n\n丰富数据集：随机使用4张图片，随机缩放，再随机分布进行拼接，丰富了图片的背景，大大丰富了检测数据集，特别是随机缩放增加了很多小目标，让网络的鲁棒性更好。\n减少GPU：四张图片拼接在一起变相地提高了batch_size，在进行batch normalization的时候也会计算四张图片，所以对本身batch_size不是很依赖，单块GPU就可以训练YOLOV4。\n\n\n\n\n\n\n\n\n\n\n最后说明一下对于标签框的处理，当进行裁剪的时候，如果裁剪了样本当中的标签框的部分区域，则将其舍弃，保留裁剪之后还完整的标签框。\n参考\n与其他增强方法对比\n\nMixup:将随机的两张样本按比例混合，分类的结果按比例分配；\nCutout:随机的将样本中的部分区域cut掉，并且填充0像素值，分类的结果不变；\nCutMix:就是将一部分区域cut掉但不填充0像素而是随机填充训练集中的其他数据的区域像素值，分类结果按一定的比例分配\n\n\n上述三种数据增强的区别： 1. cutout和cutmix就是填充区域像素值的区别; 2. mixup和cutmix是混合两种样本方式上的区别; 3. mixup是将两张图按比例进行插值来混合样本，cutmix是采用cut部分区域再补丁的形式去混合图像，不会有图像混合后不自然的情形。\n参考\nBackBone\nCSPDarknet53\n\n\n\n\n\n\n\n\n\nCSPDarknet53是在Yolov3主干网络Darknet53的基础上，借鉴2019年CSPNet的经验，产生的Backbone结构。因为Backbone有5个CSP模块（见网络结构），输入图像是608 * 608，所以特征图变化的规律是：608-&gt;304-&gt;152-&gt;76-&gt;38-&gt;19。经过5次CSP模块后得到19*19大小的特征图。 而且作者只在Backbone中采用了Mish激活函数，网络后面仍然采用Leaky_relu激活函数。\nCSPNet\n论文地址\nCSPNet全称是Cross Stage Paritial Network，主要从网络结构设计的角度解决推理中从计算量很大的问题。\n设计CSPNet的主要目的是使该体系结构能够实现更丰富的梯度组合信息，同时减少计算量。 通过将基础层的特征图划分为两个部分，然后通过提出的跨阶段层次结构将它们合并，可以实现此目标。\nCSPNet的作者认为推理计算过高的问题是由于网络优化中的梯度信息重复导致的。\n因此采用CSP模块先将基础层的特征映射划分为两部分，然后通过跨阶段层次结构将它们合并，在减少了计算量的同时可以保证准确率。\n因此Yolov4在主干网络Backbone采用CSPDarknet53网络结构，主要有三个方面的优点：\n\n增强CNN的学习能力，使得在轻量化的同时保持准确性\n降低计算瓶颈\n降低内存成本\n\n论文方法\n\n如上图(a), DenseNet的每个阶段均包含一个dense block(密集连接层)和transition layer(过渡层)，同时，每个dense block由K个密集层连接。第个密集层的输出将会同第个密集层的输入相连接，同时拼接后的输出结果作为个密集层的输入，等式表示如下所示：\n\n如果使用反向传播算法更新权重，则权重更新方程可写为：\n\n其中f是权重更新的函数，表示传播到第个密集层的梯度。 作者发现大量重复的梯度信息被用来更新不同密集层的权重。 这将导致不同的密集层重复学习复制的梯度信息。\n如上图(b),改进点在于CSPNet将浅层特征映射为两个部分，一部分经过Dense模块（图中的Partial Dense Block）,另一部分直接与Partial Dense Block输出进行concate。CSPDenseNet的前馈传递和权重更新的方程式分别显示如下：\n\n\n总体而言，CSPDenseNet保留了DenseNet的特征重用特性的优点，但同时通过截断梯度流，防止了过多的梯度信息重复。 通过设计分层特征融合策略来实现此思想，并将其用于部分过渡层。\n作者也设计了几种特征融合的策略，如下图所示： \nFustion First的方式是对两个分支的feature map先进行concatenation操作，这样梯度信息可以被重用。 Fusion Last的方式是对Dense Block所在分支先进性transition操作，然后再进行concatenation， 梯度信息将被截断，因此不会重复使用梯度信息 。\n\n\n\n\n\n\n\n\n\n所以CSP-DarkNet到底是怎么借鉴CSPNet的？ 如果按照CSPNet的思想，那特征输入应该按一定比例分为两路，分别经过Part1和Part2后concat，比如下图这样：\n\n\n来源见水印\n\n\n\n\n\n\n\n\n\n\n但实际CSP-DarkNet没有做split操作，Part1和Part2输入的是全部特征，如下图：\n\n\n来源见水印\n\n\n\n\n\n\n\n\n\n\n直接用两路的1x1卷积将输入特征进行变换。 可以理解的是，将全部的输入特征利用两路1x1进行transition，比直接划分通道能够进一步提高特征的重用性，并且在输入到resiudal block之前也确实通道减半，减少了计算量。\n参考来源 ##### Mish激活函数\n论文地址\n\n\nMish曲线\n\ny = x * tanh(ln(1+exp(x)))\n\n\n\n\n\n\n\n\n\n一种自正则的非单调神经激活函数，平滑的激活函数允许更好的信息深入神经网络，从而得到更好的准确性和泛化。论文中提出，相比Swish有0.494%的提升，相比ReLU有1.671%的提升。\n\n\n\n\n\n\n\n\n\nYolov4的Backbone中都使用了Mish激活函数，而后面的网络则还是使用leaky_relu函数。\n优点：\n\n以上无边界(即正值可以达到任何高度)避免了由于封顶而导致的饱和。2. 理论上对负值的轻微允许可以产生更好的梯度流，而不是像ReLU中那样的硬零边界。\n平滑的激活函数允许更好的信息深入神经网络，从而得到更好的准确性和泛化。\n\n更平滑的激活函数允许信息更深入地流动\n缺点：\n\n计算量肯定比relu大，占用的内存也多了不少；\n\npytorch实现\n\nDropblock\n论文地址\n\n\n\n\n\n\n\n\n\nYolov4中使用的Dropblock，其实和常见网络中的Dropout功能类似，也是缓解过拟合的一种正则化方式。\ndropout方法多是作用在全连接层上，在卷积层应用dropout方法意义不大。文章认为是因为每个feature map的位置都有一个感受野范围，仅仅对单个像素位置进行dropout并不能降低feature map学习的特征范围，也就是说网络仍可以通过该位置的相邻位置元素去学习对应的语义信息，也就不会促使网络去学习更加鲁棒的特征。 既然单独的对每个位置进行dropout并不能提高网络的泛化能力，那么很自然的，如果我们按照一块一块的去dropout，就自然可以促使网络去学习更加鲁棒的特征。思路很简单，就是在feature map上去一块一块的找，进行归零操作，类似于dropout，叫做dropblock。\n\n\n绿色阴影区域是语义特征，b图是模拟dropout的做法，随机丢弃一些位置的特征,(c)是dropblock\n\n\ndropblock有三个比较重要的参数，一个是block_size，用来控制进行归零的block大小；一个是γ，用来控制每个卷积结果中，到底有多少个channel要进行dropblock；最后一个是keep_prob，作用和dropout里的参数一样。\nM大小和输出特征图大小一致，非0即1，为了保证训练和测试一致，需要和dropout一样，进行rescale。\n上述是理论分析，在做实验时候发现，**block_size控制为7*7效果最好**，对于所有的feature map都一样，γ通过一个公式来控制，keep_prob则是一个线性衰减过程，从最初的1到设定的阈值(具体实现是dropout率从0增加到指定值为止)，论文通过实验表明这种方法效果最好。如果固定prob效果好像不好。 实践中，并没有显式的设置γ的值，而是根据keep_prob(具体实现是反的，是丢弃概率)来调整。\nNeck\n\n\n\n\n\n\n\n\n\n在目标检测领域，为了更好的提取融合特征，通常在Backbone和输出层，会插入一些层，这个部分称为Neck。相当于目标检测网络的颈部，也是非常关键的。\nYolov4的Neck结构主要采用了SPP模块、FPN+PAN的方式。\nSPP模块\n\n作者在SPP模块中，使用k={1 * 1,5 * 5,9 * 9,13 * 13}的最大池化的方式，再将不同尺度的特征图进行Concat操作。\n\n\n\n\n\n\n\n\n\n采用SPP模块的方式，比单纯的使用k*k最大池化的方式，更有效的增加主干特征的接收范围，显著的分离了最重要的上下文特征\nFPN+PAN\n论文地址\n\n\n\n\n\n\n\n\n\nPath Aggregation Network(PANet)，旨在提升基于侯选区域的实例分割框架内的信息流传播。具体来讲，通过自下向上(bottom-up)的路径增强在较低层(lower layer)中准确的定位信息流，建立底层特征和高层特征之间的信息路径，从而增强整个特征层次架构。\nyolo-v3中使用FPN具体如下：\n\n\n来源见水印\n\nFPN是自顶向下的，将高层的特征信息通过上采样的方式进行传递融合，得到进行预测的特征图。\n在yolo_v4中，在FPN的基础上增加PAN，具体结构如下：\n\n\n来源见水印\n\n如上图，紫色箭头处分别是三个中间feature map，分辨率为76 * 76、38 * 38、19 * 19\n\n\n来源见水印\n\n这样结合操作，FPN层自顶向下传达强语义特征，而特征金字塔则自底向上传达强定位特征，两两联手，从不同的主干层对不同的检测层进行参数聚合。\n\n\nyolo_v4中使用的是修改的PAN\n\nPrediction\n目标检测任务的损失函数一般由Classificition Loss（分类损失函数）和Bounding Box Regeression Loss（回归损失函数）两部分构成。\nBounding Box Regeression的Loss近些年的发展过程是：Smooth L1 Loss-&gt; IoU Loss（2016）-&gt; GIoU Loss（2019）-&gt; DIoU Loss（2020）-&gt;CIoU Loss（2020）\n我们从最常用的IOU_Loss开始，进行对比拆解分析，看下Yolov4为啥要选择CIOU_Loss。\n\n\n\n\n\n\n\n\n\n记住一点：好的目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比。\nIOU_Loss\n\nIOU的loss其实很简单，主要是交集/并集，但其实也存在两个问题。\n\n\n即状态1的情况，当预测框和目标框不相交时，IOU=0，无法反应两个框距离的远近，此时损失函数不可导，IOU_Loss无法优化两个框不相交的情况。\n即状态2和状态3的情况，当两个预测框大小相同，两个IOU也相同，IOU_Loss无法区分两者相交情况的不同。\n\n因此2019年出现了GIOU_Loss来进行改进。\nGIOU_Loss\n\n可以看到上图GIOU_Loss中，增加了相交尺度的衡量方式，缓解了单纯IOU_Loss时的尴尬。 但还有一种不足，如下：\n\n问题：状态1、2、3都是预测框在目标框内部且预测框大小一致的情况，这时预测框和目标框的差集都是相同的，因此这三种状态的GIOU值也都是相同的，这时GIOU退化成了IOU，无法区分相对位置关系。 基于这个问题，2020年的AAAI又提出了DIOU_Loss。\nDIOU_Loss\n好的目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比。\n针对IOU和GIOU存在的问题，作者从两个方面进行考虑\n一：如何最小化预测框和目标框之间的归一化距离？ 二：如何在预测框和目标框重叠时，回归的更准确？\n针对第一个问题，提出了DIOU_Loss（Distance_IOU_Loss）\n\nDIOU_Loss考虑了重叠面积和中心点距离，当目标框包裹预测框的时候，直接度量2个框的距离，因此DIOU_Loss收敛的更快。 但就像前面好的目标框回归函数所说的，没有考虑到长宽比。\n\n比如上面三种情况，目标框包裹预测框，本来DIOU_Loss可以起作用。 但预测框的中心点的位置都是一样的，因此按照DIOU_Loss的计算公式，三者的值都是相同的。\nCIOU_Loss\nCIOU_Loss和DIOU_Loss前面的公式都是一样的，不过在此基础上还增加了一个影响因子，将预测框和目标框的长宽比都考虑了进去。\n\n其中v是衡量长宽比一致性的参数，我们也可以定义为：\n\n这样CIOU_Loss就将目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比全都考虑进去了。\n对比\n再来综合的看下各个Loss函数的不同点：\nIOU_Loss：主要考虑检测框和目标框重叠面积。 GIOU_Loss：在IOU的基础上，解决边界框不重合时的问题。 DIOU_Loss：在IOU和GIOU的基础上，考虑边界框中心点距离的信息。 CIOU_Loss：在DIOU的基础上，考虑边界框宽高比的尺度信息。\nYolov4中采用了CIOU_Loss的回归方式，使得预测框回归的速度和精度更高一些。\nDIOU_nms\nDIoU用作 NMS 的一个因子。该方法在抑制冗余的边界框时会使用 IoU 和两个边界框的中心点之间的距离。这能使得模型能更加稳健地应对有遮挡的情况。 在传统NMS中，IoU指标常用于抑制冗余bbox，其中重叠区域是唯一因素，对于遮挡情况经常产生错误抑制。 DIoU-NMS将DIoU作为NMS的准则，因为在抑制准则中不仅应考虑重叠区域，而且还应考虑两个box之间的中心点距离，而DIoU就是同时考虑了重叠区域和两个box的中心距离。\nDIoU-NMS建议两个中心点较远的box可能位于不同的对象上，不应将其删除(这就是DIoU-NMS的与NMS的最大不同之处)。\n\n在上图重叠的摩托车检测中，中间的摩托车因为考虑边界框中心点的位置信息，也可以回归出来。\n因此在重叠目标的检测中，DIOU_nms的效果优于传统的nms。\n\n\n\n\n\n\n\n\n\n这里为什么不用CIOU_nms，而用DIOU_nms?\n答：因为前面讲到的CIOU_loss，是在DIOU_loss的基础上，添加的影响因子，包含groundtruth标注框的信息，在训练时用于回归。 但在测试过程中，并没有groundtruth的信息，不用考虑影响因子，因此直接用DIOU_nms即可。\n","slug":"YOLO-V4学习笔记","date":"2021-09-06T10:10:36.000Z","categories_index":"","tags_index":"detection","author_index":"Hulk Wang"},{"id":"82f698705681b0bacc9c6cad3db6d88e","title":"YOLO-V3学习笔记","content":"YOLO-V3学习笔记\n\n\n\n\n\n\n\n\n\n知识点来源于论文和网络，仅记录学习\n网络结构\nBackbone\n\n整个v3结构没有池化层和全连接层\n输出特征图缩小到输入的1/32。所以，通常都要求输入图片是32的倍数\n\n\nDBL:代码中的Darknetconv2d_BN_Leaky，是yolo_v3的基本组件。就是卷积+BN+Leaky relu。 resn:n代表数字，有res1，res2, … ,res8等等，表示这个res_block里含有多少个res_unit。 concat:张量拼接。将darknet中间层和后面的某一层的上采样进行拼接。拼接的操作和残差层add的操作是不一样的，拼接会扩充张量的维度，而add只是直接相加不会导致张量维度的改变。\nOutput\nyolo v3输出了3个不同尺度的feature map，如上图所示的y1, y2, y3。借鉴了FPN(feature pyramid networks)，采用多尺度来对不同size的目标进行检测，越精细的grid cell就可以检测出越精细的物体(大分辨率y3更能检测小物体，小分辨率y1更能检测大物体)。\ny1,y2和y3的深度都是255，边长分别为13:26:52。 对于COCO类别而言，有80个种类，所以每个box应该对每个种类都输出一个概率。\nyolo v3设定的是每个网格单元预测3个box，所以每个box需要有(x, y, w, h, confidence)五个基本参数，然后还要有80个类别的概率。所以3*(5 + 80) = 255。这个255就是这么来的。 v3用上采样的方法来实现这种多尺度的feature map，concat连接的两个张量是具有一样尺度的(两处拼接分别是26x26尺度拼接和52x52尺度拼接，通过(2, 2)上采样来保证concat拼接的张量尺度相同)。作者并没有像SSD那样直接采用backbone中间层的处理结果作为feature map的输出，而是和后面网络层的上采样结果进行一个拼接之后的处理结果作为feature map。\nBounding Box\n在Yolov1中，网络直接回归检测框的宽、高，这样效果有限。所以在Yolov2中，改为了回归基于先验框的变化值，这样网络的学习难度降低，整体精度提升不小。Yolov3沿用了Yolov2中关于先验框的技巧，并且使用k-means对数据集中的标签框进行聚类，得到类别中心点的9个框，作为先验框。在COCO数据集中（原始图片全部resize为416 × 416），九个框分别是 (10×13)，(16×30)，(33×23)，(30×61)，(62×45)，(59× 119)， (116 × 90)， (156 × 198)，(373 × 326) ，顺序为w × h。\nfeature map中的每一个cell都会预测3个边界框（bounding box） ，每个bounding box都会预测三个东西： 1. 每个框的位置（4个值，中心坐标tx和ty，框的高度bh和宽度bw） 2. 一个objectness prediction 3. N个类别\n三个output，每个对应的感受野不同，32倍降采样的感受野最大，适合检测大的目标，所以在输入为416×416时，每个cell的三个anchor box为(116 ,90); (156 ,198); (373 ,326)。16倍适合一般大小的物体，anchor box为(30,61); (62,45); (59,119)。8倍的感受野最小，适合检测小目标，因此anchor box为(10,13); (16,30); (33,23)。所以当输入为416×416时，实际总共有（52×52+26×26+13×13）×3=10647个proposal box。\n\n\n\n\n\n\n\n\n\n特征图\n13x13\n26x26\n52x52\n\n\n\n\n感受野\n大\n中\n小\n\n\n先验框\n(116 ,90)(156 ,198)(373 ,326)\n(30,61) (62,45)(59,119)\n(10,13)(16,30)(33,23)\n\n\n\n\n9种尺寸的先验框，图中蓝色框为聚类得到的先验框。黄色框式ground truth，红框是对象中心点所在的网格(来源见水印)\n\n\n\n\n\n\n\n\n\n\n这里注意bounding box 与anchor box的区别： Bounding box它输出的是框的位置（中心坐标与宽高），confidence以及N个类别。anchor box只是一个尺度即只有宽高。\nOutput Decode\nBounding box decode\n如上一节所说，v2开始，回归基于先验框的变化值，因此可以通过以下公式解码检测框的x，y，w，h.\n\n如下图，\\(\\sigma(t_x)\\)、\\(\\sigma(t_y)\\)是基于矩形框中心点左上角格点坐标的偏移量, \\(\\sigma\\)是激活函数，论文中作者使用sigmoid, \\(p_w, p_h\\)是先验框的宽、高，通过上述公式，计算出实际预测框的宽高 \\(b_w, b_h\\).\n\n\n\n\n\n\n\n\n\n\n得到对应的\\(b_w, b_h\\)后, 还需要乘以特征图对应的的采样率(32,16,8)，得到真实的检测框x,y\nobjectness score decode\n物体的检测置信度，在Yolo设计中非常重要，关系到算法的检测正确率与召回率。 置信度在输出85维中占固定一位，由sigmoid函数解码即可，解码之后数值区间在[0，1]中。\nlogistic回归用于对anchor包围的部分进行一个目标性评分(objectness score)，即这块位置是目标的可能性有多大。这一步是在predict之前进行的，可以去掉不必要anchor，可以减少计算量。作者在论文种的描述如下:\n\n\n\n\n\n\n\n\n\nIf the bounding box prior is not the best but does overlap a ground truth object by more than some threshold we ignore the prediction, following[17]. We use the threshold of 0.5. Unlike [17] our system only assigns one bounding box prior for each ground truth object.\n如果模板框不是最佳的即使它超过我们设定的阈值，我们还是不会对它进行predict。不同于faster R-CNN的是，yolo_v3只会对1个prior进行操作，也就是那个最佳prior。而logistic回归就是用来从9个anchor priors中找到objectness score(目标存在可能性得分)最高的那一个。logistic回归就是用曲线对prior相对于 objectness score映射关系的线性建模。\nClass Prediction decode\nCOCO数据集有80个类别，所以类别数在85维输出中占了80维，每一维独立代表一个类别的置信度。使用sigmoid激活函数替代了Yolov2中的softmax，取消了类别之间的互斥，可以使网络更加灵活。\n总结\n\n9个anchor会被三个输出张量平分的。根据大中小三种size各自取自己的anchor。\n作者使用了logistic回归来对每个anchor包围的内容进行了一个目标性评分(objectness score)。 根据目标性评分来选择anchor prior进行predict，而不是所有anchor prior都会有输出。\n\n训练策略\n\n\n\n\n\n\n\n\n\nYOLOv3 predicts an objectness score for each bounding box using logistic regression. This should be 1 if the bounding box prior overlaps a ground truth object by more than any other bounding box prior. If the bounding box prior is not the best but does overlap a ground truth object by more than some threshold we ignore the prediction, following [17]. We use the threshold of .5. Unlike [17] our system only assigns one bounding box prior for each ground truth object. If a bounding box prior is not assigned to a ground truth object it incurs no loss for coordinate or class predictions, only objectness.\n预测框一共分为三种情况：正例（positive）、负例（negative）、忽略样例（ignore）。\n正例：任取一个ground truth，与4032个框全部计算IOU，IOU最大的预测框，即为正例。并且一个预测框，只能分配给一个ground truth。例如第一个ground truth已经匹配了一个正例检测框，那么下一个ground truth，就在余下的4031个检测框中，寻找IOU最大的检测框作为正例。ground truth的先后顺序可忽略。正例产生置信度loss、检测框loss、类别loss。预测框为对应的ground truth box标签（需要反向编码，使用真实的x、y、w、h计算出 ）；类别标签对应类别为1，其余为0；置信度标签为1。\n忽略样例：正例除外，与任意一个ground truth的IOU大于阈值（论文中使用0.5），则为忽略样例。忽略样例不产生任何loss。\n负例：正例除外（与ground truth计算后IOU最大的检测框，但是IOU小于阈值，仍为正例），与全部ground truth的IOU都小于阈值（0.5），则为负例。负例只有置信度产生loss，置信度标签为0。\nLoss\nYolov3 Loss为三个特征图Loss之和：\n\\(Loss = Loss_n1 + Loss_n2 + Loss_n3\\)\n\n\n\\(\\lambda\\)为权重常数，控制检测框Loss、obj置信度Loss、noobj置信度Loss之间的比例，通常负例的个数是正例的几十倍以上，可以通过权重超参控制检测效果;\n\\(1^{obj}_{ij}\\) 若是正例则输出1，否则为0；\\(1^{noobj}_{ij}\\) ,若是负例则输出1，否则为0；忽略样例都输出0;\nx、y、w、h使用MSE作为损失函数，也可以使用smooth L1 loss（出自Faster R-CNN）作为损失函数。smooth L1可以使训练更加平滑。置信度、类别标签由于是0，1二分类，所以使用交叉熵作为损失函数。\n\n其他\n\nground truth为什么不按照中心点分配对应的预测box？ &gt;在Yolov3的训练策略中，不再像Yolov1那样，每个cell负责中心落在该cell中的ground truth。原因是Yolov3一共产生3个特征图，3个特征图上的cell，中心是有重合的。训练时，可能最契合的是特征图1的第3个box，但是推理的时候特征图2的第1个box置信度最高。所以Yolov3的训练，不再按照ground truth中心点，严格分配指定cell，而是根据预测值寻找IOU最大的预测框作为正例。\n为什么有忽略样例？\n\n忽略样例是Yolov3中的点睛之笔。由于Yolov3使用了多尺度特征图，不同尺度的特征图之间会有重合检测部分。比如有一个真实物体，在训练时被分配到的检测框是特征图1的第三个box，IOU达0.98，此时恰好特征图2的第一个box与该ground truth的IOU达0.95，也检测到了该ground truth，如果此时给其置信度强行打0的标签，网络学习效果会不理想。\n本身正负样本比例就不均衡（负例&gt;正例），如果强行标为0，会使不均衡更严重。\n\n\n","slug":"YOLO-V3学习笔记","date":"2021-08-30T11:59:09.000Z","categories_index":"","tags_index":"detection","author_index":"Hulk Wang"},{"id":"2a4b32b81021e06bffb6e540079ceefc","title":"hexo+github 搭建个人博客","content":"hexo+github搭建个人博客\n\n\n\n\n\n\n\n\n\n搭建环境:macOs 11.4 环境依赖:\n&gt; * git\n&gt; * npm\n&gt; * node\n&gt; * hexo\nhexo安装\n\n安装node brew install node\n安装hexo npm install -g hexo-cli\n查看hexo版本 hexo -v \n\n建站\n\n\n\n\n\n\n\n\n\n安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件\n\n初始化hexo框架 hexo init &lt;folder&gt;\n移动到目标目录 cd &lt;folder&gt;\n安装依赖组件 npm install\n生成静态文件 hexo g\n开启本地服务器 hexo s\n\n\n\n\n\n\n\n\n\n\n在浏览器中输入 http://localhost:4000 回车就可以预览效果了\n更换主题\n\n\n\n\n\n\n\n\n\n此时博客是hexo默认主题，比较普通，这里推荐一个主题：Aurora\n安装教程\n效果预览\n配置github\n\n建立respository repository名称为username.github.io\n修改配置文件 &gt; _config.yml文件\n\ndeploy:  \n\ttype: git \n\trepository: https:&#x2F;&#x2F;github.com&#x2F;username&#x2F;username.github.io.git\n\tbranch: master\n\n安装一个部署插件 npm install hexo-deployer-git --save\n重新生成部署 hexo g -d\n\n\n\n\n\n\n\n\n\n\n此时可通过 https://username.github.io 访问博客\n配置个性域名\n\n\n\n\n\n\n\n\n\n这里我购买了腾讯云的域名: hulk.show\n\n配置域名 &gt; 进入域名管理界面，选择解析，添加两条解析： \n配置git &gt; 你的项目-&gt;Setting-&gt;Pages-&gt;Custom domain,添加你的域名： \n\n\n\n\n\n\n\n\n\n\n可能需要等几分钟,即可通过购买的域名访问博客： www.hulk.show\n","slug":"hexo-github-搭建个人博客","date":"2021-08-29T10:50:28.000Z","categories_index":"","tags_index":"config","author_index":"Hulk Wang"}]