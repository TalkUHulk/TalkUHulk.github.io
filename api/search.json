[{"id":"57d3aa7cdcc56700d35ee7aa4e8c6558","title":"YOLO-V5学习笔记","content":"YOLO-V5学习笔记\n\n\n\n\n\n\n\n\n知识点来源于网络，仅记录学习来源：https://zhuanlan.zhihu.com/p/172121380\n网络结构\n Yolov5s网络结构(来源见水印)\n\n\n如上图为yolov5的整体网络结构，跟yolov4一样，分别按input、backbone、Neck以及Prediction四部分来理解。\n\n\n\n\n\n\n\n\n\nYolov5官方代码中，一共有4个版本，分别是Yolov5s、Yolov5m、Yolov5l、Yolov5x四个模型。Yolov5s是Yolov5系列中深度最小，特征图的宽度最小的网络。后面的3种都是在此基础上不断加深，不断加宽。\n输入端Mosaic数据增强与v4一样，采用Mosaic数据增强；\n自适应锚框计算将anchor初始计算(聚类)集成到训练代码中；\n自适应图片缩放针对inference阶段的优化\n在常用的目标检测算法中，不同的图片长宽都不相同，因此常用的方式是将原始图片统一缩放到一个标准尺寸，再送入检测网络中。\n\n 传统方法(来源见水印)\n\n但Yolov5代码中对此进行了改进，也是Yolov5推理速度能够很快的一个不错的trick。\n作者认为，在项目实际使用时，很多图片的长宽比不同，因此缩放填充后，两端的黑边大小都不同，而如果填充的比较多，则存在信息冗余，影响推理速度。\n因此在Yolov5的代码中datasets.py的letterbox函数中进行了修改，对原始图像自适应的添加最少的黑边。\n\n yolov5(来源见水印)\n\n举例说明填充方法：原始：800x600目标：416\n\n选择小的缩放系数，短边:min(416/800, 416/600);\n得到新的尺寸(即长边resize到目标尺寸,短边按原始长宽比变换）: (416,312);\n计算pad大小(找到大于312且能被32整除的最小整数): (416 - 312) mod 32 = 8 所以pad值为8/2=4\n\nBackboneFocus结构\n focus(来源见水印)\n\nyolov5中，Focus模块位于backbone前。具体操作是在一张图片中每隔一个像素拿到一个值，类似于邻近下采样，这样就拿到了四张近似下采样的图片，但是没有信息丢失。相当于w,h变为1/2，输入通道扩充了4倍，最后将得到的新图片再经过卷积操作，最终得到了没有信息丢失情况下的二倍下采样特征图。\n以yolov5s为例，原始的640 × 640 × 3的图像输入Focus结构，采用切片操作，先变成320 × 320 × 12的特征图，再经过一次卷积操作，最终变成320 × 320 × 32的特征图。\n具体代码实现：\n\n\n目的和作用： Focus是为了提速，和mAP无关，减少了计算量和参数量。\nThe YOLOv5 Focus layer replaces the first 3 YOLOv3 layers with a single layer:\n\n\n详见作者解答\nCSP结构Yolov5与Yolov4不同点在于，Yolov4中只有主干网络使用了CSP结构。\n而Yolov5中设计了两种CSP结构，以Yolov5s网络为例，CSP1_X结构应用于Backbone主干网络，另一种CSP2_X结构则应用于Neck中。\n\n\nNeck\nYolov5现在的Neck和Yolov4中一样，都采用FPN+PAN的结构.\n如CSP结构中讲到，Yolov5和Yolov4的不同点在于，Yolov4的Neck结构中，采用的都是普通的卷积操作。而Yolov5的Neck结构中，采用借鉴CSPnet设计的CSP2结构，加强网络特征融合的能力。\n\n\n\nPredictionBounding box损失函数Yolov5: GIOU_LossYolov4: CIOU_Loss\nnms非极大值抑制Yolov4: DIOU_nmsYolov5: 加权nms\n\n\n\n\n\n\n\n\n\nWeighted NMS出现于ICME Workshop 2017《Inception Single Shot MultiBox Detector for object detection》一文中。论文认为Traditional NMS每次迭代所选出的最大得分框未必是精确定位的，冗余框也有可能是定位良好的。那么与直接剔除机制不同，Weighted NMS顾名思义是对坐标加权平均，加权平均的对象包括M自身以及IoU≥NMS阈值的相邻框。加权的权重为 : ，表示得分与IoU的乘积。优点：Weighted NMS通常能够获得更高的Precision和Recall，只要NMS阈值选取得当，Weighted NMS均能稳定提高AP与AR，无论是AP50还是AP75，也不论所使用的检测模型是什么。缺点：顺序处理模式，且运算效率比Traditional NMS更低。加权因子是IoU与得分，前者只考虑两个框的重叠面积，这对描述box重叠关系或许不够全面；而后者受到定位与得分不一致问题的限制。\n","slug":"YOLO-V5学习笔记","date":"2021-09-09T07:42:37.000Z","categories_index":"","tags_index":"detection","author_index":"Hulk Wang"},{"id":"f45a749a54e27d0a1f22b1f23eadc80a","title":"Pixel-Level Domain Transfer论文复现","content":"Pixel-Level Domain Transfer论文复现博客迁移，原文链接\n\n\n\n\n\n\n\n\n\nAbstract.：We present an image-conditional image generation model. The model transfers an input domain to a target domain in semantic level, and generates the target image in pixel level. To generate realistic target images, we employ the real/fake-discriminator as in Generative Adversarial Nets, but also introduce a novel domain-discriminator to make the generated image relevant to the input image. We verify our model through a challenging task of generating a piece of clothing from an input image of a dressed person. We present a high quality clothing dataset containing the two domains, and succeed in demonstrating decent results.\n论文简述  整篇论文比较容易懂，主要内容就是把输入domain转换到目标domain，输入一张模特图片，得到上衣图片，如下：\n文章主要贡献主要在两个方面：\nLookBook数据集下载地址（uj3j）\n\n\n基于Gan的转换框架网络结构如下：\n\n\n生成网络是encoder-decoder结构，判别网络有两个：Dr和Da。\nDr就是一个基本的Gan的判别网络，判别fake或real；Da主要用来判断生成图像与输入是否配对，所以Dr输入是生成网络的输入和输出的concat.\n整个过程很容易懂，细节看原文即可.\n论文复现Generator：输入64x64x3图像，输出64x64x3生成图像\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n \n        def conv_block(in_channels, out_channels, kernel_size, stride&#x3D;1,\n                 padding&#x3D;0, bn&#x3D;True, a_func&#x3D;&#39;lrelu&#39;):\n \n            block &#x3D; nn.ModuleList()\n            block.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func &#x3D;&#x3D; &#39;lrelu&#39;:\n                block.append(nn.LeakyReLU(0.2))\n            elif a_func &#x3D;&#x3D; &#39;relu&#39;:\n                block.append(nn.ReLU())\n            else:\n                pass\n \n            return block\n \n        def convTranspose_block(in_channels, out_channels, kernel_size, stride&#x3D;2,\n                 padding&#x3D;0, output_padding&#x3D;0, bn&#x3D;True, a_func&#x3D;&#39;relu&#39;):\n            &#39;&#39;&#39;\n            H_out &#x3D; (H_in - 1) * stride - 2 * padding + kernel_size + output_padding\n            :param in_channels:\n            :param out_channels:\n            :param kernel_size:\n            :param stride:\n            :param padding:\n            :param output_padding:\n            :param bn:\n            :param a_func:\n            :return:\n            &#39;&#39;&#39;\n            block &#x3D; nn.ModuleList()\n            block.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride,\n                 padding, output_padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func &#x3D;&#x3D; &#39;lrelu&#39;:\n                block.append(nn.LeakyReLU(0.2))\n            elif a_func &#x3D;&#x3D; &#39;relu&#39;:\n                block.append(nn.ReLU())\n            else:\n                pass\n \n            return block\n \n \n        def encoder():\n            conv_layer &#x3D; nn.ModuleList()\n            conv_layer +&#x3D; conv_block(3, 128, 5, 2, 2, False)    # 32x32x128\n            conv_layer +&#x3D; conv_block(128, 256, 5, 2, 2)        # 16x16x256\n            conv_layer +&#x3D; conv_block(256, 512, 5, 2, 2)         # 8x8x512\n            conv_layer +&#x3D; conv_block(512, 1024, 5, 2, 2)       # 4x4x1024\n            conv_layer +&#x3D; conv_block(1024, 64, 4, 1)          # 1x1x64\n            return conv_layer\n \n        def decoder():\n            conv_layer &#x3D; nn.ModuleList()\n            conv_layer +&#x3D; conv_block(64, 4 * 4 * 1024, 1, a_func&#x3D;&#39;relu&#39;)\n            conv_layer.append(Reshape((1024, 4, 4)))                            # 4x4x1024\n            conv_layer +&#x3D; convTranspose_block(1024, 512, 4, 2, 1)               # 8x8x512\n            conv_layer +&#x3D; convTranspose_block(512, 256, 4, 2, 1)                # 16x16x256\n            conv_layer +&#x3D; convTranspose_block(256, 128, 4, 2, 1)                # 32x32x128\n            conv_layer +&#x3D; convTranspose_block(128, 3, 4, 2, 1, bn&#x3D;False, a_func&#x3D;&#39;&#39;)     # 64x64x3\n            conv_layer.append(nn.Tanh())\n            return conv_layer\n \n        self.net &#x3D; nn.Sequential(\n            *encoder(),\n            *decoder(),\n        )\n \n    def forward(self, input):\n        out &#x3D; self.net(input)\n        return out\n\nDiscriminatorR输入64x64x3图像，输出real or fake；\nclass DiscriminatorR(nn.Module):\n    def __init__(self):\n        super(DiscriminatorR, self).__init__()\n \n        def conv_block(in_channels, out_channels, kernel_size, stride&#x3D;1,\n                       padding&#x3D;0, bn&#x3D;True, a_func&#x3D;True):\n \n            block &#x3D; nn.ModuleList()\n            block.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func:\n                block.append(nn.LeakyReLU(0.2))\n \n            return block\n \n \n        self.net &#x3D; nn.Sequential(\n            *conv_block(3, 128, 5, 2, 2, False),                            # 32x32x128\n            *conv_block(128, 256, 5, 2, 2),                                 # 16x16x256\n            *conv_block(256, 512, 5, 2, 2),                                 # 8x8x512\n            *conv_block(512, 1024, 5, 2, 2),                                # 4x4x1024\n            *conv_block(1024, 1, 4, bn&#x3D;False, a_func&#x3D;False),                # 1x1x1\n            nn.Sigmoid(),\n        )\n \n    def forward(self, img):\n        out &#x3D; self.net(img)\n        return out\n\nDiscriminatorA输入64x64x6的concat图像，输出real or fake；\nclass DiscriminatorA(nn.Module):\n    def __init__(self):\n        super(DiscriminatorA, self).__init__()\n \n        def conv_block(in_channels, out_channels, kernel_size, stride&#x3D;1,\n                       padding&#x3D;0, bn&#x3D;True, a_func&#x3D;True):\n \n            block &#x3D; nn.ModuleList()\n            block.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func:\n                block.append(nn.LeakyReLU(0.2))\n \n            return block\n \n        self.net &#x3D; nn.Sequential(\n            *conv_block(6, 128, 5, 2, 2, False),                # 32x32x128\n            *conv_block(128, 256, 5, 2, 2),                     # 16x16x256\n            *conv_block(256, 512, 5, 2, 2),                     # 8x8x512\n            *conv_block(512, 1024, 5, 2, 2),                    # 4x4x1024\n            *conv_block(1024, 1, 4, bn&#x3D;False, a_func&#x3D;False),    # 1x1x1\n            nn.Sigmoid(),\n        )\n \n    def forward(self, img):\n        out &#x3D; self.net(img)\n        return out\n\nloss与原文不同，在生成损失上加了mse\ngen_loss_d &#x3D; self.adversarial_loss(torch.squeeze(gen_output), real_label)\ngen_loss_a &#x3D; self.adversarial_loss(torch.squeeze(gen_output_a), real_label)\nmse_loss &#x3D; self.mse_loss(gen_target_batch, target_batch)\n\n完整训练测试代码：GitHub\n结果tensorboard\n\n训练过程可视化\n\n验证集\n\n\n\n","slug":"Pixel-Level-Domain-Transfer论文复现","date":"2021-09-06T11:51:46.000Z","categories_index":"","tags_index":"csdn迁移","author_index":"Hulk Wang"},{"id":"260edd383abab8c5258536a42ddb3e2a","title":"YOLO-V4学习笔记","content":"YOLO-V4学习笔记\n\n\n\n\n\n\n\n\n知识点来源于网络，仅记录学习来源：https://zhuanlan.zhihu.com/p/143747206\n网络结构\n 网络结构(来源见水印)\n\n五个基本组件:\n\nCBM：Yolov4网络结构中的最小组件，由Conv+Bn+Mish激活函数三者组成。\nCBL：由Conv+Bn+Leaky_relu激活函数三者组成。\nRes unit：借鉴Resnet网络中的残差结构，让网络可以构建的更深。\nCSPX：借鉴CSPNet网络结构，由卷积层和X个Res unint模块Concate组成。\nSPP：采用1×1，5×5，9×9，13×13的最大池化的方式，进行多尺度融合。\n\n\n  Object detector\n\n如上图，大致分为四个阶段理解yolov4，分别为输入端、backbone、Neck以及Prediction。\n输入端Mosaic数据增强Yolov4中使用的Mosaic是参考2019年底提出的CutMix数据增强的方式，但CutMix只使用了两张图片进行拼接，而Mosaic数据增强则采用了4张图片，随机缩放、随机裁剪、随机排布的方式进行拼接。\n\n\n为什么要进行Mosaic数据增强?\n\n\n\n\n\n\n\n\n在平时项目训练时，小目标的AP一般比中目标和大目标低很多。而Coco数据集中也包含大量的小目标，但比较麻烦的是小目标的分布并不均匀。\n首先看下小、中、大目标的定义：2019年发布的论文《Augmentation for small object detection》对此进行了区分:\n\n\n\n\n最小矩形区域面积\n最大矩形区域面积\n\n\n\n小目标\n0 * 0\n32 * 32\n\n\n中目标\n32 * 32\n96 * 96\n\n\n大目标\n96 * 96\n∞ * ∞\n\n\n\n\n\n\n\n\n\n\n\n可以看到小目标的定义是目标框的长宽0×0~32×32之间的物体。\n\n\n\n\n小\n中\n大\n\n\n\n数据集中小目标占比\n41.4%\n34.3%\n24.3%\n\n\n数据集图片包含占比\n52.3%\n70.7%\n83.0%\n\n\n但在整体的数据集中，小、中、大目标的占比并不均衡。\n\n\n\n\n\n如上表所示，Coco数据集中小目标占比达到41.4%，数量比中目标和大目标都要多。\n\n\n\n\n\n但在所有的训练集图片中，只有52.3%的图片有小目标，而中目标和大目标的分布相对来说更加均匀一些。\n针对这种状况，Yolov4的作者采用了Mosaic数据增强的方式。\n主要有几个优点：\n\n丰富数据集：随机使用4张图片，随机缩放，再随机分布进行拼接，丰富了图片的背景，大大丰富了检测数据集，特别是随机缩放增加了很多小目标，让网络的鲁棒性更好。\n减少GPU：四张图片拼接在一起变相地提高了batch_size，在进行batch normalization的时候也会计算四张图片，所以对本身batch_size不是很依赖，单块GPU就可以训练YOLOV4。\n\n\n\n\n\n\n\n\n\n\n最后说明一下对于标签框的处理，当进行裁剪的时候，如果裁剪了样本当中的标签框的部分区域，则将其舍弃，保留裁剪之后还完整的标签框。\n参考\n与其他增强方法对比\nMixup:将随机的两张样本按比例混合，分类的结果按比例分配；\n\nCutout:随机的将样本中的部分区域cut掉，并且填充0像素值，分类的结果不变；\n\nCutMix:就是将一部分区域cut掉但不填充0像素而是随机填充训练集中的其他数据的区域像素值，分类结果按一定的比例分配\n\n\n\n\n上述三种数据增强的区别：\n\ncutout和cutmix就是填充区域像素值的区别;\nmixup和cutmix是混合两种样本方式上的区别;\nmixup是将两张图按比例进行插值来混合样本，cutmix是采用cut部分区域再补丁的形式去混合图像，不会有图像混合后不自然的情形。\n\n参考\nBackBoneCSPDarknet53\n\n\n\n\n\n\n\n\nCSPDarknet53是在Yolov3主干网络Darknet53的基础上，借鉴2019年CSPNet的经验，产生的Backbone结构。因为Backbone有5个CSP模块（见网络结构），输入图像是608 * 608，所以特征图变化的规律是：608-&gt;304-&gt;152-&gt;76-&gt;38-&gt;19。经过5次CSP模块后得到19*19大小的特征图。而且作者只在Backbone中采用了Mish激活函数，网络后面仍然采用Leaky_relu激活函数。\nCSPNet论文地址\n  CSPNet全称是Cross Stage Paritial Network，主要从网络结构设计的角度解决推理中从计算量很大的问题。\n  设计CSPNet的主要目的是使该体系结构能够实现更丰富的梯度组合信息，同时减少计算量。 通过将基础层的特征图划分为两个部分，然后通过提出的跨阶段层次结构将它们合并，可以实现此目标。 \n  CSPNet的作者认为推理计算过高的问题是由于网络优化中的梯度信息重复导致的。\n  因此采用CSP模块先将基础层的特征映射划分为两部分，然后通过跨阶段层次结构将它们合并，在减少了计算量的同时可以保证准确率。\n  因此Yolov4在主干网络Backbone采用CSPDarknet53网络结构，主要有三个方面的优点：\n\n增强CNN的学习能力，使得在轻量化的同时保持准确性\n降低计算瓶颈\n降低内存成本\n\n论文方法\n\n如上图(a), DenseNet的每个阶段均包含一个dense block(密集连接层)和transition layer(过渡层)，同时，每个dense block由K个密集层连接。第$i^{th}$个密集层的输出将会同第$i^{th}$个密集层的输入相连接，同时拼接后的输出结果作为${i+1}^{th}$个密集层的输入，等式表示如下所示：\n\n\n如果使用反向传播算法更新权重，则权重更新方程可写为：\n\n\n其中f是权重更新的函数，$g_i$表示传播到第$i^{th}$个密集层的梯度。作者发现大量重复的梯度信息被用来更新不同密集层的权重。 这将导致不同的密集层重复学习复制的梯度信息。\n如上图(b),改进点在于CSPNet将浅层特征映射为两个部分，一部分经过Dense模块（图中的Partial Dense Block）,另一部分直接与Partial Dense Block输出进行concate。CSPDenseNet的前馈传递和权重更新的方程式分别显示如下：\n\n\n\n\n\n总体而言，CSPDenseNet保留了DenseNet的特征重用特性的优点，但同时通过截断梯度流，防止了过多的梯度信息重复。 通过设计分层特征融合策略来实现此思想，并将其用于部分过渡层。\n作者也设计了几种特征融合的策略，如下图所示：\nFustion First的方式是对两个分支的feature map先进行concatenation操作，这样梯度信息可以被重用。Fusion Last的方式是对Dense Block所在分支先进性transition操作，然后再进行concatenation， 梯度信息将被截断，因此不会重复使用梯度信息 。\n\n\n\n\n\n\n\n\n\n所以CSP-DarkNet到底是怎么借鉴CSPNet的？如果按照CSPNet的思想，那特征输入应该按一定比例分为两路，分别经过Part1和Part2后concat，比如下图这样：\n\n 来源见水印\n\n\n\n\n\n\n\n\n\n\n但实际CSP-DarkNet没有做split操作，Part1和Part2输入的是全部特征，如下图：\n\n 来源见水印\n\n\n\n\n\n\n\n\n\n\n直接用两路的1x1卷积将输入特征进行变换。 可以理解的是，将全部的输入特征利用两路1x1进行transition，比直接划分通道能够进一步提高特征的重用性，并且在输入到resiudal block之前也确实通道减半，减少了计算量。\n参考来源\nMish激活函数论文地址\n\n Mish曲线\n\ny = x * tanh(ln(1+exp(x)))\n\n\n\n\n\n\n\n\n\n 一种自正则的非单调神经激活函数，平滑的激活函数允许更好的信息深入神经网络，从而得到更好的准确性和泛化。论文中提出，相比Swish有0.494%的提升，相比ReLU有1.671%的提升。\n\n\n\n\n\n\n\n\n\n Yolov4的Backbone中都使用了Mish激活函数，而后面的网络则还是使用leaky_relu函数。\n优点：\n\n以上无边界(即正值可以达到任何高度)避免了由于封顶而导致的饱和。2. 理论上对负值的轻微允许可以产生更好的梯度流，而不是像ReLU中那样的硬零边界。\n平滑的激活函数允许更好的信息深入神经网络，从而得到更好的准确性和泛化。\n\n更平滑的激活函数允许信息更深入地流动\n缺点：\n\n计算量肯定比relu大，占用的内存也多了不少；\n\npytorch实现\n\n\nDropblock论文地址\n\n\n\n\n\n\n\n\n\nYolov4中使用的Dropblock，其实和常见网络中的Dropout功能类似，也是缓解过拟合的一种正则化方式。\ndropout方法多是作用在全连接层上，在卷积层应用dropout方法意义不大。文章认为是因为每个feature map的位置都有一个感受野范围，仅仅对单个像素位置进行dropout并不能降低feature map学习的特征范围，也就是说网络仍可以通过该位置的相邻位置元素去学习对应的语义信息，也就不会促使网络去学习更加鲁棒的特征。    既然单独的对每个位置进行dropout并不能提高网络的泛化能力，那么很自然的，如果我们按照一块一块的去dropout，就自然可以促使网络去学习更加鲁棒的特征。思路很简单，就是在feature map上去一块一块的找，进行归零操作，类似于dropout，叫做dropblock。\n\n 绿色阴影区域是语义特征，b图是模拟dropout的做法，随机丢弃一些位置的特征,(c)是dropblock\n\n\n\ndropblock有三个比较重要的参数，一个是block_size，用来控制进行归零的block大小；一个是γ，用来控制每个卷积结果中，到底有多少个channel要进行dropblock；最后一个是keep_prob，作用和dropout里的参数一样。\nM大小和输出特征图大小一致，非0即1，为了保证训练和测试一致，需要和dropout一样，进行rescale。\n上述是理论分析，在做实验时候发现，block_size控制为7*7效果最好，对于所有的feature map都一样，γ通过一个公式来控制，keep_prob则是一个线性衰减过程，从最初的1到设定的阈值(具体实现是dropout率从0增加到指定值为止)，论文通过实验表明这种方法效果最好。如果固定prob效果好像不好。实践中，并没有显式的设置γ的值，而是根据keep_prob(具体实现是反的，是丢弃概率)来调整。\nNeck\n\n\n\n\n\n\n\n\n在目标检测领域，为了更好的提取融合特征，通常在Backbone和输出层，会插入一些层，这个部分称为Neck。相当于目标检测网络的颈部，也是非常关键的。\nYolov4的Neck结构主要采用了SPP模块、FPN+PAN的方式。\nSPP模块\n\n作者在SPP模块中，使用k={1 * 1,5 * 5,9 * 9,13 * 13}的最大池化的方式，再将不同尺度的特征图进行Concat操作。\n\n\n\n\n\n\n\n\n\n采用SPP模块的方式，比单纯的使用kk最大池化的方式，*更有效的增加主干特征的接收范围，显著的分离了最重要的上下文特征\nFPN+PAN论文地址\n\n\n\n\n\n\n\n\n\nPath Aggregation Network(PANet)，旨在提升基于侯选区域的实例分割框架内的信息流传播。具体来讲，通过自下向上(bottom-up)的路径增强在较低层(lower layer)中准确的定位信息流，建立底层特征和高层特征之间的信息路径，从而增强整个特征层次架构。\nyolo-v3中使用FPN具体如下：\n\n  来源见水印\n\n\nFPN是自顶向下的，将高层的特征信息通过上采样的方式进行传递融合，得到进行预测的特征图。\n在yolo_v4中，在FPN的基础上增加PAN，具体结构如下：\n\n  来源见水印\n\n如上图，紫色箭头处分别是三个中间feature map，分辨率为76 * 76、38 * 38、19 * 19\n\n 来源见水印\n\n\n这样结合操作，FPN层自顶向下传达强语义特征，而特征金字塔则自底向上传达强定位特征，两两联手，从不同的主干层对不同的检测层进行参数聚合。\n\n  yolo_v4中使用的是修改的PAN\n\n\nPrediction目标检测任务的损失函数一般由Classificition Loss（分类损失函数）和Bounding Box Regeression Loss（回归损失函数）两部分构成。\nBounding Box Regeression的Loss近些年的发展过程是：Smooth L1 Loss-&gt; IoU Loss（2016）-&gt; GIoU Loss（2019）-&gt; DIoU Loss（2020）-&gt;CIoU Loss（2020）\n我们从最常用的IOU_Loss开始，进行对比拆解分析，看下Yolov4为啥要选择CIOU_Loss。\n\n\n\n\n\n\n\n\n\n记住一点：好的目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比。\nIOU_Loss\n\nIOU的loss其实很简单，主要是交集/并集，但其实也存在两个问题。\n\n\n\n即状态1的情况，当预测框和目标框不相交时，IOU=0，无法反应两个框距离的远近，此时损失函数不可导，IOU_Loss无法优化两个框不相交的情况。\n\n即状态2和状态3的情况，当两个预测框大小相同，两个IOU也相同，IOU_Loss无法区分两者相交情况的不同。\n\n\n因此2019年出现了GIOU_Loss来进行改进。\nGIOU_Loss\n\n可以看到上图GIOU_Loss中，增加了相交尺度的衡量方式，缓解了单纯IOU_Loss时的尴尬。但还有一种不足，如下：\n\n\n问题：状态1、2、3都是预测框在目标框内部且预测框大小一致的情况，这时预测框和目标框的差集都是相同的，因此这三种状态的GIOU值也都是相同的，这时GIOU退化成了IOU，无法区分相对位置关系。基于这个问题，2020年的AAAI又提出了DIOU_Loss。\nDIOU_Loss好的目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比。\n针对IOU和GIOU存在的问题，作者从两个方面进行考虑\n一：如何最小化预测框和目标框之间的归一化距离？二：如何在预测框和目标框重叠时，回归的更准确？\n针对第一个问题，提出了DIOU_Loss（Distance_IOU_Loss）\n\n\nDIOU_Loss考虑了重叠面积和中心点距离，当目标框包裹预测框的时候，直接度量2个框的距离，因此DIOU_Loss收敛的更快。但就像前面好的目标框回归函数所说的，没有考虑到长宽比。\n\n\n比如上面三种情况，目标框包裹预测框，本来DIOU_Loss可以起作用。但预测框的中心点的位置都是一样的，因此按照DIOU_Loss的计算公式，三者的值都是相同的。\nCIOU_LossCIOU_Loss和DIOU_Loss前面的公式都是一样的，不过在此基础上还增加了一个影响因子，将预测框和目标框的长宽比都考虑了进去。\n\n\n其中v是衡量长宽比一致性的参数，我们也可以定义为：\n\n\n这样CIOU_Loss就将目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比全都考虑进去了。\n对比再来综合的看下各个Loss函数的不同点：\nIOU_Loss：主要考虑检测框和目标框重叠面积。GIOU_Loss：在IOU的基础上，解决边界框不重合时的问题。DIOU_Loss：在IOU和GIOU的基础上，考虑边界框中心点距离的信息。CIOU_Loss：在DIOU的基础上，考虑边界框宽高比的尺度信息。\nYolov4中采用了CIOU_Loss的回归方式，使得预测框回归的速度和精度更高一些。\nDIOU_nmsDIoU用作 NMS 的一个因子。该方法在抑制冗余的边界框时会使用 IoU 和两个边界框的中心点之间的距离。这能使得模型能更加稳健地应对有遮挡的情况。在传统NMS中，IoU指标常用于抑制冗余bbox，其中重叠区域是唯一因素，对于遮挡情况经常产生错误抑制。 DIoU-NMS将DIoU作为NMS的准则，因为在抑制准则中不仅应考虑重叠区域，而且还应考虑两个box之间的中心点距离，而DIoU就是同时考虑了重叠区域和两个box的中心距离。 \nDIoU-NMS建议两个中心点较远的box可能位于不同的对象上，不应将其删除(这就是DIoU-NMS的与NMS的最大不同之处)。\n\n\n在上图重叠的摩托车检测中，中间的摩托车因为考虑边界框中心点的位置信息，也可以回归出来。\n因此在重叠目标的检测中，DIOU_nms的效果优于传统的nms。\n\n\n\n\n\n\n\n\n\n这里为什么不用CIOU_nms，而用DIOU_nms?\n答：因为前面讲到的CIOU_loss，是在DIOU_loss的基础上，添加的影响因子，包含groundtruth标注框的信息，在训练时用于回归。但在测试过程中，并没有groundtruth的信息，不用考虑影响因子，因此直接用DIOU_nms即可。\n","slug":"YOLO-V4学习笔记","date":"2021-09-06T10:10:36.000Z","categories_index":"","tags_index":"detection","author_index":"Hulk Wang"},{"id":"82f698705681b0bacc9c6cad3db6d88e","title":"YOLO-V3学习笔记","content":"YOLO-V3学习笔记\n\n\n\n\n\n\n\n\n知识点来源于论文和网络，仅记录学习\n网络结构Backbone\n整个v3结构没有池化层和全连接层\n输出特征图缩小到输入的1/32。所以，通常都要求输入图片是32的倍数\n\n\n\nDBL:代码中的Darknetconv2d_BN_Leaky，是yolo_v3的基本组件。就是卷积+BN+Leaky relu。resn:n代表数字，有res1，res2, … ,res8等等，表示这个res_block里含有多少个res_unit。concat:张量拼接。将darknet中间层和后面的某一层的上采样进行拼接。拼接的操作和残差层add的操作是不一样的，拼接会扩充张量的维度，而add只是直接相加不会导致张量维度的改变。\nOutputyolo v3输出了3个不同尺度的feature map，如上图所示的y1, y2, y3。借鉴了FPN(feature pyramid networks)，采用多尺度来对不同size的目标进行检测，越精细的grid cell就可以检测出越精细的物体(大分辨率y3更能检测小物体，小分辨率y1更能检测大物体)。\ny1,y2和y3的深度都是255，边长分别为13:26:52。对于COCO类别而言，有80个种类，所以每个box应该对每个种类都输出一个概率。\nyolo v3设定的是每个网格单元预测3个box，所以每个box需要有(x, y, w, h, confidence)五个基本参数，然后还要有80个类别的概率。所以3*(5 + 80) = 255。这个255就是这么来的。v3用上采样的方法来实现这种多尺度的feature map，concat连接的两个张量是具有一样尺度的(两处拼接分别是26x26尺度拼接和52x52尺度拼接，通过(2, 2)上采样来保证concat拼接的张量尺度相同)。作者并没有像SSD那样直接采用backbone中间层的处理结果作为feature map的输出，而是和后面网络层的上采样结果进行一个拼接之后的处理结果作为feature map。\nBounding Box在Yolov1中，网络直接回归检测框的宽、高，这样效果有限。所以在Yolov2中，改为了回归基于先验框的变化值，这样网络的学习难度降低，整体精度提升不小。Yolov3沿用了Yolov2中关于先验框的技巧，并且使用k-means对数据集中的标签框进行聚类，得到类别中心点的9个框，作为先验框。在COCO数据集中（原始图片全部resize为416 × 416），九个框分别是 (10×13)，(16×30)，(33×23)，(30×61)，(62×45)，(59× 119)， (116 × 90)， (156 × 198)，(373 × 326) ，顺序为w × h。\nfeature map中的每一个cell都会预测3个边界框（bounding box） ，每个bounding box都会预测三个东西：\n\n每个框的位置（4个值，中心坐标tx和ty，框的高度bh和宽度bw）\n一个objectness prediction \nN个类别\n\n三个output，每个对应的感受野不同，32倍降采样的感受野最大，适合检测大的目标，所以在输入为416×416时，每个cell的三个anchor box为(116 ,90); (156 ,198); (373 ,326)。16倍适合一般大小的物体，anchor box为(30,61); (62,45); (59,119)。8倍的感受野最小，适合检测小目标，因此anchor box为(10,13); (16,30); (33,23)。所以当输入为416×416时，实际总共有（52×52+26×26+13×13）×3=10647个proposal box。\n\n\n\n特征图\n13x13\n26x26\n52x52\n\n\n\n感受野\n大\n中\n小\n\n\n先验框\n(116 ,90)(156 ,198)(373 ,326)\n(30,61) (62,45)(59,119)\n(10,13)(16,30)(33,23)\n\n\n\n\n\n\n\n\n\n\n\n\n这里注意bounding box 与anchor box的区别：Bounding box它输出的是框的位置（中心坐标与宽高），confidence以及N个类别。anchor box只是一个尺度即只有宽高。\nOutput DecodeBounding box decode如上一节所说，v2开始，回归基于先验框的变化值，因此可以通过以下公式解码检测框的x，y，w，h.\n\n\n\n如下图，$\\sigma(t_x)$、$\\sigma(t_y)$是基于矩形框中心点左上角格点坐标的偏移量, $\\sigma$是激活函数，论文中作者使用sigmoid,  $p_w, p_h$是先验框的宽、高，通过上述公式，计算出实际预测框的宽高 $b_w, b_h$.\n\n\n\n\n\n\n\n\n\n\n\n得到对应的$b_w, b_h$后, 还需要乘以特征图对应的的采样率(32,16,8)，得到真实的检测框x,y\nobjectness score decode物体的检测置信度，在Yolo设计中非常重要，关系到算法的检测正确率与召回率。置信度在输出85维中占固定一位，由sigmoid函数解码即可，解码之后数值区间在[0，1]中。\nlogistic回归用于对anchor包围的部分进行一个目标性评分(objectness score)，即这块位置是目标的可能性有多大。这一步是在predict之前进行的，可以去掉不必要anchor，可以减少计算量。作者在论文种的描述如下:\n\n\n\n\n\n\n\n\n\nIf the bounding box prior is not the best but does overlap a ground truth object by more than some threshold we ignore the prediction, following[17]. We use the threshold of 0.5. Unlike [17] our system only assigns one bounding box prior for each ground truth object.\n如果模板框不是最佳的即使它超过我们设定的阈值，我们还是不会对它进行predict。不同于faster R-CNN的是，yolo_v3只会对1个prior进行操作，也就是那个最佳prior。而logistic回归就是用来从9个anchor priors中找到objectness score(目标存在可能性得分)最高的那一个。logistic回归就是用曲线对prior相对于 objectness score映射关系的线性建模。\nClass Prediction decodeCOCO数据集有80个类别，所以类别数在85维输出中占了80维，每一维独立代表一个类别的置信度。使用sigmoid激活函数替代了Yolov2中的softmax，取消了类别之间的互斥，可以使网络更加灵活。\n总结\n9个anchor会被三个输出张量平分的。根据大中小三种size各自取自己的anchor。\n\n作者使用了logistic回归来对每个anchor包围的内容进行了一个目标性评分(objectness score)。根据目标性评分来选择anchor prior进行predict，而不是所有anchor prior都会有输出。\n\n\n训练策略\n\n\n\n\n\n\n\n\nYOLOv3 predicts an objectness score for each bounding box using logistic regression. This should be 1 if the bounding box prior overlaps a ground truth object by more than any other bounding box prior. If the bounding box prior is not the best but does overlap a ground truth object by more than some threshold we ignore the prediction, following [17]. We use the threshold of .5. Unlike [17] our system only assigns one bounding box prior for each ground truth object. If a bounding box prior is not assigned to a ground truth object it incurs no loss for coordinate or class predictions, only objectness.\n预测框一共分为三种情况：正例（positive）、负例（negative）、忽略样例（ignore）。\n正例：任取一个ground truth，与4032个框全部计算IOU，IOU最大的预测框，即为正例。并且一个预测框，只能分配给一个ground truth。例如第一个ground truth已经匹配了一个正例检测框，那么下一个ground truth，就在余下的4031个检测框中，寻找IOU最大的检测框作为正例。ground truth的先后顺序可忽略。正例产生置信度loss、检测框loss、类别loss。预测框为对应的ground truth box标签（需要反向编码，使用真实的x、y、w、h计算出  ）；类别标签对应类别为1，其余为0；置信度标签为1。\n忽略样例：正例除外，与任意一个ground truth的IOU大于阈值（论文中使用0.5），则为忽略样例。忽略样例不产生任何loss。\n负例：正例除外（与ground truth计算后IOU最大的检测框，但是IOU小于阈值，仍为正例），与全部ground truth的IOU都小于阈值（0.5），则为负例。负例只有置信度产生loss，置信度标签为0。\nLossYolov3 Loss为三个特征图Loss之和：\n$Loss = Loss_n1 + Loss_n2 + Loss_n3$\n\n\n\n\n$\\lambda$为权重常数，控制检测框Loss、obj置信度Loss、noobj置信度Loss之间的比例，通常负例的个数是正例的几十倍以上，可以通过权重超参控制检测效果;\n$1^{obj}{ij}$ 若是正例则输出1，否则为0；$1^{noobj}{ij}$ ,若是负例则输出1，否则为0；忽略样例都输出0;\nx、y、w、h使用MSE作为损失函数，也可以使用smooth L1 loss（出自Faster R-CNN）作为损失函数。smooth L1可以使训练更加平滑。置信度、类别标签由于是0，1二分类，所以使用交叉熵作为损失函数。\n\n其他\nground truth为什么不按照中心点分配对应的预测box？\n\n\n\n\n\n\n\n\n\n在Yolov3的训练策略中，不再像Yolov1那样，每个cell负责中心落在该cell中的ground truth。原因是Yolov3一共产生3个特征图，3个特征图上的cell，中心是有重合的。训练时，可能最契合的是特征图1的第3个box，但是推理的时候特征图2的第1个box置信度最高。所以Yolov3的训练，不再按照ground truth中心点，严格分配指定cell，而是根据预测值寻找IOU最大的预测框作为正例。\n\n为什么有忽略样例？\n\n忽略样例是Yolov3中的点睛之笔。由于Yolov3使用了多尺度特征图，不同尺度的特征图之间会有重合检测部分。比如有一个真实物体，在训练时被分配到的检测框是特征图1的第三个box，IOU达0.98，此时恰好特征图2的第一个box与该ground truth的IOU达0.95，也检测到了该ground truth，如果此时给其置信度强行打0的标签，网络学习效果会不理想。\n本身正负样本比例就不均衡（负例&gt;正例），如果强行标为0，会使不均衡更严重。\n\n\n\n","slug":"YOLO-V3学习笔记","date":"2021-08-30T11:59:09.000Z","categories_index":"","tags_index":"detection","author_index":"Hulk Wang"},{"id":"2a4b32b81021e06bffb6e540079ceefc","title":"hexo+github 搭建个人博客","content":"hexo+github搭建个人博客\n\n\n\n\n\n\n\n\n搭建环境:macOs 11.4环境依赖:\n\ngit\nnpm\nnode\nhexo\n\nhexo安装\n安装nodebrew install node\n安装hexonpm install -g hexo-cli\n查看hexo版本hexo -v![version](/images/node_version.jpg ‘’version’’)\n\n建站\n\n\n\n\n\n\n\n\n安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件\n\n初始化hexo框架hexo init &lt;folder&gt;\n移动到目标目录cd &lt;folder&gt;\n安装依赖组件npm install \n生成静态文件hexo g\n开启本地服务器hexo s\n\n\n\n\n\n\n\n\n\n\n在浏览器中输入 http://localhost:4000 回车就可以预览效果了\n更换主题\n\n\n\n\n\n\n\n\n此时博客是hexo默认主题，比较普通，这里推荐一个主题：Aurora\n安装教程\n效果预览\n配置github\n建立respositoryrepository名称为username.github.io\n\n修改配置文件\n\n\n\n\n\n\n\n\n\n_config.yml文件\n\n\ndeploy:  \n\ttype: git \n\trepository: https:&#x2F;&#x2F;github.com&#x2F;username&#x2F;username.github.io.git\n\tbranch: master\n\n安装一个部署插件npm install hexo-deployer-git --save\n\n重新生成部署hexo g -d\n\n\n\n\n\n\n\n\n\n\n\n此时可通过 https://username.github.io 访问博客\n配置个性域名\n\n\n\n\n\n\n\n\n这里我购买了腾讯云的域名: hulk.show\n\n配置域名 \n\n\n\n\n\n\n\n\n\n进入域名管理界面，选择解析，添加两条解析：\n\n配置git\n\n\n\n\n\n\n\n\n\n你的项目-&gt;Setting-&gt;Pages-&gt;Custom domain,添加你的域名：\n\n\n\n\n\n\n\n\n\n\n\n可能需要等几分钟,即可通过购买的域名访问博客：www.hulk.show\n","slug":"hexo-github-搭建个人博客","date":"2021-08-29T10:50:28.000Z","categories_index":"","tags_index":"config","author_index":"Hulk Wang"}]