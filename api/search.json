[{"id":"06a101cf6ba6fad0463c3458164de1c3","title":"Faster RCNN 记录","content":"\n\n\n\n\n\n\n\n\n参考来源：https://zhuanlan.zhihu.com/p/31426458https://zhuanlan.zhihu.com/p/86403390\n\n Faster RCNN基本结构\n\nFaster RCNN其实可以分为4个主要内容：\n\n特征提取：Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。\nRPN：RPN网络用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。\nRoi Pooling：该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。\nClassification：利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。\n\n\n Faster RCNN(VGG16)网络结构\n\n特征提取没啥可说的，特征提取，输入MxN，VGG16为例，4个pooling层，得到的feature map分辨率为：（M/16）x（N/16）\nRPN\n RPN结构\n\nRPN网络实际分为2条线，上面一条通过softmax分类anchors获得positive和negative分类，下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合positive anchors和对应bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。\nanchors以tf代码为例：\ndef _anchor_component(self):  #获得锚的数量和位置\n    with tf.variable_scope(&#39;ANCHOR_&#39; + &#39;default&#39;):\n        # just to get the shape right  只是为了让形状正确\n        height &#x3D; tf.to_int32(tf.ceil(self._im_info[0, 0] &#x2F; np.float32(self._feat_stride[0]))) #高度为图片高&#x2F;16,，就是特征图的高，tf.ceil向上取整\n        width &#x3D; tf.to_int32(tf.ceil(self._im_info[0, 1] &#x2F; np.float32(self._feat_stride[0]))) #宽度为图片宽&#x2F;16，为特征图的宽\n        anchors, anchor_length &#x3D; tf.py_func(generate_anchors_pre,\n                                            [height, width,                                        self._feat_stride,self._anchor_scales, self._anchor_ratios],\n                                            [tf.float32, tf.int32], name&#x3D;&quot;generate_anchors&quot;)  #构建生成锚的py函数，这个锚有9*（50*38）个，anchor_length是锚的个数\n        anchors.set_shape([None, 4])  #锚定义为4列\n        anchor_length.set_shape([]) #行向量，length为锚的个数\n        self._anchors &#x3D; anchors\n        self._anchor_length &#x3D; anchor_length  #length为特征图面积*9\n 函数generate_anchors_pre来生成anchor：\ndef generate_anchors_pre(height, width, feat_stride, anchor_scales&#x3D;(8, 16, 32), anchor_ratios&#x3D;(0.5, 1, 2)):\n    &quot;&quot;&quot; A wrapper function to generate anchors given different scales\n      Also return the number of anchors in variable &#39;length&#39; 给定不同比例生成锚点的包装函数也返回可变“长度”的锚点数量\n    &quot;&quot;&quot;\n    anchors &#x3D; generate_anchors(ratios&#x3D;np.array(anchor_ratios), scales&#x3D;np.array(anchor_scales))\n    A &#x3D; anchors.shape[0] #anchor的数量，为9\n    shift_x &#x3D; np.arange(0, width) * feat_stride  #将特征图的宽度进行16倍延伸至原图，以width&#x3D;4为例子，则shfit_x&#x3D;[0,16,32,48]\n    shift_y &#x3D; np.arange(0, height) * feat_stride  #将特征图的高度进行16倍衍生至原图\n    shift_x, shift_y &#x3D; np.meshgrid(shift_x, shift_y)  #生成原图的网格点\n    shifts &#x3D; np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel())).transpose()  #若width&#x3D;50，height&#x3D;38，生成（50*38）*4的数组\n    #如 [[0,0,0,0],[16,0,16,0],[32,0,32,0].......]，shift中的前两个坐标和后两个一样（保持右下和左上的坐标一样），是从左到右，从上到下的坐标点（映射到原图）\n    K &#x3D; shifts.shape[0]  #k&#x3D;50*38\n    # width changes faster, so here it is H, W, C\n    anchors &#x3D; anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2))  #列数相同的list相加就是简单的添加，而数组不一样，1*9*4和（50*38）*1*4进行相加，生成了（50*38）*9*4的数组\n    #其实意思就是右下角坐标和左上角的左边都加上同一个变换坐标\n    anchors &#x3D; anchors.reshape((K * A, 4)).astype(np.float32, copy&#x3D;False) #三维变两维，（50*38*9，4），此处就是将特征层的anchor坐标转到原图上的区域\n    length &#x3D; np.int32(anchors.shape[0])  #length&#x3D;50*38*9\n\n    return anchors, length\n\n观察第一行代码，即\nanchors &#x3D; generate_anchors(ratios&#x3D;np.array(anchor_ratios),scales&#x3D;np.array(anchor_scales))\n 这里是3x3=9个基础anchor的生成，我们进入函数generate_anchors中，发现其实生成就是一个数组，这个数组是：\n# array([[ -83.,  -39.,  100.,   56.],\n#       [-175.,  -87.,  192.,  104.],\n#       [-359., -183.,  376.,  200.],\n#       [ -55.,  -55.,   72.,   72.],\n#       [-119., -119.,  136.,  136.],\n#       [-247., -247.,  264.,  264.],\n#       [ -35.,  -79.,   52.,   96.],\n#       [ -79., -167.,   96.,  184.],\n#       [-167., -343.,  184.,  360.]])\n其中每行的4个值（x1，y1, x2, y2）表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为{1:1, 1:2, 2:1}三种，实际上通过anchors就引入了检测中常用到的多尺度方法。\n生成这个数组的代码是：\ndef generate_anchors(base_size&#x3D;16, ratios&#x3D;[0.5, 1, 2],\n                     scales&#x3D;2 ** np.arange(3, 6)):\n    &quot;&quot;&quot;\n    Generate anchor (reference) windows by enumerating aspect ratios X\n    scales wrt a reference (0, 0, 15, 15) window.  通过枚举参考(0，0，15，15)窗口的长宽比来生成锚(参考)窗口。\n    &quot;&quot;&quot;\n\n    base_anchor &#x3D; np.array([1, 1, base_size, base_size]) - 1  #生成一个base_anchor &#x3D; [0, 0, 15, 15]，其中(0, 0)是anchor左上点的坐标\n    # (15, 15)是anchor右下点的坐标，那么这个anchor的中心点的坐标是(7.5, 7.5)\n    ratio_anchors &#x3D; _ratio_enum(base_anchor, ratios)#然后产生ratio_anchors，就是将base_anchor和ratios[0.5, 1, 2],ratio_anchors生成三个anchors\n    # 传入到_ratio_enum()函数，ratios代表的是三种宽高比。\n    anchors &#x3D; np.vstack([_scale_enum(ratio_anchors[i, :], scales)  #在刚刚3个anchor基础上继续生成anchor\n                         for i in range(ratio_anchors.shape[0])])\n    return anchors\n我们发现这个数组中的每行数据（如第一行：[ -83., -39., 100., 56.] )，它们的中心位置都为（7.5，7.5），即（0，0，15，15）的中心（因为通过代码也可以得知，这9个基础框的生成也是以（0，0，15，15）为基础的，代码中base_anchor就是（0，0，15，15））。\n通过3种anchor ratio和3种anchor scale生成的9个数组，这9个数组在坐标图上如图2所示。\n anchor\n\n\n\n\n\n\n\n\n\n\n为什么这样设计呢？我们知道，其实一张图片通过特征提取网路VGG16后，长宽比都缩小了16倍得到了特征图。比如原先的800*600的原图通过VGG16后得到了50*38的特征图（以上先不考虑通道数），我们就假设，特征图上的每一个点（大小为1*1），和原图16*16区域对应（这里记住，对应不是指代感受野，只是便于理解).这里，使用对应的好处，就是特征图上的每个点（大小为1*1）负责由原图对应区域(大小为16*16）中心生成的9个anchor的训练和学习。所以Faster RCNN共产生50x38x9=17100个anchor，基本覆盖了全图各个区域。\n看generate_anchors_pre函数，shifts就是对（shift_x, shift_y）进行组合，其中shift_x是对x坐标进行移动，shift_y是对y坐标进行移动，综合起来就是将基础的中心为（7.5，7.5）的9个anchor平移到全图上，覆盖所有可能的区域。\nanchors &#x3D; anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)) #列数相同的list相加就是简单的添加，而数组不一样，1*9*4和（50*38）*1*4进行相加，生成了（50*38）*9*4的数组\n#其实意思就是右下角坐标和左上角的左边都加上同一个变换坐标\nanchors &#x3D; anchors.reshape((K * A, 4)).astype(np.float32, copy&#x3D;False) #三维变两维，（50*38*9，4），此处就是将特征层的anchor坐标转到原图上的区域\nlength &#x3D; np.int32(anchors.shape[0]) #length&#x3D;50*38*9\n上述代码就是完成了9个base anchor 的移动，输出结果就是50389个anchor。那么到此，所有的anchor都生成了，当然了，所有的anchor也和特征图产生了一一对应的关系了。\n\n\n\n\n\n\n\n\n\nanchor是辅助模型进行训练的，能让模型对物体的大小和形状有个大致的认知，也算是人为添加的先验知识了。\nRPN最终就是在原图尺度上，设置了密密麻麻的候选Anchor。然后用cnn去判断哪些Anchor是里面有目标的positive anchor，哪些是没目标的negative anchor。所以，仅仅是个二分类而已！\n\n\n解释一下上面这张图的数字。\n\n在原文中使用的是ZF model中，其Conv Layers中最后的conv5层num_output=256，对应生成256张特征图，所以相当于feature map每个点都是256-dimensions\n在conv5之后，做了rpn_conv/3x3卷积且num_output=256，相当于每个点又融合了周围3x3的空间信息\n假设在conv5 feature map中每个点上有k个anchor（默认k=9），而每个anhcor要分positive和negative，所以每个点由256d feature转化为cls=2•k scores；而每个anchor都有(x, y, w, h)对应4个偏移量，所以reg=4•k coordinates\n全部anchors拿去训练太多了，训练程序会在合适的anchors中随机选取128个postive anchors+128个negative anchors进行训练\n\nrpn中的二分类一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设 W=M/16，H=N/16。在进入reshape与softmax之前，先做了1x1卷积（此时anchor已经生成完毕），如图9：\n\n\n可以看到其num_output=18，也就是经过该卷积的输出图像为WxHx18大小,这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是positive和negative，所有这些信息都保存WxHx(9*2)大小的矩阵。\n\n\n\n\n\n\n\n\n\n综上所述，RPN网络中利用anchors和softmax初步提取出positive anchors作为候选区域（另外也有实现用sigmoid代替softmax，输出[1, 1, 9xH, W]后接sigmoid进行positive/negative二分类，原理一样）。\nbounding box regression\n\n\n\n\n\n\n\n\n在RPN网络中，进行bbox regression得到的是每个anchor的偏移量。再与anchor的坐标进行调整以后，得到proposal的坐标，经过一系列后处理，比如NMS，top-K操作以后，得到得分最高的前N个proposal传入分类网络。\n如图所示绿色框为飞机的Ground Truth(GT)，红色为提取的positive anchors，即便红色的框被分类器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机。所以我们希望采用一种方法对红色的框进行微调，使得positive anchors和GT更加接近。\n\n\n对于窗口一般使用四维向量 (x, y, w, h)表示，分别表示窗口的中心点坐标和宽高。对于下图，红色的框A代表原始的positive Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G’，即：\n\n\n\n给定anchor A=($A_x$, $A_y$, $A_w$, $A_h$)和GT=[$G_x$, $G_y$, $G_w$, $G_h$]\n寻找一种变换F，使得：F($A_x$, $A_y$, $A_w$, $A_h$)= ($G_x’$, $G_y’$, $G_w’$, $G_h’$),其中($G_x’$, $G_y’$, $G_w’$, $G_h’$)≈($G_x$, $G_y$, $G_w$, $G_h$)\n\n比较简单的思路就是:\n\n\n观察上面4个公式发现，需要学习的是$d_x(A)$,$d_y(A)$,$d_w(A)$,$d_h(A)$ 这四个变换。当输入的anchor A与GT相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调（注意，只有当anchors A和GT比较接近时，才能使用线性回归模型，否则就是复杂的非线性问题了）。\n接下来的问题就是如何通过线性回归获$d_x(A)$,$d_y(A)$,$d_w(A)$,$d_h(A)$ 了。线性回归就是给定输入的特征向量X, 学习一组参数W, 使得经过线性回归后的值跟真实值Y非常接近，即Y=WX.\n对于该问题，输入X是cnn feature map，定义为Φ；同时还有训练传入A与GT之间的变换量，即$t_x$,$t_y$,$t_w$,$t_h$。输出是$d_x(A)$,$d_y(A)$,$d_w(A)$,$d_h(A)$四个变换。那么目标函数可以表示为:\n\n\n所以在RPN中，bounding box regression通过第二条线完成：\n\n RPN中的bbox reg \n\n可以看到经过该卷积输出图像为WxHx36，在caffe blob存储为[1, 4x9, H, W]，这里相当于feature maps每个点都有9个anchors，每个anchors又都有4个用于回归的\nVGG输出(M/16)*(N/16)*256的特征，对应设置 (M/16)*(N/16)*k 个anchors，而RPN输出：\n\n大小为(M/16)*(N/16)*2k的positive/negative softmax分类特征矩阵\n大小为(M/16)*(N/16)*4k的regression坐标回归特征矩阵\n\n恰好满足RPN完成positive/negative分类+bounding box regression坐标回归.\nProposal LayerProposal Layer负责综合所有 $d_x(A)$,$d_y(A)$,$d_w(A)$,$d_h(A)$ 变换量和positive anchors，计算出精准的proposal，送入后续RoI Pooling Layer。\nProposal Layer有3个输入：\n\n分类器结果（positive vs negative anchorsrpn_cls_prob_reshape)\nbbox reg的 $d_x(A)$,$d_y(A)$,$d_w(A)$,$d_h(A)$变换量rpn_bbox_pred\nim_info；\n另外还有参数feat_stride=16，这和图4是对应的。\n\n首先解释im_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定MxN，im_info=[M, N, scale_factor]则保存了此次缩放的所有信息。然后经过Conv Layers，经过4次pooling变为WxH=(M/16)x(N/16)大小，其中feature_stride=16则保存了该信息，用于计算anchor偏移量。\nProposal Layer forward按照以下顺序依次处理：1.生成anchors，利用$d_x(A)$,$d_y(A)$,$d_w(A)$,$d_h(A)$对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致）2. 按照输入的positive softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的positive anchors3. 限定超出图像边界的positive anchors为图像边界，防止后续roi pooling时proposal超出图像边界4. 剔除尺寸非常小的positive anchors5. 对剩余的positive anchors进行NMS（nonmaximum suppression）6. Proposal Layer有3个输入：positive和negative anchors分类器结果rpn_cls_prob_reshape，对应的bbox reg的(e.g. 300)结果作为proposal输出\n之后输出proposal=[x1, y1, x2, y2]，注意，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应MxN输入图像尺度的，这点在后续网络中有用。\n\n\n\n\n\n\n\n\n\nRPN网络结构就介绍到这里，总结起来就是：生成anchors -&gt; softmax分类器提取positvie anchors -&gt; bbox reg回归positive anchors -&gt; Proposal Layer生成proposals\nRPN生成RoIsRPN在自身训练的同时，还会提供RoIs（region of interests）给Fast RCNN（RoIHead）作为训练样本。RPN生成RoIs的过程(ProposalCreator)如下：\n\n对于每张图片，利用它的feature map， 计算 (H/16)× (W/16)×9（大概20000）个anchor属于前景的概率，以及对应的位置参数。\n选取概率较大的12000个anchor\n利用回归的位置参数，修正这12000个anchor的位置，得到RoIs\n利用非极大值（(Non-maximum suppression, NMS）抑制，选出概率最大的2000个RoIs\n\n注意：在inference的时候，为了提高处理速度，12000和2000分别变为6000和300.\n注意：这部分的操作不需要进行反向传播，因此可以利用numpy/tensor实现。\nROI poolingRoI Pooling层则负责收集proposal，并计算出proposal feature maps，送入后续网络。从图2中可以看到Rol pooling层有2个输入：\n\n原始的feature maps\nRPN输出的proposal boxes（大小各不相同）\n\n为何需要RoI Pooling当网络训练好后输入的图像尺寸必须是固定值，同时网络输出也是固定大小的vector or matrix。如果输入图像大小不定，这个问题就变得比较麻烦。有2种解决办法：\n\n从图像中crop一部分传入网络\n将图像warp成需要的大小后传入网络\n\n无论采取那种办法都不好，要么crop后破坏了图像的完整结构，要么warp破坏了图像原始形状信息。\nRPN网络生成的proposals的方法：对positive anchors进行bounding box regression，那么这样获得的proposals也是大小形状各不相同，即也存在上述问题。所以Faster R-CNN中提出了RoI Pooling解决这个问题.\nRoI Pooling layer forward\n由于proposal是对应MxN尺度的，所以首先使用spatial_scale参数将其映射回(M/16)x(N/16)大小的feature map尺度；\n再将每个proposal对应的feature map区域水平分为 pooled_w*\\pooled_h的网格；\n对网格的每一份都进行max pooling处理。\n\n这样处理后，即使大小不同的proposal输出结果都是pooled_w*\\pooled_h固定大小，实现了固定长度输出。\n为什么要pooling成7×7的尺度？是为了能够共享权重。在之前讲过，除了用到VGG前几层的卷积之外，最后的全连接层也可以继续利用。当所有的RoIs都被pooling成（512×7×7）的feature map后，将它reshape 成一个一维的向量，就可以利用VGG16预训练的权重，初始化前两层全连接。最后再接两个全连接层，分别是：\nFC 21 用来分类，预测RoIs属于哪个类别（20个类+背景）FC 84 用来回归位置（21个类，每个类都有4个位置参数）\nClassificationClassification部分利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出cls_prob概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量bbox_pred，用于回归更加精确的目标检测框。Classification部分网络结构如图:\n\n Classification部分网络结构 \n\n从RoI Pooling获取到7x7=49大小的proposal feature maps后，送入后续网络，可以看到做了如下2件事：\n\n通过全连接和softmax对proposals进行分类，这实际上已经是识别的范畴了\n再次对proposals进行bounding box regression，获取更高精度的rect box\n\nAnchor到底与网络输出如何对应VGG输出 50*38*512的特征，对应设置 50*38*k个anchors，而RPN输出50*38*2k的分类特征矩阵和50*38*4k的坐标回归特征矩阵。\n\n\n其实在实现过程中，每个点的2k个分类特征与 4k回归特征，与 k个anchor逐个对应即可，这实际是一种“人为设置的逻辑映射”。当然，也可以不这样设置，但是无论如何都需要保证在训练和测试过程中映射方式必须一致。\nLoss\n\n上述公式中 i 表示anchors index， $p_i$表示positive softmax probability，$p_i^*$代表对应的GT predict概率（即当第i个anchor与GT间IoU&gt;0.7，认为是该anchor是positive，$p_i^*$=1  ；反之IoU&lt;0.3时，认为是该anchor是negative，$p_i^*$=0  ；**至于那些0.3&lt;IoU&lt;0.7的anchor则不参与训练**）；t  代表predict bounding box，  代表对应positive anchor对应的GT box。可以看到，整个Loss分为2部分：\n \n \n 在训练Faster RCNN的时候有四个损失：\n\nRPN 分类损失：anchor是否为前景（二分类）\nRPN位置回归损失：anchor位置微调\nRoI 分类损失：RoI所属类别（21分类，多了一个类作为背景）\nRoI位置回归损失：继续对RoI位置微调\n\n四个损失相加作为最后的损失，反向传播，更新参数。\n注意1\n在RPN的时候，已经对anchor做了一遍NMS，在RCNN测试的时候，还要再做一遍\n在RPN的时候，已经对anchor的位置做了回归调整，在RCNN阶段还要对RoI再做一遍\n在RPN阶段分类是二分类，而Fast RCNN阶段是21分类(voc)\n\n2RPN会产生大约2000个RoIs，这2000个RoIs不是都拿去训练，而是选择128个RoIs用以训练。选择的规则如下：\n\nRoIs和gt_bboxes 的IoU大于0.5的，选择一些（比如32个）\n选择 RoIs和gt_bboxes的IoU小于等于0（或者0.1）的选择一些（比如 128-32=96个）作为负样本\n\n为了便于训练，对选择出的128个RoIs，还对他们的gt_roi_loc 进行标准化处理（减去均值除以标准差）\n","slug":"Faster-RCNN-记录","date":"2021-10-11T07:47:29.000Z","categories_index":"","tags_index":"","author_index":"Hulk Wang"},{"id":"7f399d762217f0410e81da356daa491f","title":"FCOS学习笔记","content":"FCOS学习笔记\n\n\n\n\n\n\n\n\n笔记来源：https://blog.csdn.net/WZZ18191171661/article/details/89258086https://zhuanlan.zhihu.com/p/339023466\nFCOS是一个基于FCN的per-pixel、anchor free的one-stage目标检测算法，论文全:《FCOS: Fully Convolutional One-Stage Object Detection》\nAnchor-based不足：\n\nanchor会引入很多需要优化的超参数， 比如anchor number、anchor size、anchor ratio等；\n为了保证算法效果，需要很多的anchors，存在正负样本类别不均衡问题；\n在训练的时候，需要计算所有anchor box同ground truth boxes的IoU，计算量较大；\n\nFCOS优势：\n\n因为输出是pixel-based预测，所以可以复用semantic segmentation方向的相关tricks；\n可以修改FCOS的输出分支，用于解决instance segmentation和keypoint detection任务；\n\n\n FCOS网络结构\n\n实现细节与Anchor Base对比对于基于anchors的目标检测算法而言，我们将输入的图片送入backbone网络之后，会获得最终的feature_map，比如说是17x17x256；然后我们会在该feature_map上的每一位置上使用预先定义好的anchors。而FCOS的改动点就在这里，它是直接在feature_map上的每一点进行回归操作。\n具体的实施思路如下所示：\n\n我们可以将feature_map中的每一个点(x,y)映射回原始的输入图片中:(⌊s/2⌋ + xs, ⌊s/2⌋ + ys)其中: s为步长，(x,y)为改点对应feature map上的坐标.\n\n如果这个映射回原始输入的点在相应的GT的bbox范围之内，而且类别标签对应，我们将其作为训练的正样本块，否则将其作为正样本块；\n\n回归的目标是(l,t,r,b)，即中心点做bbox的left、top、right和bottom之间的距离，具体如下图所示：\n\n\n如果一个位置在多个bbox的内部的话，如右图，针对这样样本文中采样的方法是直接选择择面积最小的边界框作为其回归目标。由于网络中FPN的存在，导致这样的模糊样本的数量大大减少。\n\n如果这个位置(x,y)和一个bbox关联的话，该位置处的训练回归目标可制定为:其中(x1,y1)和(x2,y2)分别表示bbox的左上角和右下角坐标值。\n\n\n由于FCOS可以通过这样方式获得很多正样本块，使用这样的正样本块进行回归操作，因此获得了比较好的性能提升，而原始的基于anchor的算法需要通过计算预设的anchor和对应的GT之间的IOU值，当该IOU值大于设定的阈值时才将其看做正样本块。\n\n\nLoss\n\nloss函数如上图所示，包含两部分，Lcls表示分类loss，本文使用的是Focal_loss；Lreg表示回归loss，本文使用的是IOU loss。\ncenter-ness分支Center-ness表示的是(x,y)距目标中心的标准化后的距离，为了制止过多的低质量离目标中心远的检测框而设计。\n\n\n\n\n如上图，红色到蓝色表示center-ness从1到0，因为center-ness是在0-1之间，所以用的BCE loss，这个loss会一起加到上面我们提到的loss function中。在测试时，检测框的排序分数由center-ness乘上分类的分数。如果还有低质量的框，最后可用NMS来剔除。\n","slug":"FCOS学习笔记","date":"2021-09-13T08:19:35.000Z","categories_index":"","tags_index":"detection","author_index":"Hulk Wang"},{"id":"57d3aa7cdcc56700d35ee7aa4e8c6558","title":"YOLO-V5学习笔记","content":"YOLO-V5学习笔记\n\n\n\n\n\n\n\n\n知识点来源于网络，仅记录学习来源：https://zhuanlan.zhihu.com/p/172121380\n网络结构\n Yolov5s网络结构(来源见水印)\n\n\n如上图为yolov5的整体网络结构，跟yolov4一样，分别按input、backbone、Neck以及Prediction四部分来理解。\n\n\n\n\n\n\n\n\n\nYolov5官方代码中，一共有4个版本，分别是Yolov5s、Yolov5m、Yolov5l、Yolov5x四个模型。Yolov5s是Yolov5系列中深度最小，特征图的宽度最小的网络。后面的3种都是在此基础上不断加深，不断加宽。\n输入端Mosaic数据增强与v4一样，采用Mosaic数据增强；\n自适应锚框计算将anchor初始计算(聚类)集成到训练代码中；\n自适应图片缩放针对inference阶段的优化\n在常用的目标检测算法中，不同的图片长宽都不相同，因此常用的方式是将原始图片统一缩放到一个标准尺寸，再送入检测网络中。\n\n 传统方法(来源见水印)\n\n但Yolov5代码中对此进行了改进，也是Yolov5推理速度能够很快的一个不错的trick。\n作者认为，在项目实际使用时，很多图片的长宽比不同，因此缩放填充后，两端的黑边大小都不同，而如果填充的比较多，则存在信息冗余，影响推理速度。\n因此在Yolov5的代码中datasets.py的letterbox函数中进行了修改，对原始图像自适应的添加最少的黑边。\n\n yolov5(来源见水印)\n\n举例说明填充方法：原始：800x600目标：416\n\n选择小的缩放系数，短边:min(416/800, 416/600);\n得到新的尺寸(即长边resize到目标尺寸,短边按原始长宽比变换）: (416,312);\n计算pad大小(找到大于312且能被32整除的最小整数): (416 - 312) mod 32 = 8 所以pad值为8/2=4\n\nBackboneFocus结构\n focus(来源见水印)\n\nyolov5中，Focus模块位于backbone前。具体操作是在一张图片中每隔一个像素拿到一个值，类似于邻近下采样，这样就拿到了四张近似下采样的图片，但是没有信息丢失。相当于w,h变为1/2，输入通道扩充了4倍，最后将得到的新图片再经过卷积操作，最终得到了没有信息丢失情况下的二倍下采样特征图。\n以yolov5s为例，原始的640 × 640 × 3的图像输入Focus结构，采用切片操作，先变成320 × 320 × 12的特征图，再经过一次卷积操作，最终变成320 × 320 × 32的特征图。\n具体代码实现：\n\n\n目的和作用： Focus是为了提速，和mAP无关，减少了计算量和参数量。\nThe YOLOv5 Focus layer replaces the first 3 YOLOv3 layers with a single layer:\n\n\n详见作者解答\nCSP结构Yolov5与Yolov4不同点在于，Yolov4中只有主干网络使用了CSP结构。\n而Yolov5中设计了两种CSP结构，以Yolov5s网络为例，CSP1_X结构应用于Backbone主干网络，另一种CSP2_X结构则应用于Neck中。\n\n\nNeck\nYolov5现在的Neck和Yolov4中一样，都采用FPN+PAN的结构.\n如CSP结构中讲到，Yolov5和Yolov4的不同点在于，Yolov4的Neck结构中，采用的都是普通的卷积操作。而Yolov5的Neck结构中，采用借鉴CSPnet设计的CSP2结构，加强网络特征融合的能力。\n\n\n\nPredictionBounding box损失函数Yolov5: GIOU_LossYolov4: CIOU_Loss\nnms非极大值抑制Yolov4: DIOU_nmsYolov5: 加权nms\n\n\n\n\n\n\n\n\n\nWeighted NMS出现于ICME Workshop 2017《Inception Single Shot MultiBox Detector for object detection》一文中。论文认为Traditional NMS每次迭代所选出的最大得分框未必是精确定位的，冗余框也有可能是定位良好的。那么与直接剔除机制不同，Weighted NMS顾名思义是对坐标加权平均，加权平均的对象包括M自身以及IoU≥NMS阈值的相邻框。加权的权重为 : ，表示得分与IoU的乘积。优点：Weighted NMS通常能够获得更高的Precision和Recall，只要NMS阈值选取得当，Weighted NMS均能稳定提高AP与AR，无论是AP50还是AP75，也不论所使用的检测模型是什么。缺点：顺序处理模式，且运算效率比Traditional NMS更低。加权因子是IoU与得分，前者只考虑两个框的重叠面积，这对描述box重叠关系或许不够全面；而后者受到定位与得分不一致问题的限制。\n","slug":"YOLO-V5学习笔记","date":"2021-09-09T07:42:37.000Z","categories_index":"","tags_index":"detection","author_index":"Hulk Wang"},{"id":"f45a749a54e27d0a1f22b1f23eadc80a","title":"Pixel-Level Domain Transfer论文复现","content":"Pixel-Level Domain Transfer论文复现博客迁移，原文链接\n\n\n\n\n\n\n\n\n\nAbstract.：We present an image-conditional image generation model. The model transfers an input domain to a target domain in semantic level, and generates the target image in pixel level. To generate realistic target images, we employ the real/fake-discriminator as in Generative Adversarial Nets, but also introduce a novel domain-discriminator to make the generated image relevant to the input image. We verify our model through a challenging task of generating a piece of clothing from an input image of a dressed person. We present a high quality clothing dataset containing the two domains, and succeed in demonstrating decent results.\n论文简述  整篇论文比较容易懂，主要内容就是把输入domain转换到目标domain，输入一张模特图片，得到上衣图片，如下：\n文章主要贡献主要在两个方面：\nLookBook数据集下载地址（uj3j）\n\n\n基于Gan的转换框架网络结构如下：\n\n\n生成网络是encoder-decoder结构，判别网络有两个：Dr和Da。\nDr就是一个基本的Gan的判别网络，判别fake或real；Da主要用来判断生成图像与输入是否配对，所以Dr输入是生成网络的输入和输出的concat.\n整个过程很容易懂，细节看原文即可.\n论文复现Generator：输入64x64x3图像，输出64x64x3生成图像\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n \n        def conv_block(in_channels, out_channels, kernel_size, stride&#x3D;1,\n                 padding&#x3D;0, bn&#x3D;True, a_func&#x3D;&#39;lrelu&#39;):\n \n            block &#x3D; nn.ModuleList()\n            block.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func &#x3D;&#x3D; &#39;lrelu&#39;:\n                block.append(nn.LeakyReLU(0.2))\n            elif a_func &#x3D;&#x3D; &#39;relu&#39;:\n                block.append(nn.ReLU())\n            else:\n                pass\n \n            return block\n \n        def convTranspose_block(in_channels, out_channels, kernel_size, stride&#x3D;2,\n                 padding&#x3D;0, output_padding&#x3D;0, bn&#x3D;True, a_func&#x3D;&#39;relu&#39;):\n            &#39;&#39;&#39;\n            H_out &#x3D; (H_in - 1) * stride - 2 * padding + kernel_size + output_padding\n            :param in_channels:\n            :param out_channels:\n            :param kernel_size:\n            :param stride:\n            :param padding:\n            :param output_padding:\n            :param bn:\n            :param a_func:\n            :return:\n            &#39;&#39;&#39;\n            block &#x3D; nn.ModuleList()\n            block.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride,\n                 padding, output_padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func &#x3D;&#x3D; &#39;lrelu&#39;:\n                block.append(nn.LeakyReLU(0.2))\n            elif a_func &#x3D;&#x3D; &#39;relu&#39;:\n                block.append(nn.ReLU())\n            else:\n                pass\n \n            return block\n \n \n        def encoder():\n            conv_layer &#x3D; nn.ModuleList()\n            conv_layer +&#x3D; conv_block(3, 128, 5, 2, 2, False)    # 32x32x128\n            conv_layer +&#x3D; conv_block(128, 256, 5, 2, 2)        # 16x16x256\n            conv_layer +&#x3D; conv_block(256, 512, 5, 2, 2)         # 8x8x512\n            conv_layer +&#x3D; conv_block(512, 1024, 5, 2, 2)       # 4x4x1024\n            conv_layer +&#x3D; conv_block(1024, 64, 4, 1)          # 1x1x64\n            return conv_layer\n \n        def decoder():\n            conv_layer &#x3D; nn.ModuleList()\n            conv_layer +&#x3D; conv_block(64, 4 * 4 * 1024, 1, a_func&#x3D;&#39;relu&#39;)\n            conv_layer.append(Reshape((1024, 4, 4)))                            # 4x4x1024\n            conv_layer +&#x3D; convTranspose_block(1024, 512, 4, 2, 1)               # 8x8x512\n            conv_layer +&#x3D; convTranspose_block(512, 256, 4, 2, 1)                # 16x16x256\n            conv_layer +&#x3D; convTranspose_block(256, 128, 4, 2, 1)                # 32x32x128\n            conv_layer +&#x3D; convTranspose_block(128, 3, 4, 2, 1, bn&#x3D;False, a_func&#x3D;&#39;&#39;)     # 64x64x3\n            conv_layer.append(nn.Tanh())\n            return conv_layer\n \n        self.net &#x3D; nn.Sequential(\n            *encoder(),\n            *decoder(),\n        )\n \n    def forward(self, input):\n        out &#x3D; self.net(input)\n        return out\n\nDiscriminatorR输入64x64x3图像，输出real or fake；\nclass DiscriminatorR(nn.Module):\n    def __init__(self):\n        super(DiscriminatorR, self).__init__()\n \n        def conv_block(in_channels, out_channels, kernel_size, stride&#x3D;1,\n                       padding&#x3D;0, bn&#x3D;True, a_func&#x3D;True):\n \n            block &#x3D; nn.ModuleList()\n            block.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func:\n                block.append(nn.LeakyReLU(0.2))\n \n            return block\n \n \n        self.net &#x3D; nn.Sequential(\n            *conv_block(3, 128, 5, 2, 2, False),                            # 32x32x128\n            *conv_block(128, 256, 5, 2, 2),                                 # 16x16x256\n            *conv_block(256, 512, 5, 2, 2),                                 # 8x8x512\n            *conv_block(512, 1024, 5, 2, 2),                                # 4x4x1024\n            *conv_block(1024, 1, 4, bn&#x3D;False, a_func&#x3D;False),                # 1x1x1\n            nn.Sigmoid(),\n        )\n \n    def forward(self, img):\n        out &#x3D; self.net(img)\n        return out\n\nDiscriminatorA输入64x64x6的concat图像，输出real or fake；\nclass DiscriminatorA(nn.Module):\n    def __init__(self):\n        super(DiscriminatorA, self).__init__()\n \n        def conv_block(in_channels, out_channels, kernel_size, stride&#x3D;1,\n                       padding&#x3D;0, bn&#x3D;True, a_func&#x3D;True):\n \n            block &#x3D; nn.ModuleList()\n            block.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func:\n                block.append(nn.LeakyReLU(0.2))\n \n            return block\n \n        self.net &#x3D; nn.Sequential(\n            *conv_block(6, 128, 5, 2, 2, False),                # 32x32x128\n            *conv_block(128, 256, 5, 2, 2),                     # 16x16x256\n            *conv_block(256, 512, 5, 2, 2),                     # 8x8x512\n            *conv_block(512, 1024, 5, 2, 2),                    # 4x4x1024\n            *conv_block(1024, 1, 4, bn&#x3D;False, a_func&#x3D;False),    # 1x1x1\n            nn.Sigmoid(),\n        )\n \n    def forward(self, img):\n        out &#x3D; self.net(img)\n        return out\n\nloss与原文不同，在生成损失上加了mse\ngen_loss_d &#x3D; self.adversarial_loss(torch.squeeze(gen_output), real_label)\ngen_loss_a &#x3D; self.adversarial_loss(torch.squeeze(gen_output_a), real_label)\nmse_loss &#x3D; self.mse_loss(gen_target_batch, target_batch)\n\n完整训练测试代码：GitHub\n结果tensorboard\n\n训练过程可视化\n\n验证集\n\n\n\n","slug":"Pixel-Level-Domain-Transfer论文复现","date":"2021-09-06T11:51:46.000Z","categories_index":"","tags_index":"csdn迁移","author_index":"Hulk Wang"},{"id":"260edd383abab8c5258536a42ddb3e2a","title":"YOLO-V4学习笔记","content":"YOLO-V4学习笔记\n\n\n\n\n\n\n\n\n知识点来源于网络，仅记录学习来源：https://zhuanlan.zhihu.com/p/143747206\n网络结构\n 网络结构(来源见水印)\n\n五个基本组件:\n\nCBM：Yolov4网络结构中的最小组件，由Conv+Bn+Mish激活函数三者组成。\nCBL：由Conv+Bn+Leaky_relu激活函数三者组成。\nRes unit：借鉴Resnet网络中的残差结构，让网络可以构建的更深。\nCSPX：借鉴CSPNet网络结构，由卷积层和X个Res unint模块Concate组成。\nSPP：采用1×1，5×5，9×9，13×13的最大池化的方式，进行多尺度融合。\n\n\n  Object detector\n\n如上图，大致分为四个阶段理解yolov4，分别为输入端、backbone、Neck以及Prediction。\n输入端Mosaic数据增强Yolov4中使用的Mosaic是参考2019年底提出的CutMix数据增强的方式，但CutMix只使用了两张图片进行拼接，而Mosaic数据增强则采用了4张图片，随机缩放、随机裁剪、随机排布的方式进行拼接。\n\n\n为什么要进行Mosaic数据增强?\n\n\n\n\n\n\n\n\n在平时项目训练时，小目标的AP一般比中目标和大目标低很多。而Coco数据集中也包含大量的小目标，但比较麻烦的是小目标的分布并不均匀。\n首先看下小、中、大目标的定义：2019年发布的论文《Augmentation for small object detection》对此进行了区分:\n\n\n\n\n最小矩形区域面积\n最大矩形区域面积\n\n\n\n小目标\n0 * 0\n32 * 32\n\n\n中目标\n32 * 32\n96 * 96\n\n\n大目标\n96 * 96\n∞ * ∞\n\n\n\n\n\n\n\n\n\n\n\n可以看到小目标的定义是目标框的长宽0×0~32×32之间的物体。\n\n\n\n\n小\n中\n大\n\n\n\n数据集中小目标占比\n41.4%\n34.3%\n24.3%\n\n\n数据集图片包含占比\n52.3%\n70.7%\n83.0%\n\n\n但在整体的数据集中，小、中、大目标的占比并不均衡。\n\n\n\n\n\n如上表所示，Coco数据集中小目标占比达到41.4%，数量比中目标和大目标都要多。\n\n\n\n\n\n但在所有的训练集图片中，只有52.3%的图片有小目标，而中目标和大目标的分布相对来说更加均匀一些。\n针对这种状况，Yolov4的作者采用了Mosaic数据增强的方式。\n主要有几个优点：\n\n丰富数据集：随机使用4张图片，随机缩放，再随机分布进行拼接，丰富了图片的背景，大大丰富了检测数据集，特别是随机缩放增加了很多小目标，让网络的鲁棒性更好。\n减少GPU：四张图片拼接在一起变相地提高了batch_size，在进行batch normalization的时候也会计算四张图片，所以对本身batch_size不是很依赖，单块GPU就可以训练YOLOV4。\n\n\n\n\n\n\n\n\n\n\n最后说明一下对于标签框的处理，当进行裁剪的时候，如果裁剪了样本当中的标签框的部分区域，则将其舍弃，保留裁剪之后还完整的标签框。\n参考\n与其他增强方法对比\nMixup:将随机的两张样本按比例混合，分类的结果按比例分配；\n\nCutout:随机的将样本中的部分区域cut掉，并且填充0像素值，分类的结果不变；\n\nCutMix:就是将一部分区域cut掉但不填充0像素而是随机填充训练集中的其他数据的区域像素值，分类结果按一定的比例分配\n\n\n\n\n上述三种数据增强的区别：\n\ncutout和cutmix就是填充区域像素值的区别;\nmixup和cutmix是混合两种样本方式上的区别;\nmixup是将两张图按比例进行插值来混合样本，cutmix是采用cut部分区域再补丁的形式去混合图像，不会有图像混合后不自然的情形。\n\n参考\nBackBoneCSPDarknet53\n\n\n\n\n\n\n\n\nCSPDarknet53是在Yolov3主干网络Darknet53的基础上，借鉴2019年CSPNet的经验，产生的Backbone结构。因为Backbone有5个CSP模块（见网络结构），输入图像是608 * 608，所以特征图变化的规律是：608-&gt;304-&gt;152-&gt;76-&gt;38-&gt;19。经过5次CSP模块后得到19*19大小的特征图。而且作者只在Backbone中采用了Mish激活函数，网络后面仍然采用Leaky_relu激活函数。\nCSPNet论文地址\n  CSPNet全称是Cross Stage Paritial Network，主要从网络结构设计的角度解决推理中从计算量很大的问题。\n  设计CSPNet的主要目的是使该体系结构能够实现更丰富的梯度组合信息，同时减少计算量。 通过将基础层的特征图划分为两个部分，然后通过提出的跨阶段层次结构将它们合并，可以实现此目标。 \n  CSPNet的作者认为推理计算过高的问题是由于网络优化中的梯度信息重复导致的。\n  因此采用CSP模块先将基础层的特征映射划分为两部分，然后通过跨阶段层次结构将它们合并，在减少了计算量的同时可以保证准确率。\n  因此Yolov4在主干网络Backbone采用CSPDarknet53网络结构，主要有三个方面的优点：\n\n增强CNN的学习能力，使得在轻量化的同时保持准确性\n降低计算瓶颈\n降低内存成本\n\n论文方法\n\n如上图(a), DenseNet的每个阶段均包含一个dense block(密集连接层)和transition layer(过渡层)，同时，每个dense block由K个密集层连接。第$i^{th}$个密集层的输出将会同第$i^{th}$个密集层的输入相连接，同时拼接后的输出结果作为${i+1}^{th}$个密集层的输入，等式表示如下所示：\n\n\n如果使用反向传播算法更新权重，则权重更新方程可写为：\n\n\n其中f是权重更新的函数，$g_i$表示传播到第$i^{th}$个密集层的梯度。作者发现大量重复的梯度信息被用来更新不同密集层的权重。 这将导致不同的密集层重复学习复制的梯度信息。\n如上图(b),改进点在于CSPNet将浅层特征映射为两个部分，一部分经过Dense模块（图中的Partial Dense Block）,另一部分直接与Partial Dense Block输出进行concate。CSPDenseNet的前馈传递和权重更新的方程式分别显示如下：\n\n\n\n\n\n总体而言，CSPDenseNet保留了DenseNet的特征重用特性的优点，但同时通过截断梯度流，防止了过多的梯度信息重复。 通过设计分层特征融合策略来实现此思想，并将其用于部分过渡层。\n作者也设计了几种特征融合的策略，如下图所示：\nFustion First的方式是对两个分支的feature map先进行concatenation操作，这样梯度信息可以被重用。Fusion Last的方式是对Dense Block所在分支先进性transition操作，然后再进行concatenation， 梯度信息将被截断，因此不会重复使用梯度信息 。\n\n\n\n\n\n\n\n\n\n所以CSP-DarkNet到底是怎么借鉴CSPNet的？如果按照CSPNet的思想，那特征输入应该按一定比例分为两路，分别经过Part1和Part2后concat，比如下图这样：\n\n 来源见水印\n\n\n\n\n\n\n\n\n\n\n但实际CSP-DarkNet没有做split操作，Part1和Part2输入的是全部特征，如下图：\n\n 来源见水印\n\n\n\n\n\n\n\n\n\n\n直接用两路的1x1卷积将输入特征进行变换。 可以理解的是，将全部的输入特征利用两路1x1进行transition，比直接划分通道能够进一步提高特征的重用性，并且在输入到resiudal block之前也确实通道减半，减少了计算量。\n参考来源\nMish激活函数论文地址\n\n Mish曲线\n\ny = x * tanh(ln(1+exp(x)))\n\n\n\n\n\n\n\n\n\n 一种自正则的非单调神经激活函数，平滑的激活函数允许更好的信息深入神经网络，从而得到更好的准确性和泛化。论文中提出，相比Swish有0.494%的提升，相比ReLU有1.671%的提升。\n\n\n\n\n\n\n\n\n\n Yolov4的Backbone中都使用了Mish激活函数，而后面的网络则还是使用leaky_relu函数。\n优点：\n\n以上无边界(即正值可以达到任何高度)避免了由于封顶而导致的饱和。2. 理论上对负值的轻微允许可以产生更好的梯度流，而不是像ReLU中那样的硬零边界。\n平滑的激活函数允许更好的信息深入神经网络，从而得到更好的准确性和泛化。\n\n更平滑的激活函数允许信息更深入地流动\n缺点：\n\n计算量肯定比relu大，占用的内存也多了不少；\n\npytorch实现\n\n\nDropblock论文地址\n\n\n\n\n\n\n\n\n\nYolov4中使用的Dropblock，其实和常见网络中的Dropout功能类似，也是缓解过拟合的一种正则化方式。\ndropout方法多是作用在全连接层上，在卷积层应用dropout方法意义不大。文章认为是因为每个feature map的位置都有一个感受野范围，仅仅对单个像素位置进行dropout并不能降低feature map学习的特征范围，也就是说网络仍可以通过该位置的相邻位置元素去学习对应的语义信息，也就不会促使网络去学习更加鲁棒的特征。    既然单独的对每个位置进行dropout并不能提高网络的泛化能力，那么很自然的，如果我们按照一块一块的去dropout，就自然可以促使网络去学习更加鲁棒的特征。思路很简单，就是在feature map上去一块一块的找，进行归零操作，类似于dropout，叫做dropblock。\n\n 绿色阴影区域是语义特征，b图是模拟dropout的做法，随机丢弃一些位置的特征,(c)是dropblock\n\n\n\ndropblock有三个比较重要的参数，一个是block_size，用来控制进行归零的block大小；一个是γ，用来控制每个卷积结果中，到底有多少个channel要进行dropblock；最后一个是keep_prob，作用和dropout里的参数一样。\nM大小和输出特征图大小一致，非0即1，为了保证训练和测试一致，需要和dropout一样，进行rescale。\n上述是理论分析，在做实验时候发现，block_size控制为7*7效果最好，对于所有的feature map都一样，γ通过一个公式来控制，keep_prob则是一个线性衰减过程，从最初的1到设定的阈值(具体实现是dropout率从0增加到指定值为止)，论文通过实验表明这种方法效果最好。如果固定prob效果好像不好。实践中，并没有显式的设置γ的值，而是根据keep_prob(具体实现是反的，是丢弃概率)来调整。\nNeck\n\n\n\n\n\n\n\n\n在目标检测领域，为了更好的提取融合特征，通常在Backbone和输出层，会插入一些层，这个部分称为Neck。相当于目标检测网络的颈部，也是非常关键的。\nYolov4的Neck结构主要采用了SPP模块、FPN+PAN的方式。\nSPP模块\n\n作者在SPP模块中，使用k={1 * 1,5 * 5,9 * 9,13 * 13}的最大池化的方式，再将不同尺度的特征图进行Concat操作。\n\n\n\n\n\n\n\n\n\n采用SPP模块的方式，比单纯的使用kk最大池化的方式，*更有效的增加主干特征的接收范围，显著的分离了最重要的上下文特征\nFPN+PAN论文地址\n\n\n\n\n\n\n\n\n\nPath Aggregation Network(PANet)，旨在提升基于侯选区域的实例分割框架内的信息流传播。具体来讲，通过自下向上(bottom-up)的路径增强在较低层(lower layer)中准确的定位信息流，建立底层特征和高层特征之间的信息路径，从而增强整个特征层次架构。\nyolo-v3中使用FPN具体如下：\n\n  来源见水印\n\n\nFPN是自顶向下的，将高层的特征信息通过上采样的方式进行传递融合，得到进行预测的特征图。\n在yolo_v4中，在FPN的基础上增加PAN，具体结构如下：\n\n  来源见水印\n\n如上图，紫色箭头处分别是三个中间feature map，分辨率为76 * 76、38 * 38、19 * 19\n\n 来源见水印\n\n\n这样结合操作，FPN层自顶向下传达强语义特征，而特征金字塔则自底向上传达强定位特征，两两联手，从不同的主干层对不同的检测层进行参数聚合。\n\n  yolo_v4中使用的是修改的PAN\n\n\nPrediction目标检测任务的损失函数一般由Classificition Loss（分类损失函数）和Bounding Box Regeression Loss（回归损失函数）两部分构成。\nBounding Box Regeression的Loss近些年的发展过程是：Smooth L1 Loss-&gt; IoU Loss（2016）-&gt; GIoU Loss（2019）-&gt; DIoU Loss（2020）-&gt;CIoU Loss（2020）\n我们从最常用的IOU_Loss开始，进行对比拆解分析，看下Yolov4为啥要选择CIOU_Loss。\n\n\n\n\n\n\n\n\n\n记住一点：好的目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比。\nIOU_Loss\n\nIOU的loss其实很简单，主要是交集/并集，但其实也存在两个问题。\n\n\n\n即状态1的情况，当预测框和目标框不相交时，IOU=0，无法反应两个框距离的远近，此时损失函数不可导，IOU_Loss无法优化两个框不相交的情况。\n\n即状态2和状态3的情况，当两个预测框大小相同，两个IOU也相同，IOU_Loss无法区分两者相交情况的不同。\n\n\n因此2019年出现了GIOU_Loss来进行改进。\nGIOU_Loss\n\n可以看到上图GIOU_Loss中，增加了相交尺度的衡量方式，缓解了单纯IOU_Loss时的尴尬。但还有一种不足，如下：\n\n\n问题：状态1、2、3都是预测框在目标框内部且预测框大小一致的情况，这时预测框和目标框的差集都是相同的，因此这三种状态的GIOU值也都是相同的，这时GIOU退化成了IOU，无法区分相对位置关系。基于这个问题，2020年的AAAI又提出了DIOU_Loss。\nDIOU_Loss好的目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比。\n针对IOU和GIOU存在的问题，作者从两个方面进行考虑\n一：如何最小化预测框和目标框之间的归一化距离？二：如何在预测框和目标框重叠时，回归的更准确？\n针对第一个问题，提出了DIOU_Loss（Distance_IOU_Loss）\n\n\nDIOU_Loss考虑了重叠面积和中心点距离，当目标框包裹预测框的时候，直接度量2个框的距离，因此DIOU_Loss收敛的更快。但就像前面好的目标框回归函数所说的，没有考虑到长宽比。\n\n\n比如上面三种情况，目标框包裹预测框，本来DIOU_Loss可以起作用。但预测框的中心点的位置都是一样的，因此按照DIOU_Loss的计算公式，三者的值都是相同的。\nCIOU_LossCIOU_Loss和DIOU_Loss前面的公式都是一样的，不过在此基础上还增加了一个影响因子，将预测框和目标框的长宽比都考虑了进去。\n\n\n其中v是衡量长宽比一致性的参数，我们也可以定义为：\n\n\n这样CIOU_Loss就将目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比全都考虑进去了。\n对比再来综合的看下各个Loss函数的不同点：\nIOU_Loss：主要考虑检测框和目标框重叠面积。GIOU_Loss：在IOU的基础上，解决边界框不重合时的问题。DIOU_Loss：在IOU和GIOU的基础上，考虑边界框中心点距离的信息。CIOU_Loss：在DIOU的基础上，考虑边界框宽高比的尺度信息。\nYolov4中采用了CIOU_Loss的回归方式，使得预测框回归的速度和精度更高一些。\nDIOU_nmsDIoU用作 NMS 的一个因子。该方法在抑制冗余的边界框时会使用 IoU 和两个边界框的中心点之间的距离。这能使得模型能更加稳健地应对有遮挡的情况。在传统NMS中，IoU指标常用于抑制冗余bbox，其中重叠区域是唯一因素，对于遮挡情况经常产生错误抑制。 DIoU-NMS将DIoU作为NMS的准则，因为在抑制准则中不仅应考虑重叠区域，而且还应考虑两个box之间的中心点距离，而DIoU就是同时考虑了重叠区域和两个box的中心距离。 \nDIoU-NMS建议两个中心点较远的box可能位于不同的对象上，不应将其删除(这就是DIoU-NMS的与NMS的最大不同之处)。\n\n\n在上图重叠的摩托车检测中，中间的摩托车因为考虑边界框中心点的位置信息，也可以回归出来。\n因此在重叠目标的检测中，DIOU_nms的效果优于传统的nms。\n\n\n\n\n\n\n\n\n\n这里为什么不用CIOU_nms，而用DIOU_nms?\n答：因为前面讲到的CIOU_loss，是在DIOU_loss的基础上，添加的影响因子，包含groundtruth标注框的信息，在训练时用于回归。但在测试过程中，并没有groundtruth的信息，不用考虑影响因子，因此直接用DIOU_nms即可。\n","slug":"YOLO-V4学习笔记","date":"2021-09-06T10:10:36.000Z","categories_index":"","tags_index":"detection","author_index":"Hulk Wang"},{"id":"82f698705681b0bacc9c6cad3db6d88e","title":"YOLO-V3学习笔记","content":"YOLO-V3学习笔记\n\n\n\n\n\n\n\n\n知识点来源于论文和网络，仅记录学习\n网络结构Backbone\n整个v3结构没有池化层和全连接层\n输出特征图缩小到输入的1/32。所以，通常都要求输入图片是32的倍数\n\n\n\nDBL:代码中的Darknetconv2d_BN_Leaky，是yolo_v3的基本组件。就是卷积+BN+Leaky relu。resn:n代表数字，有res1，res2, … ,res8等等，表示这个res_block里含有多少个res_unit。concat:张量拼接。将darknet中间层和后面的某一层的上采样进行拼接。拼接的操作和残差层add的操作是不一样的，拼接会扩充张量的维度，而add只是直接相加不会导致张量维度的改变。\nOutputyolo v3输出了3个不同尺度的feature map，如上图所示的y1, y2, y3。借鉴了FPN(feature pyramid networks)，采用多尺度来对不同size的目标进行检测，越精细的grid cell就可以检测出越精细的物体(大分辨率y3更能检测小物体，小分辨率y1更能检测大物体)。\ny1,y2和y3的深度都是255，边长分别为13:26:52。对于COCO类别而言，有80个种类，所以每个box应该对每个种类都输出一个概率。\nyolo v3设定的是每个网格单元预测3个box，所以每个box需要有(x, y, w, h, confidence)五个基本参数，然后还要有80个类别的概率。所以3*(5 + 80) = 255。这个255就是这么来的。v3用上采样的方法来实现这种多尺度的feature map，concat连接的两个张量是具有一样尺度的(两处拼接分别是26x26尺度拼接和52x52尺度拼接，通过(2, 2)上采样来保证concat拼接的张量尺度相同)。作者并没有像SSD那样直接采用backbone中间层的处理结果作为feature map的输出，而是和后面网络层的上采样结果进行一个拼接之后的处理结果作为feature map。\nBounding Box在Yolov1中，网络直接回归检测框的宽、高，这样效果有限。所以在Yolov2中，改为了回归基于先验框的变化值，这样网络的学习难度降低，整体精度提升不小。Yolov3沿用了Yolov2中关于先验框的技巧，并且使用k-means对数据集中的标签框进行聚类，得到类别中心点的9个框，作为先验框。在COCO数据集中（原始图片全部resize为416 × 416），九个框分别是 (10×13)，(16×30)，(33×23)，(30×61)，(62×45)，(59× 119)， (116 × 90)， (156 × 198)，(373 × 326) ，顺序为w × h。\nfeature map中的每一个cell都会预测3个边界框（bounding box） ，每个bounding box都会预测三个东西：\n\n每个框的位置（4个值，中心坐标tx和ty，框的高度bh和宽度bw）\n一个objectness prediction \nN个类别\n\n三个output，每个对应的感受野不同，32倍降采样的感受野最大，适合检测大的目标，所以在输入为416×416时，每个cell的三个anchor box为(116 ,90); (156 ,198); (373 ,326)。16倍适合一般大小的物体，anchor box为(30,61); (62,45); (59,119)。8倍的感受野最小，适合检测小目标，因此anchor box为(10,13); (16,30); (33,23)。所以当输入为416×416时，实际总共有（52×52+26×26+13×13）×3=10647个proposal box。\n\n\n\n特征图\n13x13\n26x26\n52x52\n\n\n\n感受野\n大\n中\n小\n\n\n先验框\n(116 ,90)(156 ,198)(373 ,326)\n(30,61) (62,45)(59,119)\n(10,13)(16,30)(33,23)\n\n\n\n\n\n\n\n\n\n\n\n\n这里注意bounding box 与anchor box的区别：Bounding box它输出的是框的位置（中心坐标与宽高），confidence以及N个类别。anchor box只是一个尺度即只有宽高。\nOutput DecodeBounding box decode如上一节所说，v2开始，回归基于先验框的变化值，因此可以通过以下公式解码检测框的x，y，w，h.\n\n\n\n如下图，$\\sigma(t_x)$、$\\sigma(t_y)$是基于矩形框中心点左上角格点坐标的偏移量, $\\sigma$是激活函数，论文中作者使用sigmoid,  $p_w, p_h$是先验框的宽、高，通过上述公式，计算出实际预测框的宽高 $b_w, b_h$.\n\n\n\n\n\n\n\n\n\n\n\n得到对应的$b_w, b_h$后, 还需要乘以特征图对应的的采样率(32,16,8)，得到真实的检测框x,y\nobjectness score decode物体的检测置信度，在Yolo设计中非常重要，关系到算法的检测正确率与召回率。置信度在输出85维中占固定一位，由sigmoid函数解码即可，解码之后数值区间在[0，1]中。\nlogistic回归用于对anchor包围的部分进行一个目标性评分(objectness score)，即这块位置是目标的可能性有多大。这一步是在predict之前进行的，可以去掉不必要anchor，可以减少计算量。作者在论文种的描述如下:\n\n\n\n\n\n\n\n\n\nIf the bounding box prior is not the best but does overlap a ground truth object by more than some threshold we ignore the prediction, following[17]. We use the threshold of 0.5. Unlike [17] our system only assigns one bounding box prior for each ground truth object.\n如果模板框不是最佳的即使它超过我们设定的阈值，我们还是不会对它进行predict。不同于faster R-CNN的是，yolo_v3只会对1个prior进行操作，也就是那个最佳prior。而logistic回归就是用来从9个anchor priors中找到objectness score(目标存在可能性得分)最高的那一个。logistic回归就是用曲线对prior相对于 objectness score映射关系的线性建模。\nClass Prediction decodeCOCO数据集有80个类别，所以类别数在85维输出中占了80维，每一维独立代表一个类别的置信度。使用sigmoid激活函数替代了Yolov2中的softmax，取消了类别之间的互斥，可以使网络更加灵活。\n总结\n9个anchor会被三个输出张量平分的。根据大中小三种size各自取自己的anchor。\n\n作者使用了logistic回归来对每个anchor包围的内容进行了一个目标性评分(objectness score)。根据目标性评分来选择anchor prior进行predict，而不是所有anchor prior都会有输出。\n\n\n训练策略\n\n\n\n\n\n\n\n\nYOLOv3 predicts an objectness score for each bounding box using logistic regression. This should be 1 if the bounding box prior overlaps a ground truth object by more than any other bounding box prior. If the bounding box prior is not the best but does overlap a ground truth object by more than some threshold we ignore the prediction, following [17]. We use the threshold of .5. Unlike [17] our system only assigns one bounding box prior for each ground truth object. If a bounding box prior is not assigned to a ground truth object it incurs no loss for coordinate or class predictions, only objectness.\n预测框一共分为三种情况：正例（positive）、负例（negative）、忽略样例（ignore）。\n正例：任取一个ground truth，与4032个框全部计算IOU，IOU最大的预测框，即为正例。并且一个预测框，只能分配给一个ground truth。例如第一个ground truth已经匹配了一个正例检测框，那么下一个ground truth，就在余下的4031个检测框中，寻找IOU最大的检测框作为正例。ground truth的先后顺序可忽略。正例产生置信度loss、检测框loss、类别loss。预测框为对应的ground truth box标签（需要反向编码，使用真实的x、y、w、h计算出  ）；类别标签对应类别为1，其余为0；置信度标签为1。\n忽略样例：正例除外，与任意一个ground truth的IOU大于阈值（论文中使用0.5），则为忽略样例。忽略样例不产生任何loss。\n负例：正例除外（与ground truth计算后IOU最大的检测框，但是IOU小于阈值，仍为正例），与全部ground truth的IOU都小于阈值（0.5），则为负例。负例只有置信度产生loss，置信度标签为0。\nLossYolov3 Loss为三个特征图Loss之和：\n$Loss = Loss_n1 + Loss_n2 + Loss_n3$\n\n\n\n\n$\\lambda$为权重常数，控制检测框Loss、obj置信度Loss、noobj置信度Loss之间的比例，通常负例的个数是正例的几十倍以上，可以通过权重超参控制检测效果;\n$1^{obj}{ij}$ 若是正例则输出1，否则为0；$1^{noobj}{ij}$ ,若是负例则输出1，否则为0；忽略样例都输出0;\nx、y、w、h使用MSE作为损失函数，也可以使用smooth L1 loss（出自Faster R-CNN）作为损失函数。smooth L1可以使训练更加平滑。置信度、类别标签由于是0，1二分类，所以使用交叉熵作为损失函数。\n\n其他\nground truth为什么不按照中心点分配对应的预测box？\n\n\n\n\n\n\n\n\n\n在Yolov3的训练策略中，不再像Yolov1那样，每个cell负责中心落在该cell中的ground truth。原因是Yolov3一共产生3个特征图，3个特征图上的cell，中心是有重合的。训练时，可能最契合的是特征图1的第3个box，但是推理的时候特征图2的第1个box置信度最高。所以Yolov3的训练，不再按照ground truth中心点，严格分配指定cell，而是根据预测值寻找IOU最大的预测框作为正例。\n\n为什么有忽略样例？\n\n忽略样例是Yolov3中的点睛之笔。由于Yolov3使用了多尺度特征图，不同尺度的特征图之间会有重合检测部分。比如有一个真实物体，在训练时被分配到的检测框是特征图1的第三个box，IOU达0.98，此时恰好特征图2的第一个box与该ground truth的IOU达0.95，也检测到了该ground truth，如果此时给其置信度强行打0的标签，网络学习效果会不理想。\n本身正负样本比例就不均衡（负例&gt;正例），如果强行标为0，会使不均衡更严重。\n\n\n\n","slug":"YOLO-V3学习笔记","date":"2021-08-30T11:59:09.000Z","categories_index":"","tags_index":"detection","author_index":"Hulk Wang"},{"id":"2a4b32b81021e06bffb6e540079ceefc","title":"hexo+github 搭建个人博客","content":"hexo+github搭建个人博客\n\n\n\n\n\n\n\n\n搭建环境:macOs 11.4环境依赖:\n\ngit\nnpm\nnode\nhexo\n\nhexo安装\n安装nodebrew install node\n安装hexonpm install -g hexo-cli\n查看hexo版本hexo -v![version](/images/node_version.jpg ‘’version’’)\n\n建站\n\n\n\n\n\n\n\n\n安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件\n\n初始化hexo框架hexo init &lt;folder&gt;\n移动到目标目录cd &lt;folder&gt;\n安装依赖组件npm install \n生成静态文件hexo g\n开启本地服务器hexo s\n\n\n\n\n\n\n\n\n\n\n在浏览器中输入 http://localhost:4000 回车就可以预览效果了\n更换主题\n\n\n\n\n\n\n\n\n此时博客是hexo默认主题，比较普通，这里推荐一个主题：Aurora\n安装教程\n效果预览\n配置github\n建立respositoryrepository名称为username.github.io\n\n修改配置文件\n\n\n\n\n\n\n\n\n\n_config.yml文件\n\n\ndeploy:  \n\ttype: git \n\trepository: https:&#x2F;&#x2F;github.com&#x2F;username&#x2F;username.github.io.git\n\tbranch: master\n\n安装一个部署插件npm install hexo-deployer-git --save\n\n重新生成部署hexo g -d\n\n\n\n\n\n\n\n\n\n\n\n此时可通过 https://username.github.io 访问博客\n配置个性域名\n\n\n\n\n\n\n\n\n这里我购买了腾讯云的域名: hulk.show\n\n配置域名 \n\n\n\n\n\n\n\n\n\n进入域名管理界面，选择解析，添加两条解析：\n\n配置git\n\n\n\n\n\n\n\n\n\n你的项目-&gt;Setting-&gt;Pages-&gt;Custom domain,添加你的域名：\n\n\n\n\n\n\n\n\n\n\n\n可能需要等几分钟,即可通过购买的域名访问博客：www.hulk.show\n","slug":"hexo-github-搭建个人博客","date":"2021-08-29T10:50:28.000Z","categories_index":"","tags_index":"config","author_index":"Hulk Wang"}]