[{"id":"97849acb8180c6583eb4b95717ffcfb3","title":"3DDFA知识总结","content":"\n\n优化demo\n\n3DMM(3D Morphable models)\n来源：知乎-言有三\n3DMM，即三维可变形人脸模型，是一个通用的三维人脸模型，用固定的点数来表示人脸。它的核心思想就是人脸可以在三维空间中进行一一匹配，并且可以由其他许多幅人脸正交基加权线性相加而来。我们所处的三维空间，每一点(x,y,z)，实际上都是由三维空间三个方向的基量，(1,0,0)，(0,1,0)，(0,0,1)加权相加所得，只是权重分别为x,y,z。\n转换到三维空间，道理也一样。每一个三维的人脸，可以由一个数据库中的所有人脸组成的基向量空间中进行表示，而求解任意三维人脸的模型，实际上等价于求解各个基向量的系数的问题。\n人脸的基本属性包括形状和纹理，每一张人脸可以表示为形状向量和纹理向量的线性叠加。\n形状向量Shape Vector：S=(X1,Y1,Z1,X2,Y2,Z2,...,Yn,Zn)； 纹理向量Texture Vector：T=(R1,G1,B1,R2,G2,B2,...,Rn,Bn)；\n任意的人脸模型可以由数据集中的m个人脸模型进行加权组合如下：\n  \n其中Si，Ti就是数据库中的第i张人脸的形状向量和纹理向量。但是我们实际在构建模型的时候不能使用这里的Si，Ti作为基向量，因为它们之间不是正交相关的，所以接下来需要使用PCA进行降维分解。\n\n首先计算形状和纹理向量的平均值。\n中心化人脸数据。\n分别计算协方差矩阵。\n求得形状和纹理协方差矩阵的特征值α，β和特征向量si，ti。\n\n上式可以转换为下式：\n\n\n其中第一项是形状和纹理的平均值，而si，ti则都是Si，Ti减去各自平均值后的协方差矩阵的特征向量，它们对应的特征值按照大小进行降序排列。\n等式右边仍然是m项，但是累加项降了一维，减少了一项。si，ti都是线性无关的，取其前几个分量可以对原始样本做很好的近似，因此可以大大减少需要估计的参数数目，并不失精度。\n基于3DMM的方法，都是在求解这几个系数，随后的很多模型会在这个基础上添加表情，光照等系数，但是原理与之类似。\nBFM(Basel Face Model)\n官网\nDetails of the Basel Face Model\nThe geometry of the BFM consists of 53,490 3D vertices connected by 160,470 triangles. Faces of different identities can be composed as linear combinations of 199 principal components. The model is given by:\n\nThe average shape\nThe principal shape components\nThe shape variance\nThe mesh topology\nThe average texture\nThe principal texture components\nThe texture variance\n\nThe mean and the first principal components (visualized: +/- 5 STD) of the shape and texture model are:\n\n\nshape components and texture components\n\nAttributes\nThe training data was labelled with gender, height, weight, and age. By varying face coefficients along the directions of maximal variance for an attribute as observed in the training data, it is possible to systematically manipulate these attributes.\n\n\nthe model attributes\n\n3DDFA-V2\nPipeLine\n\n记录几个重要的点： * 原始BFM中，包含199维形状参数和29维的表情参数； * 3DDFA中分别只使用了前40维形状参数和前10维表情参数； * 3DDFA输出62-d参数，分别为12-d pose，40-d shape和10-d expression；p = [f, pitch, yaw, roll, t2d, αid, αexp]. * 3D-&gt;2D核心公式：  * 整个论文前提！Pr是弱投影矩阵，透视投影矩阵下不成立；\n其他详细看论文吧，还有一些细节，如方向锁、loss等，笔记记在论文中了。\n","slug":"3DDFA知识总结","date":"2021-11-05T02:50:23.000Z","categories_index":"","tags_index":"work summary","author_index":"Hulk Wang"},{"id":"40ac598c9729991661764e8c3d733bc9","title":"妆容迁移总结","content":"\n\n\n\n\n\n\n\n\n先上一个自己简单优化的效果,对非正脸/五官有遮挡的图效果待优化,在真实图片上测试，比几个开源模型效果好\n\nBeautyGan\nBeautyGAN: Instance-level Facial Makeup Transfer with Deep Generative Adversarial Network\n\n化妆风格是因人而异的，需要在instance-level进行迁移。然而GAN网络主要是应用于域级的迁移，比如CycleGAN，强调域间差异而忽略域内差异。 &gt; For instance, CycleGAN realizes image-to-image translation between two collections (e.g., horses and zebras), and emphasizes inter-domain differences while omits intra-domain differences\n妆容风格不仅是全局的风格，还包括了每种妆独立的风格。因此，我们想extract出整体的妆容风格并且同时保留每种妆的特点是很难的。\n\n\n\n\n\n\n\n\n\n\nMakeup style is beyond a global style and includes independent local styles. Therefore, it is difficult to extract makeup style as a whole while preserving particular traits of various cosmetics\n为了解决以上问题，作者提出了一个基于双重GAN网络的BeautyGAN模型。 &gt; To address the above issues, we propose a novel dual input/output generative adversarial network called BeautyGAN, to realize makeup style transfer in an unified framework.\nFramework\n\n如图，主要包括一个生成器和两个判别器。\nLoss\n主要包括4中Loss:\n1. Adversarial Loss\n\n用来区分生成的去妆容图片和真实的未化妆图片： \n用来区分生成的化妆图片和真实的化妆图片： \n\n所以:\n\n\n\n\n\n\n\n\n\n\n对抗loss的作用是使得生成的图片更加形象逼真\n2. perceptual Loss\n \n用来表示x图片在VGG网络中第l层特征,分别表示feature map的通道，高度，宽度。\n\n\n\n\n\n\n\n\n\nperceptual loss的作用是保持原图像面部特征\n3. cycle consistency Loss\n前向路径: \n\nThe distance function dist(·)could be chosen as L1 norm, L2 norm or other metrics.\n\n\n\n\n\n\n\n\n\n作用:保证妆容迁移前后图片的背景信息保持一致\n4. makeup Loss\n\n\n\n\n\n\n\n\n\n本文创新的地方\n在妆容迁移前后要保证将颜色和面部的形态区分开，只迁移颜色而不改变形态。该模型在pixel-level上使用直方图匹配的方法(Histogram Matching)来保证输出图片与参考图片的妆容特征完全相同.\n首先进行Face parsing，利用PSPNet对图片进行分割，得到面部不同位置的mask，找到face,lip,eye(化妆位置)处理成binary mask得到。通过mask与原图像相乘提取出图像中对于mask的区域，做基于颜色的Histogram Matching(原理参考)，得到具有相同颜色分布的H(x,y) ,其保留了图像x的内容信息以及图像y的颜色分布。最后求原图上妆后的图与直方图匹配函数的MSE即为makeup-loss。\n\nTotal Loss\n\n迁移效果\n\nPSGAN\nPSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable Makeup Transfer\n实现了对不同角度和表情的妆容迁移，且可以做到控制妆容迁移的程度(浓淡)，以及控制只对特定的部位进行妆容迁移.\n\n\n\n\n\n\n\n\n\nOur PSGAN not only achieves state-of-the-art re- sults even when large pose and expression differences exist but also is able to perform partial and shade-controllable makeup transfer.\nFramework\n\n如图，主要包括以下三个模块\nMDNet(Makeup Distillation Network)\n该模块可以从参考图像上提炼出两个妆容矩阵和，其具有与原图像一样的维度。妆容矩阵和分别是两个的feature map, 他们由reference image分别经过1x1的卷积层产生。\nAMM(Attentive makeup morphing module)\n这个模块是核心模块，该模块接收MDNet产生的妆容矩阵，将其转化为source image上对应位置的妆容矩阵'和'(因为原来的和是对应reference image的妆容矩阵)。这个模块解决了因source image和reference image的姿态差异而导致的不对齐问题，即实现了(pose-robust)不同角度，不同表情的妆容迁移。\n因为source image 和 reference image 可能会有不同的姿态和表情, 所以MDNet生成的妆容矩阵是不能直接用于source image的。因此AMM模块计算一个attentive矩阵A(如上图)， 其中表示的是图x的第i个pixel  与图y的第j个pixel  的attentive value。\n\n\n\n\n\n\n\n\n\n该模块还使用68人脸关键点作为anchor points，相关看论文\nMANet(Makeup apply network)\n典型的encode-bottleneck-decode模型。该模块先对source image进行卷积做encode, 而后在bottleneck部分应用AMM产生的妆容矩阵'和'做pixel-level的weighted multiplication 和 addition。通过调整权值与[0,1]之间可以控制妆容迁移的程度。\n\n\n\n\n\n\n\n\n\nthe encoder part of MANet shares the same architecture with MDNet, but they do not share parameters\n在bottleneck部分应用从AMM中得到的morph makeup tensor Γ' 和 B'：\n\n\n\n\n\n\n\n\n\n\n这个过程类似于进行Instance Normalization（在encoder时每一层都有IN，除了最后一层是只有norm没有尺度和位移因子，因此可以直接使用scaling Γ' 和 shift B' 形成一个完整的IN）\nLoss\n同BeautyGan\n迁移效果\n\nCPM\nLipstick ain’t enough: Beyond Color Matching for In-the-Wild Makeup Transfer\n不仅考虑颜色迁移，同时增加了图案迁移 &gt; In this work, we consider makeup as a combination of color transformation and pattern addition.\n而且引入3D人脸中的UV map，网络输入UV map.\n\n\n\n\n\n\n\n\n\n\"UV\"这里是指u,v纹理贴图坐标的简称(它和空间模型的X, Y, Z轴是类似的)。 它定义了图片上每个点的位置的信息。这些点与3D模型是相互联系的, 以决定表面纹理贴图的位置。 UV就是将图像上每一个点精确对应到模型物体的表面. 在点与点之间的间隙位置由软件进行图像光滑插值处理。这就是所谓的UV贴图。\nFramework\n\n如上图，整个网络分为颜色和图案样式两个branch；\nLoss\n颜色branch与之前一样： \n样式branch：  如论文中所述，就是计算一个二值图的IOU；\n迁移效果\n\nIPM\nReal-World Automatic Makeup via Identity Preservation Makeup Net\n基于MUNIT 来disentangle 不同的特征，减少了妆容迁移中对背景造成的变化，同时加入了identity-aware的loss，用的不是vgg这种general的特征提取器，而是用了person recognition中用的vgg-face网络。 解决迁移前后复杂背景和身份信息不一致的问题；\nFramework\n看着很头大的一个Framework:\n\n三个branch,上面branch送入目标妆容图片，编码得到妆容风格code;中间一个输入待上妆图片和其对应的纹理和面部mask，编码得到identity content code;最下面branch和最上面branch一样，编码得到未化妆风格code。 然后就是identity content code和style code组合，送入生成器获取对应图片了。\n\n\n\n\n\n\n\n\n\n如图b，inference 阶段 只需要执行前两个branch\nLoss\n很多loss组合，对抗loss，前背景loss（根据mask），id loss（face roi），重建loss（像素级），perpetual loss...大杂烩，详细看论文代码吧。\n迁移效果\n使用官方预处理的图片测试\n\n","slug":"妆容迁移总结","date":"2021-11-02T15:22:22.000Z","categories_index":"","tags_index":"work summary","author_index":"Hulk Wang"},{"id":"48031087686b5c5ecd7460e246ad1434","title":"StyelGan及其应用","content":"\n\n\n\n\n\n\n\n\n对项目realworld-stylegan2-encoder相关技术的总结，欢迎star～\n\nThe demo of different style with age edit.\n\n\nStyleGan\n个人认为，效果好的主要两点是： 1. Mapping Network对隐藏空间(latent space)进行解耦 2. Synthesis Network的非线性映射层\n亮点: 1. Style mixing\n\nMapping Network\n\n\n\n\n\n\n\n\n\nMapping network 要做的事就是把隐向量z解耦为w(latent code)\n\n\n\n\n\n\n\n\n\nlatent code 简单理解就是，为了更好的对数据进行分类或生成，需要对数据的特征进行表示，但是数据有很多特征，这些特征之间相互关联，耦合性较高，导致模型很难弄清楚它们之间的关联，使得学习效率低下，因此需要寻找到这些表面特征之下隐藏的深层次的关系，将这些关系进行解耦，得到的隐藏特征，即latent code。由 latent code组成的空间就是 latent space。\n为什么解耦？\n\n一般 z 是符合均匀分布或者高斯分布的随机向量，但在实际情况中，各个属性构成的特征空间并不是这样的，他们往往存在一定的先验。例如文中所列举的例子：长头发和男子气概往往不会同时出现，如图(a)中，左上角则表示男子气概和长头发同时存在的分布空缺.那么，在传统的以 z 作为隐变量的GAN（b）中，由于 z 来自于一个对称的分布，所以它是一个圆形。而为了填补（a）中左上角的空缺，特征的分布必将被扭曲，这就造成了当仅仅改变 z 的某一维度时，输出图像会有多个特征同时发生变动，这就是entanglement。而StyleGAN中Mapping Network的作用，就是将图（b）映射成（c），mapping network学到一种非线性变换，将原本均匀的特征空间扭曲变形，使其接近真实情况。\n作者认为generator偏好于基于解耦的特征去生成:\n\n\n\n\n\n\n\n\n\nWe posit that there is pressure for the generator to do so, as it should be easier to generate realistic images based on a disentangled representation than based on an entangled representa- tion.\nSynthesis Network\nStyleGAN认为，所谓image就是style的集合，而style是有层级的 * Coarse styles -&gt; pose, hair, face shape * Middle styles -&gt; facial features, eyes * Fine styles -&gt; color scheme\nSynthesis Network里面用到了AdaIn模块。\n\n与风格迁移不同，StyleGAN是从向量w而不是风格图像计算而来。\nconstant input\nStyleGAN生成图像的特征是由w和AdaIN控制的，那么生成器的初始输入可以被忽略，并用常量值替代。这样做的理由是，首先可以降低由于初始输入取值不当而生成出一些不正常的照片的概率（这在GANs中非常常见），另一个好处是它有助于减少特征纠缠，对于网络在只使用w不依赖于纠缠输入向量的情况下更容易学习。\nStochastic variation\nstylegan通过添加per-pixel noise为生成的图像增加随机细节，如头发丝、胡须、毛孔等。在不同层添加噪声会有不一样的效果。\n\nStyle mixing\n Style mixing 的本意是去找到控制不同style的latent code的区域位置，具体做法是将两个不同的latent code z1和 z2 输入到 mappint network 中，分别得到w1和w2，分别代表两种不同的 style，然后在 synthesis network 中随机选一个中间的交叉点，交叉点之前的部分使用 w1，交叉点之后的部分使用 w2，生成的图像应该同时具有 source A 和 source B 的特征，称为 style mixing。\n\n\n\n\n\n\n\n\n\n根据实验，由此可以大致推断，低分辨率的style 控制姿态、脸型、配件 比如眼镜、发型等style，高分辨率的style控制肤色、头发颜色、背景色等style。\nLoss\n什么是饱和损失函数 Non-Saturating LossFunction\n\n\n\n\n\n\n\n\n\n非饱和损失函数能在训练早期提供更大的梯度\n\n饱和Loss：生成器希望最小化被判断为假的概率:\n\n\n\n非饱和Loss：生成器希望最大化被判断为真的概率:\n\n\nor\n\n\n\n\n\n\n\n\n\n\n后者能提供的梯度信息更好，为什么呢？ 在训练的初始阶段，G生成的样本很容易被D识别出来，也就是D(G(z)) 趋近于0，而此时Loss1的梯度也趋近于0，所以饱和了。而 Loss2的梯度不是趋近于0的，能够为网络的权重更新提供好的梯度方向，帮助收敛，所以没饱和。\nStylegan使用loss\nStyleGAN官方代码采用的损失为G_logistic_nonsaturating与D_logistic_simplegp。\n\n\n训练\nTruncation Trick\nTruncation trick的目的是为了得到更高的平均FID值。由于训练集必定会存在一定程度的样本分布不均匀，低概率密度（样本量少的数据）没有得到很好的训练，因此当w刚好处于低概率密度时，生成的图像质量很差，为了避免生成这些影响FID的图像，stylegan使用了truncation trick，其实现方式如下：\n计算平均latent code 通过style scale控制当前w与平均w的占比\n是W中心, 是一个实数，表示压缩倍数,截断或者压缩后的（truncated）w‘公式如下：\n\n\n来源见水印\n\n调整style scale可使最终的latent code总在平均w左右，保证了生成图像的质量。\n实验效果: \n\n\n\n\n\n\n\n\n\nstylegan中truncation trcik仅在低分辨率层中使用，这样可以保证细粒度的人脸细节不变，改变粗粒度下的人脸特征（人脸朝向、配饰、年龄、发型长度、性别等）\nProgressive Growing\nprogressive growing的训练方式，先训一个小分辨率的图像生成，训好了之后再逐步过渡到更高分辨率的图像。然后稳定训练当前分辨率，再逐步过渡到下一个更高的分辨率。\n其他\nPerceptual path length\n\n\n\n\n\n\n\n\n\nPerceptual path length 是一个指标，用于判断生成器是否选择了最近的路线\n如果latent space是完全线性的（充分解耦）、那么两个latent code进行插值得到的图像会处于z1和z2的连线上（如上图蓝色的线)，当解耦不够充分时，插值 得到的图像就会出现较大的偏差，因此可以通过latent code的插值结果是否与z1、z2之间相差较大衡量GAN model是否可靠，其中使用VGG16对各个图像进 行编码，这就是perception path length。\n\n\n来源见水印\n\nStyleGan2\nStyleGAN V1在生成图像时有两个明显的问题： * AdaIN导致的水滴效应。\n\n\nProgressive Generation导致生成高分辨率时，牙齿等细节不随着人脸移动而移动。\n\n\n\nRemoving normalization artifacts\n改进1. 去除const input后的操作；Norm中去除mean；将noise和bias 移到style block外。\nadain对每个feature map进行归一化，因此有可能会破坏掉feature之间的信息\n重新审视AdaIN模块：将AdaIN拆解为Normalization和Modulation两部分（如上图a为AdaIN，拆解后为b），然后重组一下得到style block（灰色块）。通过实验发现，style block内的偏置和噪声会对后面的normalization中的方差幅度有些许影响，而如果将这两个（bias和noise）移到style block外面，网络结果则更加稳定。而此时normalization和modulation的减均值操作也可以去掉，最终得到如图c的结构.\n\n\n\n\n\n\n\n\n\nWe pinpoint the problem to the AdaIN operation that normalizes the mean and variance of each feature map separately, thereby potentially destroying any information found in the magnitudes of the features relative to each other.\n\n\n\n\n\n\n\n\n\n\n通过这一步，作者发现原本AdaIN导致的水滴效应明显改善。（主要归因于去除了Instance Normalization）.但是styleGAN的一个亮点是 style mixing，仅仅只改网络结构，虽然能去除水珠，但是无法对style mixing 有 scale-specific级别的控制\n\n\n\n\n\n\n\n\n\nIn practice, style modulation may amplify certain feature maps by an order of magnitude or more. For style mixing towork, we must explicitly counteract this amplification on a per-sample basis—otherwise the subsequent layers would not be able to operate on the data in a meaningful way. If we were willing to sacrifice scale-specific controls (see video), we could simply remove the normalization, thus removing the artifacts and also improving FID slightly. We will now propose a better alternative that removes the artifacts while retaining full controllability.\n改进2. weight demodulation\n对特征图的一系列操作改为对权重的操作。特征图只经过卷积处理并添加噪声。该方法在保留完全可控性的同时消除了伪影。\n缩放特征图改为缩放卷积权重（mod）:\n\n经过缩放和卷积后，输出激活的标准差为:\n\ndemod权重，旨在使输出恢复到单位标准差:\n\n\n\n\n\n\n\n\n\n\n尽管这种方式与Instance Norm在数学上并非完全等价，但是weight demodulation同其它normalization 方法一样，使得输出特征图有着standard的unit和deviation。\n\n\n\n\n\n\n\n\n\nwhere w and w′ are the original and modulated weights, respectively, si is the scale corresponding to the ith input feature map, and j and k enumerate the output feature maps and spatial footprint of the convolution, respectively.\n\nno Progressive growth\nProgressive growth是先训练低分辨率，等训练稳定后，再加入高一层的分辨率进行训练，但这样可能会导致“phase” artifacts。\n作者们认为问题在于，在逐步增长的过程中，每个分辨率都会瞬间用作输出分辨率，迫使其生成最大频率细节，然后导致受过训练的网络在中间层具有过高的频率，从而损害了位移不变性。\n&lt; We believe the problem is that in progressive growing each resolution serves momentarily as the output resolution, forcing it to generate maximal frequency details, which then leads to the trained network to have excessively high frequencies in the intermediate layers, compromising shift invariance.\n使用Progressive growth的原因是高分辨率图像生成需要的网络比较大比较深，当网络过深的时候不容易训练，但是skip connection可以解决深度网络的训练，因此有了下图中的三种网络结构，都采用了skip connection，三种网络结构的效果也进行了实验评估:\n\n三种生成器（虚线上方）和判别器体系结构如下图。Up和Down分别表示双线性上和下采样。 在残差网络中，这些还包括1×1卷积以调整特征图的channel数。tRGB和fRGB在RGB和高维每像素数据之间转换。 Config E和F中使用的体系结构以绿色突出显示。\n其他优化\nLazy regularization\nloss 是由损失函数和正则项组成，优化的时候也是同时优化这两项的，lazy regularization就是正则项可以减少优化的次数，比如每16个minibatch才优化一次正则项，这样可以减少计算量，同时对效果也没什么影响。\npath lenth regularization\n之前提到path length 是用来评价GAN的生成质量的，在插值的过程中我们希望latent code在潜在空间中移动多少，图像的变化幅度就是多少，不希望图像变化幅度和潜在空间的位移存在较大差距，因此通过正则化对大的变化幅度进行惩罚，从而实现上述目的。\n\n\n\n\n\n\n\n\n\n实现：在图像上的梯度 用图像乘上变换的梯度来表示\nStyleGan2-ada\n来源\nTraining Generative Adversarial Networks with Limited Data\nStyleGAN、StyleGAN2的生成效果非常好，很大原因是有强大的数据集，比如生成的高清人脸训练集有14w张（FFHQ有7w张，图像翻转x2就是14w张）。大规模的数据集一般情况下很难采集，但是小数据集会导致模型过拟合，为了解决过拟合，可以在训练的时候对数据进行图像增强，比如随机裁剪、水平翻转、加噪声、一定范围内改变色调等。但是，数据增强会导致生成图片也有对应的增强，比如对原图加入了噪声，会导致生成图片也有噪声，这是我们不期望看到的。StyleGan2-ada解决小数据集的数据增强出现在生成结果中的问题。\n\nstylegan2-ada 是基于bCR (balanced consistency regularization) 方法. bCR方法的基本思想是对同一张输入图片，如果做两种不同的图像增强，这两种增强得到的输出图片应该是一样的。此外，bCR还在判别器损失里增加了一致性正则项（consistency regularization terms），如上图（a）所示，蓝色框对应的是图像增强的操作。\nstylegan2-ada 认为 bCR 只对判别器做了图像增强，但是没对生成器做图像增强，因此无法约束生成器，stylegan2-ada 在bCR 基础上进行改进，对所有图像进行了图像增强，移除了在损失里增加一致性正则项（CR loss term)，在生成器和判别器里都用增强后的图像，stylegan2-ada的网络设计如上图（b）.\nstylegan2-ada 的 ada 是指 adaptive discriminator augmentation，解决的主要是通过网络自适应的得到数据增强的概率p。一般我们在做图像增强时，会设置一个概率p，以p的概率决定对图像是否做图像增强。这个p一般是人为设定的超参，作者通过前期实验表明，p的取值对生成结果有很大影响\n\n\n\n\n\n\n\n\n\nIdeally, we would like to avoid manual tuning of the augmentation strength and instead control it dynamically based on the degree of overfitting\n\n有了过拟合的启发式指标，如何调节 p 呢？我们初始 p 为0，然后在每4个minibatch后调节它的值，调节的方法就是基于 r 值，如果 r 值过大或者过小，就对p增大或者减小固定的数量。\n\n\n\n\n\n\n\n\n\nWe control the augmentation strength p as follows. We initialize p to zero and adjust its value once every four minibatches2 based on the chosen overfitting heuristic. If the heuristic indicates too much/little overfitting, we counter by incrementing/decrementing p by a fixed amount.\n\n\n\n\n\n\n\n\n\n\n出处 因为目前StyleGAN生成的都是虚拟人物，如果我们能找到现实人物在初始域中对应的编码的话，那就意味着可以对现实中的人物进行操作和变化，这会带来一个很有意思的场景：我们每个人的人脸都可以用一个（18,512）维度的向量来表示，并且只要对这个向量稍作一些变动，就能生成出一个略微不同于我们的新的人脸模样. 虽然我们无法保证StyleGAN的生成分布域涵盖了地球上所有人脸的样貌，但是由于StyleGAN的生成基于分级控制特征，并且训练集涵盖了人种、性别、年龄等各种样式的人脸，因此我们可以在生成分布中找到一张与现实人脸无比接近的人脸，并最终计算出其在初始域中对应的编码。 以下几个使用预训练StyleGan的相关应用。实际就是使用预训练stylegan作为decoder，弄一个encoder，就可以做image2image。\npixel2style2pixel\nEncoding in Style: a StyleGAN Encoder for Image-to-Image Translation\n网络结构\n\n网络结构如图，使用预训练的Synthesis Network作为decoder，encoder使用类似resnet+fpn，输出latent code（18x512）送到decoder。\nLoss\n四部分构成： 1. pixel-wise L2 loss\n\n生成图片和原图的L2.\n\nLPIPS Loss(感知损失)\n\n\nF(·) denotes the perceptual feature extractor.\n\nregularzation loss\n\n\n\n\n\n\n\n\n\n\nTo encourage the encoder to output latent style vectors closer to the average latent vector, we additionally define the following regularization loss\n\n\n\n\n\n\n\n\n\n\nSimilar to the truncation trick introduced in StyleGAN, we find that adding this regularization in the training of our encoder improves image quality without harming the fidelity of our outputs, especially in some of the more am- biguous tasks explored below.\n\nID loss\n\n\n\n\n\n\n\n\n\n\nFinally, a common challenge when handling the specific task of encoding facial images is the preservation of the input identity. To tackle this, we incorporate a dedicated recognition loss measuring the cosine similarity between the output image and its source\n\nR is the pretrained ArcFace network.\nTotal Loss:\n\nencoder4edit\nDesigning an Encoder for StyleGAN Image Manipulation\n这边论文在pSp的基础上做了改进，主要解决的是distortion editability和proper reconstruction的tradeoff.\n\n如pSp中所说，和w+相比w的表达能力不足； &gt; The expressiveness of the W latent space has been shown to be limited, in that not every image can be accurately mapped into W. &gt; The space W+ has more degrees of freedom, and is thus signifi- cantly more expressive than W.\n\n\n\n\n\n\n\n\n\n\n这篇论文不再使用w+； Note, that here, we depart from the commonly used W+ notation due to its ambiguity with various works referring to it as both $w_k^ and \n\n有更好的“扭曲”能力，生成的图片质量更低（内容不真实），而且作者发现w“编辑”能力更强\n\n\n\n\n\n\n\n\n\n\nIt is well known that  achieves lower, i.e. better, distortion than W. Additionally, we find that W is more editable\n\n\n\n预训练的StyleGan是在w上完成，所以生成图片的质量（内容方面）更好更稳定，维度更高（18x512），所以表达能力更强\n\n\n\n\n\n\n\n\n\n\nObserve that since StyleGAN is originally trained in the W space, it is not surprising that W is more well-behaved and has better perceptual quality compared to its  counterpart. On the other hand, observe that due the significantly higher dimensionality of  and the architecture of StyleGAN,  has far greater expressive power.\n\n所以经过一顿阐述和实验，作者设计了如下方法：\n\n\n\n\n\n\n\n\n\n\nThe first approach for getting closer to W is to encour- age the inferred  latent codes to lie closer to , i.e. minimize the variance between the different style codes, or equivalently, encourage the style codes to be identical. To this end, we propose a novel “progressive” training scheme.\n即学习相对平均w的18个偏移，这样18维表达能力更好，且不同的style code方差更小，更接近w，生成图片质量更高。\n网络结构\n\n整体网络结构如图，与pSp结构基本一致，不同的设计是encoder输出的是18个偏移，然后作用在平均w上组成送入decoder；\nLoss\n\nDistortion\n\nID loss\n\nC: 人脸就用人脸识别模型，非人脸用MoCo；\nL2 loss \nLPIPS loss \nF(·) denotes the perceptual feature extractor.\n\n\n\n\nPerceptual quality and editability\n\n\n\n\n\n\n\n\n\n\nTo increase the perceptual quality and editability, First, we apply a delta-regularization loss to ensure proximity to W∗ when learning the offsets ∆i. Second,we use an adversarial loss using our latent discriminator, which encourages each learned style code to lie within the distribution W.\n\ndelta-regularization loss\n\n\nw偏移的regularization loss，目的是使尽量不远离\n\n\n\n\n\n\n\n\n\nE(x) = (,,...,) denote the output of the encoder, where N is the number of style-modulation layers; E(x) = (w, w + , ..., w + ).\n\nadversarial loss\n\n如结构图中橘黄色,使用非饱和Gan loss，增加R1正则损失\n\n\n\n\n\n\n\n\n\nwe adopt a latent discriminator which is trained in an adversarial manner to discriminate between real samples from the W space (generated by StyleGAN’s mapping function) and the encoder’s learned latent codes.\n\n\n\n\n\n\n\n\n\n\nIn every iteration, we calculate the GAN loss for every E(x)i and average over all i-s.\n\n\nTotal Loss\n\n\n\n\n\n\n\n\n\n\noverall loss objective is defined as a weighted combination of the distortion and editability losses: \nprogressive training scheme\n\n\n\n\n\n\n\n\n\nWe note that low frequency details greatly control the distortion quality. Thus, our progressive training scheme first focuses on improving the low frequency distortion by tuning the coarse-level offsets. Then, the encoder gradually complements these offsets with higher frequency details introduced by the finer-level offsets\nEditability\nInterpreting the Latent Space of GANs for Semantic Face Editing\n这里主要介绍一下InterFaceGAN解读的方法，概括起来如下：\n对于一个二分类语义（例如男性和女性），语义的判定边界对应隐空间的一个超平面,分界面z∈,=0.的法向量为n，在超平面一端的的符号相同，因此对应的语义特征一样，若距离的符号改变，那么对应语义特征也改变。\n\n\n图片来源\n\nSAM\nOnly a Matter of Style: Age Transformation Using a Style-Based Regression Model\n针对年龄搞了一个encoder，和上两个基本换汤不换药。 个人感觉改进点，或是说效果好的原因： 1. 额外又增加了一个encoder，来学习一个非线性的偏移； 2. loss：加入circle loss\n网络结构\n\nLoss\n在L2、ID loss、LPIPS Loss和regularization loss的基础上，增加了age loss,同时修改了ID loss。（毕竟不同年龄的一个人不能一毛一样）\nID Loss\n\nAge Loss\n\nA:预训练年龄预测模型；\n同时，为了解决背景等非必要部分的改变，加入循环一致性损失，如下：\n\n","slug":"StyelGan及其应用","date":"2021-10-30T03:52:16.000Z","categories_index":"","tags_index":"work summary","author_index":"Hulk Wang"},{"id":"793b82534a9fb29d46fde7f1cf49752a","title":"NIMA: Neural Image Assessment","content":"\n\n\n\n\n\n\n\n\n主要记录下本文的Loss部分，工作中有借鉴；\n论文地址\n对于打分任务，本文并没有采用高分/低分的分类方法，也没采用回归平均分的方法，而是将分数的分布作为直方图来预测。同时因为平方EMD（earth mover's distance）在有序分类上有很好的表现，因此将其作为损失。\nBackbone不是重点，主要记录下Loss.\n\n\n\n\n\n\n\n\n\n对于有顺序的分类任务，交叉熵损失缺少类间关系的描述。也有争议提到有顺序的分类可以通过回归的方法来做，不过有文章指出，用分类的方法效果优于回归。由于平方EMD(earth mover’s distance)损失在有序类别的分类上效果很好，所以本文采用该损失作为损失函数。\nEMD(earth mover’s distance)\nEMD定义为用最小的代价从一个分布往另一个分move the mass。其公式如下：\n\n其中，p是预测的概率，p'是真是的概率，N是有序类别的个数（1～10分即10），r是一个惩罚项，r为2的时候更容易做梯度下降。\n是的累积分布函数。\n网络最后经过softmax(确保和为1)，然后计算EMD Loss；\n\n\n\n\n\n\n\n\n\n可以理解为让p的分布更接近p'的分布。\npytorch 代码\nclass EDMLoss(nn.Module):\n    def __init__(self):\n        super(EDMLoss, self).__init__()\n\n    def forward(self, p_target: torch.Tensor, p_estimate: torch.Tensor):\n        assert p_target.shape == p_estimate.shape\n        # cdf for values [1, 2, ..., 10]\n        cdf_target = torch.cumsum(p_target, dim=1)\n        # cdf for values [1, 2, ..., 10]\n        cdf_estimate = torch.cumsum(p_estimate, dim=1)\n        cdf_diff = cdf_estimate - cdf_target\n        samplewise_emd = torch.sqrt(torch.mean(torch.pow(torch.abs(cdf_diff), 2)))\n        return samplewise_emd.mean()\n计算分数\ndef mean_score(scores):\n    si = np.arange(1, 11, 1)\n    mean = np.sum(scores * si)\n    return mean\n\n# calculate standard deviation of scores for AVA dataset\ndef std_score(scores):\n    si = np.arange(1, 11, 1)\n    mean = mean_score(scores)\n    std = np.sqrt(np.sum(((si - mean) ** 2) * scores))\n    return std\n","slug":"NIMA-Neural-Image-Assessment","date":"2021-11-04T02:53:59.000Z","categories_index":"","tags_index":"work summary","author_index":"Hulk Wang"},{"id":"84b6109cde20e5bfc02c6af7734d88cb","title":"无监督对比学习(Contrastive LearningOC)","content":"\n\n\n\n\n\n\n\n\n推荐阅读\n对比学习\n原理: 输入N个图片，用不同的数据增强方法为每个图片生成两个view，分别对它们编码得到y和y'。我们对上下两批表示两两计算cosine，得到NxN的矩阵，每一行的对角线位置代表y和y'的相似度，其余代表y和N-1个负例的相似度。\n\n对每一行做softmax分类，采用交叉熵损失作为loss，就得到对比学习的损失了：\n\n其中是可调节的系数。\n\n\n\n\n\n\n\n\n\n比较粗暴的优化方向就是增加view难度、增加更多负例(batch)、提升encoder表现\nMoCo系列\n既然对比是在正负例之间进行的，那负例越多，这个任务就越难，于是一个优化方向就是增加负例。 纯粹的增大batch size是不行的，总会受到GPU内存限制。一个可行的办法就是增加memory bank，把之前编码好的样本存储起来，计算loss的时候一起作为负例\n\nmemory bank存在的问题：右侧存储好的编码都是之前的编码器计算的，而左侧的编码器缺一直在更新，所以会有两侧不一致的情况，影响目标优化。\n\n\n\n\n\n\n\n\n\n虽然可以用最新的左侧encoder更新编码再放入memory bank，但这依然避免不了memory bank中表示不一致的情况，\nMoCoV1\n\n核心：解决新旧候选样本编码不一致的问题\n方法：延续memory bank的思想，使用动量的方式更新encoder参数\n\n\n如上三种方式： (a) end to end: 负样本数量会受到batch_size大小的限制，从而限制影响模型的性能； (b) memory bank: 内存需求较大, 且会有两侧不一致的情况； (c) MoCo: 增加momentum encoder，将dictionary作为一个动态进出的队列，目标是构建一个大的且能在训练过程中保持一致性的dictionary，作者用该队列维护最近几个mini-batch中样本特征表示，并将队列作为所有样本采样的子集，对于负样例的encoder参数, 采用Momentum update方法, 复制正例encoder的参数, 公式为：\n公式\nm ∈ [0, 1) is a momentum coefficient;\n\n步骤:\n\n对于每个batch x：\n\n随机增强出, 两种view\n分别用,对输入进行编码得到归一化的q和k，并去掉k的梯度更新\n将q和k一一对应相乘得到正例的cosine（Nx1），再将q和队列中存储的K个负样本相乘（NxK），拼接起来的到 Nx(1+K) 大小的矩阵，这时第一个元素就是正例，直接计算交叉熵损失，更新的参数\n动量更新的参数：公式1\n将k加入队列，把队首的旧编码出队，负例最多时有65536个.\n\n这样每次入队的新编码都是上一步更新后的编码器输出，以很低的速度慢慢迭代，与旧编码尽量保持一致。实验发现，m=0.999时比m=0.9好上很多。\n\n数据增强方法:\n\na 224×224-pixel crop is taken from a randomly resized image, and then undergoes random color jittering, random horizontal flip, and random grayscale con- version\nMoCoV2\n改进: * 改进了数据增强方法, 增加使用blur augmentation来进行增强 * 训练时在encoder的表示上增加了相同的非线性层 * 学习率采用SimCLR的cosine衰减\n\nSimClr系列\n\n\n\n\n\n\n\n\n\n与MoCo相比，SimCLR关注的重点是正负样例的构建方式，同时SimCLR还探究了非线性层在对比学习中的作用，并分析了batch_size大小、训练轮数等超参数对对比学习的影响\nSimCLR-V1\n\n\n探究了不同的数据增强组合方式，选取了最优的;(数据增强对对比学习效果提升有明显作用，并且多种数据增强的组合效果更好；数据增强对对比学习的提升比对有监督学习的提升高)\n在encoder之后增加了一个非线性映射。研究发现encoder编码后的会保留和数据增强变换相关的信息，而非线性层的作用就是去掉这些信息，让表示回归数据的本质。注意非线性层只在无监督训练时用，在迁移到其他任务时不使用\n计算loss时多加了负例。以前都是拿右侧数据的N-1个作为负例，SimCLR将左侧的N-1个也加入了进来，总计2(N-1)个负例。\n\n\n\n\n\n\n\n\n\n\nSimCLR不采用memory bank，而是用更大的batch size，最多的时候bsz为8192，有16382个负例\nSimCLR-V2\n\n采用更深但维度略小的encoder，从 ResNet-50 (4×) 改到了 ResNet-152 (3×+SK)，在1%的监督数据下提升了29个点\n采用更深的3层MLP，并在迁移到下游任务时保留第一层（以前是完全舍弃），在1%的监督数据下提升了14个点\n参考了MoCo使用memory，但由于batch已经足够大了，只有1%左右的提升\n\n\nSEED\n(SEED Self-supervised Distillation For Visual Representation)[https://arxiv.org/abs/2101.04731]\n对比自监督学习方法在大模型训练方面表现出了很大进展，然这些方法在小模型上的表现并不好,作者推测，与大模型相比，小模型因为参数量比较小，无法有效地学习到大量数据的特征表示；\n\n\n\n\n\n\n\n\n\nWe conjecture that this is because smaller models with fewer parameters cannot effectively learn instance level discriminative representation with large amount of data.\n因此，作者借鉴了蒸馏的方法，在实例相似性分布的基础上进行知识蒸馏。 类似MoCo，作者维护一个实例队列以保存老师模型的编码输出，对于新样本，我们计算它与队列中所有样本的相似性得分。我们希望：学生模型与老师模型的相似性得分分布尽可能相似，这就构成了老师模型与老师模型的相似性得分分布的交叉熵\n结构如下：\n\n","slug":"无监督对比学习-Contrastive-LearningOC","date":"2021-11-02T03:39:11.000Z","categories_index":"","tags_index":"work summary","author_index":"Hulk Wang"},{"id":"66e6fbbf30cdfd5fb293d5ee17ed3935","title":"手撕代码","content":"1. IOU\npython\ndef bb_intersection_over_union(boxA, boxB):\n    boxA = [int(x) for x in boxA]\n    boxB = [int(x) for x in boxB]\n\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n\n    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n\n    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n    \n    iou = interArea / float(boxAArea + boxBArea - interArea)\n\n    return iou\nc++\n#include&lt;iostream&gt;\n#include&lt;vector&gt;\n#include&lt;algorithm&gt;\nusing namespace std;\n\ntypedef struct Bbox\n{\n    int x1;\n    int y1;\n    int x2;\n    int y2;\n    float score;\n}Bbox;\n\nfloat iou(Bbox box1,Bbox box2)\n{\n    max_x = max(box1.x1,box2.x1);  // 找出左上角坐标哪个大\n    min_x = min(box1.x2,box2.x2);  // 找出右上角坐标哪个小\n    max_y = max(box1.y1,box2.y1);\n    min_y = min(box1.y2,box2.y2);\n    if(min_x&lt;=max_x || min_y&lt;=max_y) // 如果没有重叠\n        return 0;\n    float over_area = (min_x - max_x) * (min_y - max_y);  // 计算重叠面积\n    float area_a = (box1.x2 - boxa.x1) * (box1.y2 - boxa.y1);\n    float area_b = (box2.x2 - boxb.x1) * (box2.y2 - boxb.y1);\n    float iou = over_area / (area_a + area_b - over_area);\n    return iou;\n}\n2. NMS\npython\ndef nms(det, thresh):\n    x1 = det[..., 0]\n    y1 = det[..., 1]\n    x2 = det[..., 2]\n    y2 = det[..., 3]\n    scores = det[..., 4]\n    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = np.argsort(scores)[::-1]  # Returns the indices that would sort an array.\n    keep = []\n    while order.size &gt; 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(x2[i], x2[order[1:]])\n        w = np.maximum(0, xx2 - xx1 + 1)\n        h = np.maximum(0, yy2 - yy1 + 1)\n        inter = w * h\n        union = area[i] + area[order[1:]] - inter\n        iou = inter / union\n        next_i = np.where(iou &lt;= thresh)[0]  # 只有条件 (condition)，没有x和y，则输出满足条件 (即非0) 元素的坐标\n        order = order[next_i + 1]\n    return keep\nc++\n/*\n  将bbx按照confidence从高到低排序\n*/\nbool sort_score(Bbox box1,Bbox box2)\n{\n    return (box1.score &gt; box2.score);\n}\n/*\n(1) 获取当前目标类别下所有bbx的信息\n(2) 将bbx按照confidence从高到低排序,并记录当前confidence最大的bbx\n(3) 计算最大confidence对应的bbx与剩下所有的bbx的IOU,移除所有大于IOU阈值的bbx\n(4) 对剩下的bbx，循环执行(2)和(3)直到所有的bbx均满足要求（即不能再移除bbx）\n*/\nvector&lt;Bbox&gt; nms(vector&lt;Bbox&gt;&amp;vec_boxs, float threshold)\n{\n    vector&lt;Bbox&gt;  res;\n    while(vec_boxs.size() &gt; 0)\n    {\n        sort(vec_boxs.begin(),vec_boxs.end(),cmp);\n        res.push_back(vec_boxs[0]);\n        for(int i =0;i &lt;vec_boxs.size()-1;i++)\n        {\n            float iou_value =iou(vec_boxs[0],vec_boxs[i+1]);\n            if (iou_value &gt;threshold)\n            {\n                vec_boxs.erase(vec_boxs[i+1]);\n            }\n        }\n        vec_boxs.erase(vec_boxs[0]);  // res 已经保存，所以可以将最大的删除了\n \n    }\n    return res;\n}\n3. 卷积\npython\ndef conv_naive(x, c_out, ksize=3, padding=0, stride=1):\n    b, c_in, h, w = x.shape\n    kernel = np.random.rand(c_out, c_in, ksize, ksize)\n    out_height = (h - ksize + 2 * padding) // stride + 1\n    out_width = (w - ksize + 2 * padding) // stride + 1\n\n    out_x = np.random.rand(b, c_out, out_height,  out_width)\n    if padding &gt; 0:\n        pad_x = np.zeros((b, c_in, h + 2 * padding, w + 2 * padding))\n        pad_x[..., padding:-padding, padding:-padding] = x\n    else:\n        pad_x = x\n\n    for y in range(out_height):\n        for x in range(out_width):\n            roi = pad_x[..., y * stride:y * stride + ksize, x * stride: x * stride + ksize]\n            conv = np.tile(np.expand_dims(roi, axis=1), (1, c_out, 1, 1, 1)) * kernel\n            # conv = np.repeat(np.expand_dims(roi, axis=1), axis=1, repeats=c_out) * kernel\n            out_x[..., y, x] = np.squeeze(np.sum(conv, axis=(2, 3, 4), keepdims=True), axis=(2, 3, 4))\n\n    return out_x\nc++\n/二维卷积的实现\n#include&lt;cassert&gt;\n#include&lt;vector&gt;\n\n\nvoid conv2(int** filter, int **mat, int** res, const int filter_rows, const int filter_cols, const int mat_rows, const int mat_cols);//指针数组版本\nstd::vector&lt;std::vector&lt;int&gt; &gt; conv2(std::vector&lt;std::vector&lt;int&gt; &gt; filter, std::vector&lt;std::vector&lt;int&gt; &gt; mat);//向量版本\n\n\nint main(void)\n{\n    return 0;\n}//main\n\nvoid conv2(int** filter, int **mat, int** res, const int filter_rows, const int filter_cols, const int mat_rows, const int mat_cols)\n{\n    assert(filter_cols &lt; mat_cols &amp;&amp; filter_rows &lt; mat_rows);\n    for(int i = 0; i &lt; mat_rows - 1; ++i)\n        for (int j = 0; j &lt; mat_cols - 1; ++j)\n        {\n            int tmp = 0;\n            for (int m = 0; m &lt; filter_rows; ++m)\n                for (int n = 0; n &lt; filter_cols; ++n)\n                    if(0 &lt;= i -m  &amp;&amp; i - m &lt; mat_rows &amp;&amp; 0 &lt;= j - n &amp;&amp; j - n &lt; mat_cols)\n                        tmp += filter[m][n] * mat[i - m][j - n];//卷积公式\n\n            res[i][j] = tmp;\n        }\n}\n\nstd::vector&lt;std::vector&lt;int&gt; &gt; conv2(std::vector&lt;std::vector&lt;int&gt; &gt; filter, std::vector&lt;std::vector&lt;int&gt; &gt; mat )//向量版本\n{\n    const int filter_rows = filter.size();\n    const int filter_cols = filter[0].size();\n\n    const int mat_rows = mat.size();\n    const int mat_cols = mat[0].size();\n\n    assert(filter_cols &lt; mat_cols &amp;&amp; filter_rows &lt; mat_rows);\n    std::vector&lt;std::vector&lt;int&gt; &gt; res(mat_rows, std::vector&lt;int&gt;(mat_cols, 0));\n\n    for (int i = 0; i &lt; mat_rows - 1; ++i)\n        for (int j = 0; j &lt; mat_cols - 1; ++j)\n        {\n            int tmp = 0;\n            for (int m = 0; m &lt; filter_rows; ++m)\n                for (int n = 0; n &lt; filter_cols; ++n)\n                    if (0 &lt;= i - m &amp;&amp; i - m &lt; mat_rows &amp;&amp; 0 &lt;= j - n &amp;&amp; j - n &lt; mat_cols)\n                        tmp += filter[m][n] * mat[i - m][j - n];//卷积公式\n\n            res[i][j] = tmp;\n        }\n    return res;\n}\n4. Pooling\nmaxpooling\n版本1(简单版)\ndef max_pooling(x, kernel_size=2, stride=2):\n    b, c_in, h, w = x.shape\n    ow = (w - kernel_size) // stride + 1\n    oh = (h - kernel_size) // stride + 1\n\n    out = np.zeros([b, c_in, oh, ow])\n    x_input = x\n    for y in range(oh):\n        for x in range(ow):\n            roi = x_input[..., y * stride: y * stride + kernel_size, x * stride: x * stride + kernel_size]\n            max_val = np.squeeze(np.max(roi, axis=(2, 3), keepdims=True), axis=(2, 3))\n            out[..., y, x] = max_val\n    return out\n版本2(反向传播)\nimport numpy as np\nimport torch\nclass MaxPooling2D:\n    def __init__(self, kernel_size=(2, 2), stride=2):\n        self.kernel_size = kernel_size\n        self.w_height = kernel_size[0]\n        self.w_width = kernel_size[1]\n\n        self.stride = stride\n\n        self.x = None\n        self.in_height = None\n        self.in_width = None\n\n        self.out_height = None\n        self.out_width = None\n\n        self.arg_max = None\n\n    def __call__(self, x):\n        self.x = x\n        self.in_height = np.shape(x)[0]\n        self.in_width = np.shape(x)[1]\n\n        self.out_height = int((self.in_height - self.w_height) / self.stride) + 1\n        self.out_width = int((self.in_width - self.w_width) / self.stride) + 1\n\n        out = np.zeros((self.out_height, self.out_width))\n        self.arg_max = np.zeros_like(out, dtype=np.int32)\n\n        for i in range(self.out_height):\n            for j in range(self.out_width):\n                start_i = i * self.stride\n                start_j = j * self.stride\n                end_i = start_i + self.w_height\n                end_j = start_j + self.w_width\n                out[i, j] = np.max(x[start_i: end_i, start_j: end_j])\n                self.arg_max[i, j] = np.argmax(x[start_i: end_i, start_j: end_j])\n        self.arg_max = self.arg_max\n        return out\n\n    def backward(self, d_loss):\n        dx = np.zeros_like(self.x)\n        for i in range(self.out_height):\n            for j in range(self.out_width):\n                start_i = i * self.stride\n                start_j = j * self.stride\n                end_i = start_i + self.w_height\n                end_j = start_j + self.w_width\n                index = np.unravel_index(self.arg_max[i, j], self.kernel_size)\n                dx[start_i:end_i, start_j:end_j][index] = d_loss[i, j] #\n        return dx\n\ntest\n\nnp.set_printoptions(precision=8, suppress=True, linewidth=120)\nx_numpy = np.random.random((1, 1, 6, 9))\nx_tensor = torch.tensor(x_numpy, requires_grad=True)\n\nmax_pool_tensor = torch.nn.MaxPool2d((2, 2), 2)\nmax_pool_numpy = MaxPooling2D((2, 2), stride=2)\n\nout_numpy = max_pool_numpy(x_numpy[0, 0])\nout_tensor = max_pool_tensor(x_tensor)\n\nd_loss_numpy = np.random.random(out_tensor.shape)\nd_loss_tensor = torch.tensor(d_loss_numpy, requires_grad=True)\nout_tensor.backward(d_loss_tensor)\n\ndx_numpy = max_pool_numpy.backward(d_loss_numpy[0, 0])\ndx_tensor = x_tensor.grad\n# print('input \\n', x_numpy)\nprint(\"out_numpy \\n\", out_numpy)\nprint(\"out_tensor \\n\", out_tensor.data.numpy())\n\nprint(\"dx_numpy \\n\", dx_numpy)\nprint(\"dx_tensor \\n\", dx_tensor.data.numpy())\ndef pooling(feature_map, size=2, stride=2):\n    channel=feature_map.shape[0]\n    height=feature_map.shape[1]\n    width=feature_map.shape[2]\n    padding_height=np.uint16(round((height-size+1)/stride))\n    padding_width=np.uint16(round((width-size+1)/stride))\n    print(padding_height,padding_width)\n\n    pool_out = np.zeros((channel,padding_height,padding_width),dtype=np.uint8)\n    \n    for map_num in range(channel):  \n        out_height = 0  \n        for r in np.arange(0,height, stride):  \n            out_width = 0  \n            for c in np.arange(0, width, stride):  \n                pool_out[map_num,out_height, out_width] = np.max(feature_map[map_num,r:r+size,c:c+size])  \n                out_width=out_width+1\n            out_height=out_height+1\n    return pool_out\navg-pooling\n版本1(简单版)\ndef avg_pooling(x, kernel_size=2, stride=2): b, c_in, h, w = x.shape ow = (w - kernel_size) // stride + 1 oh = (h - kernel_size) // stride + 1\nout = np.zeros([b, c_in, oh, ow])\nx_input = x\nfor y in range(oh):\n    for x in range(ow):\n        roi = x_input[..., y * stride: y * stride + kernel_size, x * stride: x * stride + kernel_size]\n        max_val = np.average(roi, axis=(2, 3))\n        out[..., y, x] = max_val\nreturn out\n版本2(反向传播)\nimport numpy as np\nimport torch\n\nclass AvgPooling2D:\n    def __init__(self, kernel_size=(2, 2), stride=2):\n        self.stride = stride\n        self.kernel_size = kernel_size\n        self.w_height = kernel_size[0]\n        self.w_width = kernel_size[1]\n\n    def __call__(self, x):\n        self.x = x\n        self.in_height = x.shape[0]\n        self.in_width = x.shape[1]\n\n        self.out_height = int((self.in_height - self.w_height) / self.stride) + 1\n        self.out_width = int((self.in_width - self.w_width) / self.stride) + 1\n        out = np.zeros((self.out_height, self.out_width))\n\n        for i in range(self.out_height):\n            for j in range(self.out_width):\n                start_i = i * self.stride\n                start_j = j * self.stride\n                end_i = start_i + self.w_height\n                end_j = start_j + self.w_width\n                out[i, j] = np.mean(x[start_i: end_i, start_j: end_j])\n        return out\n\n    def backward(self, d_loss):\n        dx = np.zeros_like(self.x)\n\n        for i in range(self.out_height):\n            for j in range(self.out_width):\n                start_i = i * self.stride\n                start_j = j * self.stride\n                end_i = start_i + self.w_height\n                end_j = start_j + self.w_width\n                dx[start_i: end_i, start_j: end_j] = d_loss[i, j] / (self.w_width * self.w_height)\n        return dx\n\n\ntest\n\nnp.set_printoptions(precision=8, suppress=True, linewidth=120)\nx_numpy = np.random.random((1, 1, 6, 9))\nx_tensor = torch.tensor(x_numpy, requires_grad=True)\n\navg_pool_tensor = torch.nn.AvgPool2d((2, 2), 2)\navg_pool_numpy = AvgPooling2D((2, 2), stride=2)\n\nout_numpy = avg_pool_numpy(x_numpy[0, 0])\nout_tensor = avg_pool_tensor(x_tensor)\n\nd_loss_numpy = np.random.random(out_tensor.shape)\nd_loss_tensor = torch.tensor(d_loss_numpy, requires_grad=True)\nout_tensor.backward(d_loss_tensor)\n\ndx_numpy = avg_pool_numpy.backward(d_loss_numpy[0, 0])\ndx_tensor = x_tensor.grad\n# print('input \\n', x_numpy)\nprint(\"out_numpy \\n\", out_numpy)\nprint(\"out_tensor \\n\", out_tensor.data.numpy())\n\nprint(\"dx_numpy \\n\", dx_numpy)\nprint(\"dx_tensor \\n\", dx_tensor.data.numpy())\n5. mAP\npython\nc++\n6. softnms\npython\n版本1\nsoft_nms操作，这里假设boxes是无序(未按score做降序)的，所以每轮soft_nms迭代都需要类似冒泡排序操作，选择当前top-1 bbox做NMS\nNt：计算IoU的阈值，IoU &gt; Nt，对应bbox的score权重就要降低\nthreshold：降权后通过threshold进一步剔除低权重bbox\ndef cpu_soft_nms(boxes, sigma=0.5, Nt=0.3, threshold=0.001, method=0): N = boxes.shape[0] for i in range(N): maxscore = boxes[i, 4] # 获取当前index下的bbox maxpos = i\n    tx1 = boxes[i, 0]\n    ty1 = boxes[i, 1]\n    tx2 = boxes[i, 2]\n    ty2 = boxes[i, 3]\n    ts = boxes[i, 4]\n\n    pos = i + 1  # 下面操作就很常规了，找到当前index i之后所有bboxes中，score最大的bbox，并将之赋值给maxscore、maxpos\n    while pos &lt; N:\n        if maxscore &lt; boxes[pos, 4]:\n            maxscore = boxes[pos, 4]\n            maxpos = pos\n        pos = pos + 1\n\n    # 下面操作更简单，想想我们最开始学C语言，a、b两变量如何交换\n    # add max box as a detection\n    boxes[i, 0] = boxes[maxpos, 0]  # maxpos内的信息，放到index i处，也是当前需要处理的bbox\n    boxes[i, 1] = boxes[maxpos, 1]\n    boxes[i, 2] = boxes[maxpos, 2]\n    boxes[i, 3] = boxes[maxpos, 3]\n    boxes[i, 4] = boxes[maxpos, 4]\n\n    # swap ith box with position of max box\n    boxes[maxpos, 0] = tx1  # 别忘了tx1中可是保存了boxes[i,0]备份的\n    boxes[maxpos, 1] = ty1\n    boxes[maxpos, 2] = tx2\n    boxes[maxpos, 3] = ty2\n    boxes[maxpos, 4] = ts\n\n    tx1 = boxes[i, 0]  # 此时tx1就保存的maxpos位置的bbox信息了\n    ty1 = boxes[i, 1]\n    tx2 = boxes[i, 2]\n    ty2 = boxes[i, 3]\n    ts = boxes[i, 4]\n\n    pos = i + 1\n    # NMS iterations, note that N changes if detection boxes fall below threshold，N值是动态变化的\n    while pos &lt; N:  # 向后做NMS比较\n        x1 = boxes[pos, 0]  # 当前位置的bbox\n        y1 = boxes[pos, 1]\n        x2 = boxes[pos, 2]\n        y2 = boxes[pos, 3]\n        s = boxes[pos, 4]\n\n        area = (x2 - x1 + 1) * (y2 - y1 + 1)  # pos下box的面积\n        iw = (min(tx2, x2) - max(tx1, x1) + 1)  # 计算Insection的宽iw，如果iw &lt; 0，说明没相交，可以直接忽略了\n        if iw &gt; 0:\n            ih = (min(ty2, y2) - max(ty1, y1) + 1)  # 计算Insection的宽ih，如果ih &lt; 0，说明没相交，可以直接忽略了\n            if ih &gt; 0:\n                ua = float((tx2 - tx1 + 1) * (ty2 - ty1 + 1) + area - iw * ih)  # U的面积\n                ov = iw * ih / ua  # iou between max box and detection box\n\n                if method == 1:  # soft_nms中linear降权操作，与ov负相关\n                    if ov &gt; Nt:\n                        weight = 1 - ov\n                    else:\n                        weight = 1\n                elif method == 2:  # soft_nms中gaussian降权操作\n                    weight = np.exp(-(ov * ov) / sigma)\n                else:  # original NMS，weight = 0就直接把score置0\n                    if ov &gt; Nt:\n                        weight = 0\n                    else:\n                        weight = 1\n\n                boxes[pos, 4] = weight * boxes[pos, 4]  # 权重重新调整\n\n                # if box score falls below threshold, discard the box by swapping with last box，update N\n                # 如果bbox调整后的权重，已经小于阈值threshold，那么这个bbox就可以忽略了，\n                # 操作方式是直接用最后一个有效的bbox替换当前pos上的bbox\n                if boxes[pos, 4] &lt; threshold:\n                    boxes[pos, 0] = boxes[N - 1, 0]\n                    boxes[pos, 1] = boxes[N - 1, 1]\n                    boxes[pos, 2] = boxes[N - 1, 2]\n                    boxes[pos, 3] = boxes[N - 1, 3]\n                    boxes[pos, 4] = boxes[N - 1, 4]\n                    N = N - 1  # N-1位置上的bbox已经赋值到前面了，该bbox就可以忽略了；\n                    pos = pos - 1  # pos位置上引入了新的有效bbox(N-1)，就需要再计算一遍了\n\n        pos = pos + 1  # 当前pos bbox计算完毕\n\n# 求满足soft_nms筛选条件的所有bbox数量，并打散为list，但一个问题是：如何与bbox index对应起来？\n# 方式很简单，bbox也做了对应的调整、筛选，bbox list中top-N就对应着最高score，且soft-nms筛选通过的bbox，\n# 不过每个bbox的score也同样经过soft-nms调整了\nkeep = [i for i in range(N)]\n\nreturn keep\n版本2\ndef py_cpu_softnms(dets, sc, Nt=0.3, sigma=0.5, thresh=0.001, method=2):\n    \"\"\"\n    py_cpu_softnms\n    :param dets:   boexs 坐标矩阵 format [y1, x1, y2, x2]\n    :param sc:     每个 boxes 对应的分数\n    :param Nt:     iou 交叠门限\n    :param sigma:  使用 gaussian 函数的方差\n    :param thresh: 最后的分数门限\n    :param method: 使用的方法\n    :return:       留下的 boxes 的 index\n    \"\"\"\n\n    # indexes concatenate boxes with the last column\n    N = dets.shape[0]\n    indexes = np.array([np.arange(N)])\n    dets = np.concatenate((dets, indexes.T), axis=1)\n\n    # the order of boxes coordinate is [y1,x1,y2,x2]\n    y1 = dets[:, 0]\n    x1 = dets[:, 1]\n    y2 = dets[:, 2]\n    x2 = dets[:, 3]\n    scores = sc\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n\n    for i in range(N):\n        # intermediate parameters for later parameters exchange\n        tBD = dets[i, :].copy()\n        tscore = scores[i].copy()\n        tarea = areas[i].copy()\n        pos = i + 1\n\n        #\n        if i != N-1:\n            maxscore = np.max(scores[pos:], axis=0)\n            maxpos = np.argmax(scores[pos:], axis=0)\n        else:\n            maxscore = scores[-1]\n            maxpos = 0\n        if tscore &lt; maxscore:\n            dets[i, :] = dets[maxpos + i + 1, :]\n            dets[maxpos + i + 1, :] = tBD\n            tBD = dets[i, :]\n\n            scores[i] = scores[maxpos + i + 1]\n            scores[maxpos + i + 1] = tscore\n            tscore = scores[i]\n\n            areas[i] = areas[maxpos + i + 1]\n            areas[maxpos + i + 1] = tarea\n            tarea = areas[i]\n\n        # IoU calculate\n        xx1 = np.maximum(dets[i, 1], dets[pos:, 1])\n        yy1 = np.maximum(dets[i, 0], dets[pos:, 0])\n        xx2 = np.minimum(dets[i, 3], dets[pos:, 3])\n        yy2 = np.minimum(dets[i, 2], dets[pos:, 2])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[pos:] - inter)\n\n        # Three methods: 1.linear 2.gaussian 3.original NMS\n        if method == 1:  # linear\n            weight = np.ones(ovr.shape)\n            weight[ovr &gt; Nt] = weight[ovr &gt; Nt] - ovr[ovr &gt; Nt]\n        elif method == 2:  # gaussian\n            weight = np.exp(-(ovr * ovr) / sigma)\n        else:  # original NMS\n            weight = np.ones(ovr.shape)\n            weight[ovr &gt; Nt] = 0\n\n        scores[pos:] = weight * scores[pos:]\n\n    # select the boxes and keep the corresponding indexes\n    inds = dets[:, 4][scores &gt; thresh]\n    keep = inds.astype(int)\n\n    return keep\ndef test():\n    # boxes and scores\n    boxes = np.array([[200, 200, 400, 400], [220, 220, 420, 420], [200, 240, 400, 440], [240, 200, 440, 400], [1, 1, 2, 2]], dtype=np.float32)\n    boxscores = np.array([0.9, 0.8, 0.7, 0.6, 0.5], dtype=np.float32)\n    index = py_cpu_softnms(boxes, boxscores, method=3)\nc++\n\n#include &lt;bits/stdc++.h&gt;\n\nnamespace nms\n{\nstruct proposal\n{\n  float score, x1, y1, x2, y2;\n};\n\ninline static bool cmp(const proposal&amp; a, const proposal&amp; b)\n{\n  return a.score &lt; b.score;\n}\n\ninline static float iou(const proposal&amp;, const proposal&amp;) __attribute__((always_inline));\n\nstatic float iou(const proposal&amp; a, const proposal&amp; b)\n{\n  auto overlap = 0.f;\n  float iw  = std::min(b.x2, a.x2) - std::max(b.x1, a.x1) + 1;\n  if (iw &gt; 0) {\n    float ih = std::min(b.y2, a.y2) - std::max(b.y1, a.y1) + 1;\n    if (ih &gt; 0) {\n      float ab = (b.x2 - b.x1 + 1) * (b.y2 - b.y1 + 1);\n      float aa = (a.x2 - a.x1 + 1) * (a.y2 - a.y1 + 1);\n      float inter = iw * ih;\n      overlap = inter / (aa + ab - inter);\n    }\n  }\n  return overlap;\n}\n\nenum class Method : uint32_t\n{\n  LINEAR = 0,\n  GAUSSIAN,\n  HARD\n};\n\nsize_t soft_nms(float* boxes,\n                int32_t* index,\n                size_t count,\n                Method method,\n                float Nt,\n                float sigma,\n                float threshold)\n{\n  std::iota(index, index + count, 0);  // np.arange()\n  auto p = reinterpret_cast&lt;proposal*&gt;(boxes);\n\n  auto N = count;\n  for (size_t i = 0; i &lt; N; ++i) {\n    auto max = std::max_element(p + i, p + N, cmp);\n    std::swap(p[i], *max);\n    std::swap(index[i], index[max - p]);\n\n    auto j      = i + 1;\n    auto weight = 0.f;\n    while (j &lt; N) {\n      auto ov = iou(p[i], p[j]);\n      switch (method) {\n        case Method::LINEAR:\n          weight = ov &gt; Nt ? 1.f - ov : 1.f;\n          break;\n        case Method::GAUSSIAN:\n          weight = std::exp(-(ov * ov) / sigma);\n          break;\n        case Method::HARD:\n          weight = ov &gt; Nt ? 0.f : 1.f;\n          break;\n      }\n      p[j].score *= weight;\n      if (p[j].score &lt; threshold) {\n        N--;\n        std::swap(p[j], p[N]);\n        std::swap(index[j], index[N]);\n        j--;\n      }\n      j++;\n    }\n  };\n\n  return N;\n}\n} /* namespace nms */\n\n7. 实现one-hot特征\npython\none_hot_t= np.zeros_like(y)  #生成和y形状一样的元素为零的数组\nfor j, i in zip(range(t.size), t):\n    #有多少个样本就应该对应多少个标签\n    one_hot_t[j][i] = 1      #变为one-hot类型标签：j表示样本，i表示标签索引\n\n8. softmax\npython\n由于指数函数的放大作用过于明显，如果直接使用softmax计算公式𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑥𝑖)=𝑒𝑥𝑝(𝑥𝑖)/∑𝑒𝑥𝑝(𝑥𝑗)进行函数实现，容易导致数据溢出(上溢)。所以我们在函数实现时利用其性质：先对输入数据进行处理，之后再利用计算公式计算。具体使得实现步骤为： 查找每个向量x的最大值c； 每个向量减去其最大值c, 得到向量y = x-c; 利用公式进行计算,softmax(x) = softmax(x-c) = softmax(y)\nimport numpy as np\n\ndef softmax(x: np.array):\n    x_max = np.max(x, axis=-1, keepdims=True)\n    x -= x_max\n    x_exp = np.exp(x)\n    s = x_exp / np.sum(x_exp, axis=-1, keepdims=True)\n    return s\n\n9. 各种滤波\n马赛克\n马赛克的实现原理是把图像上某个像素点一定范围邻域内的所有点用邻域内左上像素点的颜色代替，这样可以模糊细节，但是可以保留大体的轮廓。 import cv2\n\ndef do_mosaic(frame, x, y, w, h, neighbor=9):\n    \"\"\"\n    :param frame: opencv frame\n    :param int x :  马赛克左顶点\n    :param int y:  马赛克右顶点\n    :param int w:  马赛克宽\n    :param int h:  马赛克高\n    :param int neighbor:  马赛克每一块的宽\n    \"\"\"\n    fh, fw = frame.shape[0], frame.shape[1]\n    if (y + h &gt; fh) or (x + w &gt; fw):\n        return\n    for i in range(0, h - neighbor, neighbor):  # 关键点0 减去neightbour 防止溢出\n        for j in range(0, w - neighbor, neighbor):\n            rect = [j + x, i + y, neighbor, neighbor]\n            color = frame[i + y][j + x].tolist()  # 关键点1 tolist\n            left_up = (rect[0], rect[1])\n            right_down = (rect[0] + neighbor - 1, rect[1] + neighbor - 1)  # 关键点2 减去一个像素\n            cv2.rectangle(frame, left_up, right_down, color, -1)\n\n\nim = cv2.imread('test.jpg', 1)\ndo_mosaic(im, 219, 61, 460 - 219, 412 - 61)\n高斯滤波\n出处\n\n二维高斯函数\n\nimport cv2\nimport numpy as np\n\ndef gaussian_filter(img, K_size=3, sigma=1.3):\n\n    if len(img.shape) == 3:\n        H, W, C = img.shape\n    else:\n        img = np.expand_dims(img, axis=-1)\n        H, W, C = img.shape\n\n    ## Zero padding\n    pad = K_size // 2\n    out = np.zeros((H + pad * 2, W + pad * 2, C), dtype=np.float)\n    out[pad: pad + H, pad: pad + W] = img.copy().astype(np.float)\n\n    ## prepare Kernel\n    K = np.zeros((K_size, K_size), dtype=np.float)\n    for x in range(-pad, -pad + K_size):\n        for y in range(-pad, -pad + K_size):\n            K[y + pad, x + pad] = np.exp( -(x ** 2 + y ** 2) / (2 * (sigma ** 2)))\n\n    K /= (2 * np.pi * sigma * sigma)\n    K /= K.sum()\n    tmp = out.copy()\n\n    # filtering\n    for y in range(H):\n        for x in range(W):\n            for c in range(C):\n                out[pad + y, pad + x, c] = np.sum(K * tmp[y: y + K_size, x: x + K_size, c])\n\n    out = np.clip(out, 0, 255)\n    out = out[pad: pad + H, pad: pad + W].astype(np.uint8)\n    return out\n\n均值滤波\nimport numpy as np\n\n\ndef means_filter(input_image, filter_size):\n    '''\n    均值滤波器\n    :param input_image: 输入图像\n    :param filter_size: 滤波器大小\n    :return: 输出图像\n\n    注：此实现滤波器大小必须为奇数且 &gt;= 3\n    '''\n    input_image_cp = np.copy(input_image)  # 输入图像的副本\n    filter_template = np.ones((filter_size, filter_size))  # 空间滤波器模板\n    pad_num = int((filter_size - 1) / 2)  # 输入图像需要填充的尺寸\n    input_image_cp = np.pad(input_image_cp, (pad_num, pad_num), mode=\"constant\", constant_values=0)  # 填充输入图像\n    m, n = input_image_cp.shape  # 获取填充后的输入图像的大小\n    output_image = np.copy(input_image_cp)  # 输出图像\n\n    # 空间滤波\n    for i in range(pad_num, m - pad_num):\n        for j in range(pad_num, n - pad_num):\n            output_image[i, j] = np.sum(filter_template * input_image_cp[i - pad_num:i + pad_num + 1, j - pad_num:j + pad_num + 1]) / (filter_size ** 2)\n    output_image = output_image[pad_num:m - pad_num, pad_num:n - pad_num]  # 裁剪\n\n    return output_image\n中值滤波\n# 中值滤波#\nimport cv2\nimport numpy as np\n\ndef MedianFilter(img,k=3,padding=None):\n    imarray=img\n    height = imarray.shape[0]\n    width = imarray.shape[1]\n    if not padding:\n        edge = int((k - 1) / 2)\n        if height - 1 - edge &lt;= edge or width - 1 - edge &lt;= edge:\n            print(\"The parameter k is to large.\")\n            return None\n        new_arr = np.zeros((height, width), dtype=\"uint8\")\n        for i in range(edge,height-edge):\n            for j in range(edge,width-edge):\n                new_arr[i, j] = np.median(imarray[i - edge:i + edge + 1, j - edge:j + edge + 1])# 调用np.median求取中值\n    return new_arr\n\nKmeans\nhttps://zhuanlan.zhihu.com/p/35959301 #### 积分图均值滤波 https://blog.csdn.net/weixin_40647819/article/details/88775598\n","slug":"手撕代码","date":"2021-10-19T03:41:08.000Z","categories_index":"","tags_index":"interview summary","author_index":"Hulk Wang"},{"id":"2d84f3892209cec11720cffbf464a897","title":"CV面试基础总结","content":"1. 评测指标\n1.1 基本概念\n1.1.1 TP TN FP FN\n\nT-Ture;F-False 表示预测结果的正确性，T表示预测正确，F表示预测错误； P-positive;N-negative 表示预测的正负性，P表示预测为正样本，N表示预测为负样本；\n\n\n\n---\n---\n\n\n\n\nTP——True Positive\n真正例，表示样本为正，预测值为正——预测正确T\n\n\nFP——False Positive\n假正例，表示样本为负，预测值为正——预测错误F\n\n\nFN——False Negative\n假负例，表示样本为正，预测值为负——预测错误F\n\n\nTN——True Negative\n真负例，表示样本为负，预测值为负——预测正确T\n\n\n\n1.1.2 Accuracy Precision Recall\n\nAccuracy:\n\n用于判定预测结果的准确程度，即预测正确的总数/样本总数。 预测正确又分为两种情况：样本为正、预测为正和样本为负、预测为负，即TP+TN。\n\n\nPrecision:\n\n精确率用于描述在所有预测为正的样本中，预测正确的比例,在有的翻译中，Precision也被称为查准率。 查准率针对的是预测，顾名思义，查准率主要用来判断“查的准不准”，其依据是查出的正例占所有正例预测的比率。\n\n\nRecall:\n\n和精确率不同，Precison针对的是预测，Recall针对的是样本。召回率表示在所有正例的样本中，有多少被预测/检测了出来，所以在有的翻译中，Recall也被称为查全率。 查全率针对的是样本，即对样本而言，有多少比例的正例样本在预测中被检出。\n\n\n\n\n\n\n\n\n\n\n最优目标是希望查准率和查全率都接近100%，但通常这二者的关系是负相关.\n1.1.3 F1-score\nF1 score是围绕Recall和Precision衍生出来的一个参考值，公式 = Precision和Recall的调和平均值\n\n\n\n\n\n\n\n\n\n\n其数值大小通常接近二者中的较小数、当recall = precision时，F1 = recall = precision，如果F1值较高，说明recall和precision都较高,我们希望取到较高的F1值。\n1.1.4 PR曲线\nPrecision-Recall curve，即PR曲线，是在Precision查准率和Recall查全率概念上衍生出的曲线，X轴为Recall，Y轴为Precision。如下图A,B,C三个模型，绘制出的PR曲线：\n\n\n\n\n\n\n\n\n\n\n我们可以通过PR曲线，找到最优化的模型。比较上图A和C模型，很容易比较出A模型更优，对于模型的PR曲线来说，如果这条曲线能包住另一条模型曲线，则可以肯定这条曲线下的模型更优秀（因为其完全覆盖了模型C,故可以轻易取到PR值都接近理想话（1,1）的点）但当模型的PR图有交叉时，就不太容易通过肉眼比较了，譬如这里A和B。\n\n\n\n\n\n\n\n\n\n对于模型PR曲线有交叉的情况，我们可以考察其F1值，F1值越大，模型越优！（因为通常F1值越高，反应到图像上，表明此模型曲线围绕X轴覆盖的面积更大，而面积越接近1，表明F和P两者都很高）上图A和B模型，模型A的F1&gt;模型B的F1值，故模型A更优秀。\n1.1.5 ROC曲线\nReceiver Operating Characteristic curve，即ROC曲线，译为：受试者工作特征曲线。ROC曲线起源于“二战”，是一个用于敌机检测的雷达信号分析技术，后来应用到了医疗检测、心理学等领域，1989年被引入机器学习领域。在ROC曲线中，X轴为FPR（假正例率）,Y轴为TPR（真正例率）。其中FPR是False Positive Rate缩写，即FP率（假正例率），TPR是True Positive Rate的缩写，即TP率（真正例率）。结合图示更清晰：\nTPR分子：TP,即预测为正例，实际为正理例（预测正确）； TPR分母：所有正例样本 = TP + FN\nFPR分子：FP,即预测为正例，实际为负例（预测错误）； FPR分母：所有负例样本数 = FP + TN\n\n\n\n\n\n\n\n\n\n可以看出，TPR实际上就 = Recall查全率,FPR实际上就 = Precision查准率，只是两者X轴和Y轴次序相反\n\n\n\n\n\n\n\n\n\n\n当TPR越大，而FPR越小时，说明分类结果是较好的; 和PR曲线类似，如果一个模型曲线将另一条模型曲线“完全包住”，则可以断定此模型性能更优！否则，还是需要通过ROC曲线围X轴的面积来确定模型优劣，此面积即AUC(Area Under ROC Curve)\n1.1.6 AUC\nAUC，即Area Under ROC Curve，表示ROC曲线中，曲线和X轴围成的面积，通常比较两个模型的AUC大小，AUC大者，表明其面积大，更接近1,模型的TPR和FPR两者都相对较高，模型更优。\n1.1.7 AP,mAP\nAverage Precision平均精准率 = （某一个类别）每个样本的精确率求和/样本总数N。 看一个例子，假设VOC数据集中，分别有person,train,cat,dog...总计20个分类类别，测试集有1000张图片，则针对其中某一类，譬如cat类，我们计算其平均精准率AP即可用：,为什么是1000张图片的Precision累加？因为每一张图都是潜在的待分类图片，可能包含20个类别中的1～多个分类，且可能包含这些分类下的1～n个bounding box，故对于某个分类，单独一张图片有需要独立计算其精准度。\nMean Average Precision均值平均精准率 = 所有类别的AP值累加求和/类别数\n\n1.1.8 IS(inception score)\n\n\n\n\n\n\n\n\n\n衡量图片的生成质量是一个比较难的问题，一直以来也没有一个特别好的度量方式，inception score的思想，是通过将生成模型的评估问题，通过映射到分类器上，以此来简化评估的难易程度，是一个非常好的创新。当然这种映射必然存在的问题就是信息的丢失。真实图片的inception_score是肯定比较高的，但是inception score高并不能代表生成的质量就好。(最简单的例子就是通过adversarial samples,可以通过简单的生成每个类的特征图谱，看似噪声的图像，但是其inception score会很高。)\n\n\n\n\n\n\n\n\n\nIS值越高，图片质量和多样性则好\nIS用来衡量GAN网络的两个指标：\n\n生成图片的质量\n多样性\n\n1.1.8.1 图片的质量\n熵（entropy）可以用来描述随机性：如果一个随机变量是高度可预测的，那么它就有较低的熵；相反，如果它是高度不可预测，那么它就用较高的熵。 如下图，我们有两个概率分布，p2比p1有更高的熵值，因为p2是一个均匀分布，我们很难预测x的值。\n\n在GAN中，我们希望条件概率｜可以被高度预测x表示给定的图片，y表示这个图片包含的主要物体，看到后面你会更加清楚这个概率是什么意思），也就是希望它的熵值较低。例如，给定一个图片，我们很容易的知道其中包含什么物体。 因此，我们使用inception network（可以理解这是一个固定的分类网络）来对生成的图像进行分类。（这里都是针对ImageNet数据集而言）然后预测 ｜， 这里y就是标签。用这个概率来反应图片的质量。 简单来说，假如inception network能够以较高的概率预测图片中包含的物体，也就是有很高的把握对其进行正确分类，这就说明图片质量较高。相反，比如我们人眼并无法看出这张图片是什么，就说明这个图片质量不高。\n\n\n\n\n\n\n\n\n\n综上，我们知道概率｜代表了图片的质量，概率越大，质量则越高。\n1.1.8.2 多样性\n如果生成的图像多样化很好，那么预测的标签y的分布则会有较高的熵，因为数量多了，我们就更难准确预测 y。\n结合以上两个指标来说，我们的目标应该就是这样的： 1. 图片质量：针对每一张生成的图片，已知的分类器应该很确信的知道它属于哪一类。而这可以用条件概率｜来表示，它越大越好。而｜熵应该是越小越好。\n\n图片的多样性：我们这时候考虑的是标签的分布情况，我们希望标签分布均匀，而不希望模型生成的都是某一类图片。这时候我们考虑的不是条件概率了，而是边缘概率，也就是P(y),展开来写应该是 ，这里的n就是原训练数据的类数。我们希望，从熵的角度来说，我们希望 P(y)熵越大越好。\n\n\n\n\n\n\n\n\n\n\n1.最大化H(y);也就是对于输入的样本，通过inception_v3模型后的类别要均衡，衡量模式坍塌。\n\n\n\n\n\n\n\n\n\n2.最小化H(y|x);说明对于输入的样本，通过inception_v3模型后预测某类别的置信度要高，衡量图片生成的质量。\nIS缺点：当只产生一种物体的图像时，我们仍会认为这是均匀分布，而导致评价不正确。当模型坍塌时，结果就可能产生同样的图片。\n1.1.9 FID\n在计算FID中我们也同样使用inception network网络。我们还是先来简单回顾一下什么是inception network，它就是一个特征提取的深度网络，最后一层是一个pooling层，然后可以输出一张图像的类别。在计算FID时，我们去掉这个最后一层pooling层，得到的是一个2048维的高层特征，以下简称n维特征。我们继续简化一下，那么这个n维特征是一个向量。则有：对于我们已经拥有的真实图像，这个向量是服从一个分布的，（我们可以假设它是服从一个高斯分布）；对于那些用GAN来生成的n维特征它也是一个分布；我们应该立马能够知道了，GAN的目标就是使得两个分布尽量相同。假如两个分布相同，那么生成图像的真实性和多样性就和训练数据相同了。 于是，现在的问题就是，怎么计算两个分布之间的距离呢？我们需要注意到这两个分布是多变量的，也就是前面提到的n维特征。也就是说我们计算的是两个多维变量分布之间的距离，数学上可以用Wasserstein-2 distance或者Frechet distance来进行计算。以下简单介绍一下如何计算这个距离。\n\n\n\n\n\n\n\n\n\n假如一个随机变量服从高斯分布，这个分布可以用一个均值和方差来确定。那么两个分布只要均值和方差相同，则两个分布相同。我们就利用这个均值和方差来计算这两个单变量高斯分布之间的距离。但我们这里是多维的分布，我们知道协方差矩阵可以用来衡量两个维度之间的相关性。所以，我们使用均值和协方差矩阵来计算两个分布之间的距离。均值的维度就是前面n维特征的维度，也就是n维；协方差矩阵则是n*n的矩阵。\nFID公式： \n公式中，Tr表示矩阵对角线上元素的总和，矩阵论中俗称“迹”（trace）。均值为μ,协方差为Σ 。此外x表示真实的图片，g是生成的图片。 较低的FID意味着两个分布之间更接近，也就意味着生成图片的质量较高、多样性较好。 FID对模型坍塌更加敏感。相比较IS来说，FID对噪声有更好的鲁棒性。因为假如只有一种图片时，FID这个距离将会相当的高。因此，FID更适合描述GAN网络的多样性。\n\n\n\n\n\n\n\n\n\n同样的，FID和IS都是基于特征提取，也就是依赖于某些特征的出现或者不出现。但是他们都无法描述这些特征的空间关系。如下图,这里我们我们人不会认为这是一张好的人脸图片。但是根据FID和IS，他们就是一个很好的人脸图片。因为它有人脸必要的特征，虽然这些特征的空间关系不好。\n\n1.2 实际问题\n1.2.1 样本不均衡的情况下Accuracy、ROC和PR曲线的差别和表现\n\n\n兼顾正例和负例的权衡。因为TPR聚焦于正例，FPR聚焦于与负例，使其成为一个比较均衡的评估方法。\nROC曲线选用的两个指标,都不依赖于具体的类别分布。\n\n注意TPR用到的TP和FN同属P列，FPR用到的FP和TN同属N列，所以即使P或N的整体数量发生了改变，也不会影响到另一列。也就是说，即使正例与负例的比例发生了很大变化，ROC曲线也不会产生大的变化，而像Precision使用的TP和FP就分属两列，则易受类别分布改变的影响。\n上面提到ROC曲线的优点是不会随着类别分布的改变而改变，但这在某种程度上也是其缺点。因为负例N增加了很多，而曲线却没变，这等于产生了大量FP。像信息检索中如果主要关心正例的预测准确性的话，这就不可接受了。在类别不平衡的背景下，负例的数目众多致使FPR的增长不明显，导致ROC曲线呈现一个过分乐观的效果估计。\n\n\n\n\n\n\n\n\n\nTPR考虑的是第一行，实际都是正例，FPR考虑的是第二行，实际都是负例。因此，在正负样本数量不均衡的时候，比如负样本的数量增加到原来的10倍，那TPR不受影响，FPR的各项也是成比例的增加，并不会有太大的变化。因此，在样本不均衡的情况下，同样ROC曲线仍然能较好地评价分类器的性能，这是ROC的一个优良特性，也是为什么一般ROC曲线使用更多的原因。\n\n\n\n\n\n\n\n\n\nROC曲线和PR曲线的区别 1.当正负原本数量比较均衡的时候，两者差别不大，当数量比例失衡时，ROC曲线不如PR曲线能更好的反映出分类器的真实性能，通过PR曲线的查准率指标反映出来。 2.PR曲线比ROC曲线更关注正样本，ROC曲线兼顾了两者。\n\n\n\n\n\n\n\n\n\n对于正负样本数量极不均衡的情况，只通过准确率（Accuracy）往往难以反映系统的真实性能。 举例说明，对于一个地震预测系统，假设所有样本中，1000天中有1天发生地震，记：0：不地震，1：地震，分类器不假思索的将所有样本分类为0，即可得到99.99%的准确率，当地震真正来临时，并不能成功预测，这种结果是我们不能接受的。\n1.2.2 一句话说明AUC的本质和计算规则\n本质：一个正例，一个负例，预测为正例的概率值大于预测为负的概率值的可能性；\n计算规则：ROC曲线下的面积：\n1.2.3 AUC高可以理解为精确率高吗？\n不可以，精确率是基于某个阈值进行计算的，AUC是基于所有可能的阈值进行计算的，具有更高的健壮性。AUC不关注某个阈值下的表现如何，综合所有阈值的预测性能，所以精确率高，AUC不一定大，反之亦然。\n1.2.4 同时增大或减小样本的预测概率，会对AUC产生什么影响？\n没影响，因为是同时增大，AUC本质是预测为正例的概率值大于预测为负的概率值的可能性。\n2. CNN基础知识\n2.1 卷积\n\n\n\n\n\n\n\n\n\n所谓两个函数的卷积，本质上就是先将一个函数翻转，然后进行滑动叠加。\n对卷积的意义的理解： 1. 从“积”的过程可以看到，我们得到的叠加值，是个全局的概念。以信号分析为例，卷积的结果是不仅跟当前时刻输入信号的响应值有关，也跟过去所有时刻输入信号的响应都有关系，考虑了对过去的所有输入的效果的累积。在图像处理的中，卷积处理的结果，其实就是把每个像素周边的，甚至是整个图像的像素都考虑进来，对当前像素进行某种加权处理。所以说，“积”是全局概念，或者说是一种“混合”，把两个函数在时间或者空间上进行混合。 2. 那为什么要进行“卷”？直接相乘不好吗？我的理解，进行“卷”（翻转）的目的其实是施加一种约束，它指定了在“积”的时候以什么为参照。在信号分析的场景，它指定了在哪个特定时间点的前后进行“积”，在空间分析的场景，它指定了在哪个位置的周边进行累积处理。\n卷积时间复杂度 =  k是kenerl的尺寸，m是输出的下一层的feature map的尺寸.\n底层实现: 主要用的就是矩阵相乘，也就是GEMM。先用一个叫im2col的函数把图像转成矩阵，然后和卷积核做矩阵乘法。参考\n感受野计算:\n\n\n增大感受野的方法:\npooling 池化:\n池化主要任务是对数据降维，减小网络参数，提升网络的计算效率，同时，池化也是增加感受野的方法之一，但在增加感受野的同时，伴随着分辨率的降低，图像细节损失\ndilated conv空洞卷积:\n空洞卷积的出现为了解决pooling层增大感受野之后进行上采样（增加图像的分辨率）过程中，图像信息缺失问题。\n1x3、3x1代替3x3卷积:\n\n\n\n\n\n\n\n\n\n各向同性的是可以的，参考高斯滤波，但是深度学习里面的卷积核不太可能各向同性\nif a 2D kernel has a rank of one, the operation can be equivalently transformed into a series of 1D convolutions\nBut\n\nHowever, as the learned kernels in deep networks have distributed eigenvalues, their intrinsic rank is higher than one in practice, thus applying the transformation directly to the kernels results in signifificant information loss。\nHowever, the authors found out that such replacement is not equivalent as it did not work well on the low-level layers\n\n参考论文\n加速:\nWinograd快速卷积算法\n2.2 反卷积/转置卷积/sub-pixel\n解释\n转置卷积的计算:\n输入层：\n超参数： * 过滤器个数：k\n\n过滤器中卷积核维度：w*h\n滑动步长（Stride）：s\n填充值（Padding）：p\n输出层： （为简化计算，设，则记\n\n其中输出层和输入层之间的参数关系分为两种情况：\n情况一：(in-1) * s -2p + k = out\n\n情况二：(in-1) * s -2p + k != out\n\n棋盘效应\n知乎：Redflashing 在使用转置卷积时观察到一个棘手的现象（尤其是深色部分常出现）就是\"棋盘格子状伪影\"，被命名为棋盘效应（Checkboard artifacts）。\n\n棋盘效应是由于转置卷积的“不均匀重叠”（Uneven overlap）的结果。使图像中某个部位的颜色比其他部位更深。尤其是当卷积核（Kernel）的大小不能被步长（Stride）整除时，反卷积就会不均匀重叠。虽然原则上网络可以通过训练调整权重来避免这种情况，但在实践中神经网络很难完全避免这种不均匀重叠。 ​下面通过一个详细的例子，更为直观展示棋盘效应。下图的顶部部分是输入层，底部部分为转置卷积输出结果。结果转置卷积操作，小尺寸的输入映射到较大尺寸的输出（体现在长和宽维度）。 在（a）中，步长为1，卷积核为2*2。如红色部分所展示，输入第一个像素映射到输出上第一个和第二个像素。而正如绿色部分，输入的第二个像素映射到输出上的第二个和第三个像素。则输出上的第二个像素从输入上的第一个和第二个像素接收信息。总而言之，输出中间部分的像素从输入中接收的信息存在重叠区域。在示例（b）中的卷积核大小增加到3时，输出所接收到的大多数信息的中心部分将收缩。但这并不是最大的问题，因为重叠仍然是均匀的。\n\n如果将步幅改为2，在卷积核大小为2的示例中，输出上的所有像素从输入中接收相同数量的信息。由下图（a）可见，此时描以转置卷积的重叠。若将卷积核大小改为4（下图（b）），则均匀重叠区域将收缩，与此同时因为重叠是均匀的，故仍然为有效输出。但如果将卷积核大小改为3，步长为2（下图（c）），以及将卷积核大小改为5，步长为2（下图（d）），问题就出现了，对于这两种情况输出上的每个像素接收的信息量与相邻像素不同。在输出上找不到连续且均匀重叠区域。\n\n在二维情况下棋盘效应更为严重，下图直观地展示了在二维空间内的棋盘效应。\n\n如何避免棋盘效应\n\n采取可以被步长整除的卷积核长度 该方法较好地应对了棋盘效应问题，但仍然不够圆满，因为一旦我们的卷积核学习不均匀，依旧会产生棋盘效应（如下图所示）\n\n\n\n插值 可以直接进行插值Resize操作，然后再进行卷积操作。该方式在超分辨率的相关论文中比较常见。例如我们可以用常见的图形学中常用的双线性插值和近邻插值以及样条插值来进行上采样。\n\n\n2.3 空洞卷积/扩张卷积/膨胀卷积\n知乎：代辰 空洞卷积（Atrous Convolution），向卷积层引入了一个称为“扩张率(dilation rate)”的新参数，该参数定义了卷积核处理数据时各值的间距。换句话说，相比原来的标准卷积，扩张卷积多了一个超参数称之为dilation rate（扩张率），指的是kernel各点之间的间隔数量，正常的卷积核的扩张率为1。\n\n上图是一个扩张率为2，尺寸为 3×3 的空洞卷积，感受野与5×5的卷积核相同，而且仅需要9个参数。在相同的计算条件下，空洞卷积提供了更大的感受野。当网络层需要较大的感受野，但计算资源有限而无法提高卷积核数量或大小时，可以考虑空洞卷积。\n\n\n\n\n\n\n\n\n\n空洞卷积主要有两方面的作用： 1.扩大感受野在深度学习网络中为了增加感受野且降低计算量，总要进行降采样(pooling或conv)，这样虽然可以增加感受野，但空间分辨率降低了。为了能不丢失分辨率，且仍然扩大感受野，可以使用空洞卷积。这在检测，分割任务中十分有用。一方面感受野大了可以检测分割大目标，另一方面分辨率高了可以精确定位目标。 2.捕获多尺度上下文信息空洞卷积有一个参数可以设置dilation rate，具体含义就是在卷积核中填充dilation rate-1个0，因此，当设置不同dilation rate时，感受野就会不一样，也即获取了多尺度信息。多尺度信息在视觉任务中相当重要啊。DeepLab中的ASPP模块 从这里可以看出，空洞卷积可以任意扩大感受野，且不需要引入额外参数，但如果把分辨率增加了，算法整体计算量肯定会增加。\n\n\n\n\n\n\n\n\n\n空洞卷积虽然有这么多优点，但在实际中不好优化，速度会大大折扣。\n空洞卷积存在的问题:\n空洞卷积是存在理论问题的，论文中称为gridding，其实就是网格效应/棋盘问题。因为空洞卷积得到的某一层的结果中，邻近的像素是从相互独立的子集中卷积得到的，相互之间缺少依赖。\n\n局部信息丢失： 由于空洞卷积的计算方式类似于棋盘格式，某一层得到的卷积结果，来自上一层的独立的集合，没有相互依赖，因此该层的卷积结果之间没有相关性，即局部信息丢失。\n远距离获取的信息没有相关性： 由于空洞卷积稀疏的采样输入信号，使得远距离卷积得到的信息之间没有相关性，影响分类结果。\n\n解决方法\n\nRemoving max pooling：由于maxpool会引入更高频的激活，这样的激活会随着卷积层往后传播，使得grid问题更明显。\nAdding layers：在网络最后增加更小空洞率的残参block, 有点类似于HDC。\nRemoving residual connections：去掉残参连接，防止之前层的高频信号往后传播。\n\n\n\n\n\n\n\n\n\n\n总结：简单来说，就是空洞卷积虽然在参数不变的情况下保证了更大的感受野，但是对于一些很小的物体，本身就不要那么大的感受野来说，这是不友好的。\n空洞卷积感受野如何计算\n\n\n\n\n\n\n\n\n\n和标准卷积是一致的 空洞卷积实际卷积核大小：K=k+(k-1)(r-1)，k为原始卷积核大小，r为空洞卷积参数空洞率；\n以三个r=2的3*3/s1空洞卷积为例计算感受野：\nK=k+(k-1)(r-1)=3+2*1=5\nR=1+4+4+4=13\n\n\n\n\n\n\n\n\n\n相当于三个kernel size=5的卷积核串联，如果步长设置为1的话，每两层的尺寸关系为X0-5+1=X1,也就是X0-4=X1,因此可以得到通项公式，X0=Xi+i*4。因此对于三个卷积核有四个层，对应i=3，X3=1,所以是13\n2.4 depthwise卷积\n\n\n常规卷积(来源见水印)\n\nDepthwise Convolution的一个卷积核负责一个通道，一个通道只被一个卷积核卷积。常规卷积每个卷积核是同时操作输入图片的每个通道。\n同样是对于一张5×5像素、三通道彩色输入图片（shape为5×5×3），Depthwise Convolution首先经过第一次卷积运算，不同于上面的常规卷积，DW完全是在二维平面内进行。卷积核的数量与上一层的通道数相同（通道和卷积核一一对应）。所以一个三通道的图像经过运算后生成了3个Feature map(如果有same padding则尺寸与输入层相同为5×5)，如下图所示。\n\n\ndepthwise卷积(来源见水印)\n\n其中一个Filter只包含一个大小为3×3的Kernel，卷积部分的参数个数计算如下： N_depthwise = 3 × 3 × 3 = 27\n\n\n\n\n\n\n\n\n\nDepthwise Convolution完成后的Feature map数量与输入层的通道数相同，无法扩展Feature map。而且这种运算对输入层的每个通道独立进行卷积运算，没有有效的利用不同通道在相同空间位置上的feature信息。因此需要Pointwise Convolution来将这些Feature map进行组合生成新的Feature map。\n2.5 pointwise卷积\nPointwise Convolution的运算与常规卷积运算非常相似，它的卷积核的尺寸为 1×1×M，M为上一层的通道数。所以这里的卷积运算会将上一步的map在深度方向上进行加权组合，生成新的Feature map。有几个卷积核就有几个输出Feature map。如下图所示。\n\n\npointwise卷积(来源见水印)\n\n由于采用的是1×1卷积的方式，此步中卷积涉及到的参数个数可以计算为： N_pointwise = 1 × 1 × 3 × 4 = 12\n经过Pointwise Convolution之后，同样输出了4张Feature map，与常规卷积的输出维度相同。\n\n\n\n\n\n\n\n\n\n参数对比 回顾一下，常规卷积的参数个数为： N_std = 4 × 3 × 3 × 3 = 108 Separable Convolution的参数由两部分相加得到： N_depthwise = 3 × 3 × 3 = 27 N_pointwise = 1 × 1 × 3 × 4 = 12 N_separable = N_depthwise + N_pointwise = 39 相同的输入，同样是得到4张Feature map，Separable Convolution的参数个数是常规卷积的约1/3。因此，在参数量相同的前提下，采用Separable Convolution的神经网络层数可以做的更深。\n2.6 conv，depthwise和pointwise 计算量\n假设输入特征图大小为  ×  × M， 输出特征图大小为  ×  × N， 卷积核大小为  × \n参数量：     标准卷积参数量为：          ×  × M × N\n    深度可分离卷积参数量为(depthwise+pointwise)：          ×  × M + M × N\n计算量：     标准卷积计算量为：          ×  × M ×  ×  × N\n    深度可分离卷积计算量为：          ×  × M ×  ×  + M ×  ×  × N\n\n可见参数数量和乘加操作的运算量均下降为原来的 我们通常所使用的是3×3的卷积核，也就是会下降到原来的九分之一到八分之一。\n2.7 分组卷积\n\n第一张图代表标准卷积操作。若输入特征图尺寸为 ，卷积核尺寸为 ，输出特征图尺寸为，标准卷积层的参数量为：。（一个滤波器在输入特征图 大小的区域内操作，输出结果为1个数值，所以需要 个滤波器。）\n第二张图代表分组卷积操作。将输入特征图按照通道数分成g组，则每组输入特征图的尺寸为 ，对应的卷积核尺寸为，每组输出特征图尺寸为。将g组结果拼接(concat)，得到最终尺寸为 的输出特征图。分组卷积层的参数量为 。\n深入思考一下，常规卷积输出的特征图上，每一个点是由输入特征图个点计算得到的；而分组卷积输出的特征图上，每一个点是由输入特征图 个点计算得到的。自然，分组卷积的参数量是标准卷积的 。\n2.8 PixelShuffle\nPixelShuffle(像素重组)的主要功能是将低分辨的特征图，通过卷积和多通道间的重组得到高分辨率的特征图。这一方法最初是为了解决图像超分辨率问题而提出的，这种称为Sub-Pixel Convolutional Neural Network的方法成为了上采样的有效手段。\n\npixelshuffle的主要功能就是将这个通道的特征图组合为新的w*r, h*r的上采样结果。具体来说，就是将原来一个低分辨的像素划分为r各更小的格子，利用r个特征图对应位置的值按照一定的规则来填充这些小格子。按照同样的规则将每个低分辨像素划分出的小格子填满就完成了重组过程。在这一过程中模型可以调整r*r个shuffle通道权重不断优化生成的结果。 主要实现了这样的功能：N*(C*r*r)*W*H----&gt;&gt;N*C*(H*r)*(W*r)\n\n\n\n\n\n\n\n\n\n在图像超分辨和图像增强的算法中需要对特征图进行上下采样的过程，可以解决插值和解卷积的一些人工痕迹问题。\n2.9 Pooling\n知乎\nPooling 的本质，其实是采样。Pooling 对于输入的 Feature Map，选择某种方式对其进行压缩。如Max-Pooling,还有Mean-Pooling，Stochastic-Pooling 等。它们的具体实现如名称所示，具体选择哪一个则取决于具体的任务。\nPooling 的意义，主要有两点：\n\n减少参数。通过对 Feature Map 降维，有效减少后续层需要的参数\nTranslation Invariance(平移不变性)。它表示对于 Input，当其中像素在邻域发生微小位移时，Pooling Layer 的输出是不变的。这就使网络的鲁棒性增强了，有一定抗扰动的作用\n\n2.10 maxpooling/averagepooling怎么传递导数？(反池化)\n参考\n池化层在反向传播时，它是不可导的，因为它是对特征图进行下采样会导致特征图变小，比如一个2x2的池化，在L+1层输出的特征图是16个神经元，那么对应L层就会有64个神经元，两层之间经过池化操作后，特征图尺寸改变，无法一一对应，这使得梯度无法按位传播。那么如何解决池化层不可导但却要参与反向传播呢？\n在反向传播时，梯度是按位传播的，那么，一个解决方法，就是如何构造按位的问题，但一定要遵守传播梯度总和保持不变的原则。\n对于平均池化，其前向传播是取某特征区域的平均值进行输出，这个区域的每一个神经元都是有参与前向传播了的，因此，在反向传播时，框架需要将梯度平均分配给每一个神经元再进行反向传播，相关的操作示意如下图所示。\n\n对于最大池化，其前向传播是取某特征区域的最大值进行输出，这个区域仅有最大值神经元参与了前向传播，因此，在反向传播时，框架仅需要将该区域的梯度直接分配到最大值神经元即可，其他神经元的梯度被分配为0且是被舍弃不参与反向传播的，但如何确认最大值神经元，这个还得框架在进行前向传播时记录下最大值神经元的Max ID位置，这是最大池化与平均池化差异的地方，相关的操作示意如下图所示。\n\n其中，上述表格中，前向传播时，每个单元格表示特征图神经元值，而在反向传播时，每个单元格表示的是分配给对应神经元的梯度值。\n2.11 BN IN LN GN ADAIN\n\nBN计算均值和标准差时，固定channel(在一个channel内)，对HW和batch作平均；\nLN计算均值和标准差时，固定batch(在一个batch内)，对HW和channel作平均；\nIN计算均值和标准差时，同时固定channel和batch(在一个batch内中的一个channel内)，对HW作平均；\nGN计算均值和标准差时，固定batch且对channel作分组(在一个batch内对channel作分组)，在分组内对HW作平均。\n2.11.1 BN\n在网络层数加深的时候，会影响我们每一层输出的数据分布。而之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（以Sigmoid函数为例），所以这导致后向传播时低层神经网络的梯度很小甚至消失，这是训练深层神经网络收敛越来越慢的本质原因， 而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，所以就可以让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。\nBN核心公式： \n\nBN的具体操作为：先计算B的均值和方差，之后将B集合的均值、方差变换为0、1，最后将B中每个元素乘以γ再加β，输出。 γ、β是可训练参数，参与整个网络的BP；\n归一化的目的：将数据规整到统一区间，减少数据的发散程度，降低网络的学习难度。BN的精髓在于归一之后，使用γ、β作为还原参数，在一定程度上保留原数据的分布。\n\n训练与推理时BN中的均值、方差分别是什么？ 训练时，均值、方差分别是该批次内数据相应维度的均值与方差； 推理时，均值、方差是基于所有批次的期望计算所得，公式如下：  其中，E(x)表示x的期望.\n代码实现技巧：\n\n在代码实现中有一个技巧，如果训练几百万个Batch，那么是不是要将其均值方差全部储存，最后再计算推理时所用的均值和方差？这样显然太过笨拙，占用内存随着训练次数不断上升。为了避免该问题，实际代码（如tf）使用了滑动平均，储存固定个数Batch的均值和方差，不断迭代更新推理时需要的E(x)与Var(x)。 注意到代码中： 1. beta、gamma在训练状态下，是可训练参数，在推理状态下，直接加载训练好的数值。 2. moving_mean、moving_var在训练、推理中都是不可训练参数，只根据滑动平均计算公式更新数值，不会随着网络的训练BP而改变数值；在推理时，直接加载储存计算好的滑动平均之后的数值，作为推理时的均值和方差。\nBN层的本质: 平滑了优化空间\n对比白化操作:\n白化是机器学习里面 常用的一种规范化数据分布的方法，主要是PCA白化与ZCA白化。白化是对输入数据分布进行变换，进而达到以下两个目的： （1）使得输入特征分布具有相同的均值与方差。 （2）去除特征之间的相关性。 但是白化也存在两个问题： （1）计算成本太大，像PCA白化还要算协方差矩阵等。 （2）白化过程由于改变了网络每一层的分布，因而改变了网络层中本身数据的表达能力。底层网络学习到的参数信息会被白化操作丢失掉。而BN正是针对了白化的这两个缺点进行了改善\nBN的缺陷:\n\n需要较大的 batchsize 才能合理估训练数据的均值和方差(横向计算)，这导致内存很可能不够用;\n很难应用在训练数据长度不同的 RNN 模型上.\n\nBN和卷积层融合:\n\n2.11.2 IN\nIN是针对于不同的batch, 不同的chennel进行归一化。还是把图像的尺寸表示为[N, C, H, W]的话，IN则是针对于[H,W]进行归一化。这种方式通常会用在风格迁移的训练中。\n\n\n\n\n\n\n\n\n\nIN 操作也在单个样本内部进行，不依赖 batch。\n2.11.3 LN\nLN的方法是对于每一个sample中的多个feature(也就是channel)进行归一化操作。把图像的尺寸表示为[N, C, H, W]的话，LN则是对于[C,H,W]进行归一化。相对于BN中所表示的同一个feature在不同的batch之间拥有同样的均值和方差。LN中所表示的则是在同一个sample中，不同的feature上有着相同的均值和方差。\n与BN相比，LN也不依赖于mini-batch size的大小。这种操作通常运用在RNN的网络中。\n\n\n\n\n\n\n\n\n\nLayer Normalization (LN) 的一个优势是不需要批训练，在单条数据内部就能归一化。LN 对每个样本的 C、H、W 维度上的数据求均值和标准差，保留 N 维度。\n2.11.4 GN\nGN是介乎于instance normal 和 layer normal 之间的一种归一化方式。也就是说当我们把所有的channel都放到同一个group中的时候就变成了layer normal， 如果我们把每个channel都归为一个不同的group，则变成了instance normal.\nGN同样可以针对于mini batch size较小的情况。因为它有不受batch size的约束。\n可以看到与BN不同，LN/IN和GN都没有对batch作平均，所以当batch变化时，网络的错误率不会有明显变化。但论文的实验显示：LN和IN 在时间序列模型(RNN/LSTM)和生成模型(GAN)上有很好的效果，而GN在视觉模型上表现更好。\n\n\n\n\n\n\n\n\n\nGroup Normalization (GN) 适用于占用显存比较大的任务，例如图像分割,对这类任务，可能 batchsize 只能是个位数，再大显存就不够用了。而当 batchsize 是个位数时，BN 的表现很差，因为没办法通过几个样本的数据量，来近似总体的均值和标准差。GN 也是独立于 batch 的，它是 LN 和 IN 的折中\n2.11.5 映射参数γ和β的区别\n对于 BN，IN，GN， 其γ和β都是维度等于通道数 C 的向量。而对于 LN，其γ和β都是维度等于 normalized_shape 的矩阵。 最后，BN和IN 可以设置参数：momentum 和 track_running_stats来获得在全局数据上更准确的 running mean 和 running std。 而 LN 和 GN 只能计算当前 batch 内数据的真实均值和标准差。\n2.11.6 dropout\n在训练时，每个神经单元以概率𝑝被保留(Dropout丢弃率为1−𝑝)； 在预测阶段（测试阶段），每个神经单元都是存在的，权重参数𝑤要乘以𝑝，输出是：𝑝𝑤。\n预测阶段需要乘上𝑝的原因： 　　前一层隐藏层的一个神经元在𝑑𝑟𝑜𝑝𝑜𝑢𝑡之前的输出是𝑥，训练时𝑑𝑟𝑜𝑝𝑜𝑢𝑡之后的期望值是𝐸=𝑝𝑥+(1−𝑝)0˙; 在预测阶段该层神经元总是激活，为了保持同样的输出期望值并使下一层也得到同样的结果，需要调整𝑥−&gt;𝑝𝑥. 其中𝑝是Bernoulli分布（0-1分布）中值为1的概率。\n为什么说Dropout可以解决过拟合？ 1. 取平均的作用： 先回到标准的模型即没有dropout，我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。例如3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。 2. 减少神经元之间复杂的共适应关系： 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。 3. Dropout类似于性别在生物进化中的角色：物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝。\ndropout会改变数据分布, 导致训练测试样本分布不一致，怎么解决\n实践发现droput之后改变了数据的标准差（令标准差变大，若数据均值非0时，甚至均值也会产生改变）。 如果同时又使用了BN归一化，由于BN在训练时保存了训练集的均值与标准差。dropout影响了所保存的均值与标准差的准确性（不能适应未来预测数据的需要），那么将影响网络的准确性。\ndropout 有两种实现方式，Vanilla Dropout 和 Inverted Dropout。 主流框架使用Inverted Dropout： 知乎：神经网络Dropout层中为什么dropout后还需要进行rescale？\n\n当模型使用了dropout layer，训练的时候只有占比为p的隐藏层单元参与训练，那么在预测的时候，如果所有的隐藏层单元都需要参与进来，则得到的结果相比训练时平均要大，为了避免这种情况，就需要测试的时候将输出结果乘以p使下一层的输入规模保持不变。而利用inverted dropout，我们可以在训练的时候直接将dropout后留下的权重扩大倍，这样就可以使结果的scale保持不变，而在预测的时候也不用做额外的操作了，更方便一些。\n\n\n\n\n\n\n\n\n\n每个神经元的丢弃概率p遵循概率p的伯努利分布;伯努利实验是 “Single trial with only two outcomes\",二项实验是 “Repeating a Bernoulli experiment for n times”. 二项分布就是，将伯努利实验重复n次后的概率分布。\n2.11.7 CNN的平移不变性\n出处\nCNN的平移不变性。简单来说，平移不变性（translation invariant）指的是CNN对于同一张图及其平移后的版本，都能输出同样的结果。这对于图像分类（image classification）问题来说肯定是最理想的，因为对于一个物体的平移并不应该改变它的类别。而对于其它问题，比如物体检测（detection）、物体分割（segmentation）来说，这个性质则不应该有，原因是当输入发生平移时，输出也应该相应地进行平移。这种性质又称为平移等价性（translation equivalence）。\n3. 激活函数相关\n\n\n\n\n\n\n\n\n\n无论是全连接层还是卷积层的计算都是简单的乘加运算，也称之为线性运算，这样我们可以作为线性层。但是，线性层的特征表达能力是有限的，所以在这些线性计算之后又引入了非线性计算，增强模型特征的表达能力，也就是大家熟知的激活层，也称为非线性层。 参考\n激活函数可以分为两大类 ： 饱和激活函数： sigmoid、 tanh 非饱和激活函数: ReLU 、Leaky Relu 、ELU【指数线性单元】、PReLU【参数化的ReLU 】、RReLU【随机ReLU】 相对于饱和激活函数，使用“非饱和激活函数”的优势在于两点： 1. 首先，“非饱和激活函数”能解决深度神经网络【层数非常多！！】的“梯度消失”问题，浅层网络【三五层那种】才用sigmoid 作为激活函数。 2. 其次，它能加快收敛速度。\n3.1 Sigmoid\nsigmoid函数也称为Logistic函数, sigmod函数的取值范围在（0, 1）之间，可以将网络的输出映射在这一范围，方便分析。\n3.1.1 公式及导数\n\n\n3.1.2 曲线\n\n\n(来源见水印)\n\n3.1.3 特点\n优点： 平滑、易于求导。\n缺点： 1. 激活函数计算量大（在正向传播和反向传播中都包含幂运算和除法）； 2. 反向传播求误差梯度时，求导涉及除法； 3. Sigmoid导数取值范围是[0, 0.25]，由于神经网络反向传播时的“链式反应”，很容易就会出现梯度消失的情况。例如对于一个10层的网络， 根据，第10层的误差相对第一层卷积的参数的梯度将是一个非常小的值，这就是所谓的“梯度消失”。 4. Sigmoid的输出不是0均值（即zero-centered）；这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入，随着网络的加深，会改变数据的原始分布。\n3.2 Softmax\n\n\n\n\n\n\n\n\n\n通过Softmax函数就可以将多分类的输出值转换为范围在[0, 1]和为1的概率分布。\n3.1.1 公式及导数\n 其中  为第i个节点的输出值，C为输出节点的个数，即分类的类别个数。\n用表示softmax:\n\n3.1.2 引入指数形式\n优点: 1. 指数函数曲线呈现递增趋势，最重要的是斜率逐渐增大，也就是说在x轴上一个很小的变化，可以导致y轴上很大的变化。这种函数曲线能够将输出的数值拉开距离。 2. 在深度学习中通常使用反向传播求解梯度进而使用梯度下降进行参数更新的过程，而指数函数在求导的时候比较方便。如: \n缺点(上溢和下溢问题): 指数函数的曲线斜率逐渐增大虽然能够将输出值拉开距离，但是也带来了缺点，当值非常大的话，计算得到的数值也会变的非常大，数值可能会溢出。同理当输入为负数且绝对值也很大的时候，会分子、分母会变得极小，有可能四舍五入为0，导致下溢出。\n\n\n\n\n\n\n\n\n\n有一个方法计算上溢的情况，也就是每个变量都减去最大值，然后做softmax；还有一种方法是直接用log softmax\n\n3.1.3 Sigmoid 和 Softmax 区别\nsigmoid将一个real value映射到（0,1）的区间，用来做二分类。而 softmax 把一个 k 维的real value向量（a1,a2,a3,a4….）映射成一个（b1,b2,b3,b4….）其中 bi 是一个 0～1 的常数，输出神经元之和为 1.0，所以相当于概率值，然后可以根据 bi 的概率大小来进行多分类的任务。\n二分类问题时 sigmoid 和 softmax 是一样的，求的都是 cross entropy loss，而 softmax 可以用于多分类问题多个logistic回归通过叠加也同样可以实现多分类的效果，但是 softmax回归进行的多分类，类与类之间是互斥的，即一个输入只能被归为一类；多个logistic回归进行多分类，输出的类别并不是互斥的，即\"苹果\"这个词语既属于\"水果\"类也属于\"3C\"类别。\n\n\n\n\n\n\n\n\n\n使用softmax回归或者多个logistics回归解决多分类问题时：使用哪一个主要根据类别之间是否互斥。对于选择softmax分类器还是k个logistics分类器，取决于所有类别之间是否互斥。所有类别之间明显互斥用softmax；所有类别之间不互斥有交叉的情况下最好用k个logistics分类器。\n\n3.1.4 为什么soft?\n知乎: hardmax最大的特点就是只选出其中一个最大的值，即非黑即白。但是往往在实际中这种方式是不合情理的，比如对于文本分类来说，一篇文章或多或少包含着各种主题信息，我们更期望得到文章对于每个可能的文本类别的概率值（置信度），可以简单理解成属于对应类别的可信度。所以此时用到了soft的概念，Softmax的含义就在于不再唯一的确定某一个最大值，而是为每个输出分类的结果都赋予一个概率值，表示属于每个类别的可能性。\n3.3 ReLU, ReLU6\n\n\n\n\n\n\n\n\n\nrelu和sigmoid比，1.不会梯度弥散; 2.稀疏参数; 3计算简单\nRelu(Rectified Linear Unit)——修正线性单元函数：该函数形式比较简单， 公式：relu=max(0, x)\n\n\n(来源见水印)\n\n从上图可知，ReLU的有效导数是常数1，解决了深层网络中出现的梯度消失问题，也就使得深层网络可训练。同时ReLU又是非线性函数，所谓非线性，就是一阶导数不为常数；对ReLU求导，在输入值分别为正和为负的情况下，导数是不同的，即ReLU的导数不是常数，所以ReLU是非线性的（只是不同于Sigmoid和tanh，relu的非线性不是光滑的）。\nReLU在x&gt;0下，导数为常数1的特点： 导数为常数1的好处就是在“链式反应”中不会出现梯度消失，但梯度下降的强度就完全取决于权值的乘积，这样就可能会出现梯度爆炸问题。解决这类问题：一是控制权值，让它们在（0，1）范围内；二是做梯度裁剪，控制梯度下降强度，如ReLU(x)=min(6, max(0,x))\nReLU在x&lt;0下，输出置为0的特点：\n描述该特征前，需要明确深度学习的目标：深度学习是根据大批量样本数据，从错综复杂的数据关系中，找到关键信息（关键特征）。换句话说，就是把密集矩阵转化为稀疏矩阵，保留数据的关键信息，去除噪音，这样的模型就有了鲁棒性。ReLU将x&lt;0的输出置为0，就是一个去噪音，稀疏矩阵的过程。而且在训练过程中，这种稀疏性是动态调节的，网络会自动调整稀疏比例，保证矩阵有最优的有效特征。 但是ReLU 强制将x&lt;0部分的输出置为0（置为0就是屏蔽该特征），可能会导致模型无法学习到有效特征，所以如果学习率设置的太大，就可能会导致网络的大部分神经元处于‘dead’状态，所以使用ReLU的网络，学习率不能设置太大。\nReLU作为激活函数的特点： 1. 相比Sigmoid和tanh，ReLU摒弃了复杂的计算，提高了运算速度。 2. 解决了梯度消失问题，收敛速度快于Sigmoid和tanh函数，但要防范ReLU的梯度爆炸 3. 容易得到更好的模型，但也要防止训练中出现模型‘Dead’情况。\nrelu小于0会导致梯度消失，怎么办？ &gt; 在小于的时候，激活函数梯度为零，梯度消失，神经元不更新，变成了死亡节点。出现这个原因可能是因为学习率太大，导致w更新巨大，使得输入数据在经过这个神经元的时候，输出值小于0，从而经过激活函数的时候为0，从此不再更新。所以relu为激活函数，学习率不能太大.\nRelu6:\nReLU6(x)=min(max(0,x),6) \nMobileNetV1 中使用 ReLU6。ReLU6 就是普通的ReLU，但是限制最大输出为6。这是为了在移动端设备 float16/int8 的低精度的时候也能 有很好的数值分辨率。如果对 ReLU 的激活范围不加限制，输出范围为 0 到正无穷，如果激活值非常大，分布在一个很大的范围内，则低精度的 float16/int8 无法很好地精确描述如此大范围的数值，带来精度损失。\nrelu6的好处： 1. 可以让模型更早地学到稀疏特征。并没有可信的理论推导，作者试验出来的结果是这样的。注意，relu6只是比relu更早的学习到稀疏特征，而不是说relu学不到稀疏特征。 2. 可以防止数值爆炸。 3. 增强浮点数的小数位表达能力。因为整数位最大是6，所以只占3个bit，其他bit全部用来表达小数位。\n3.4 Leaky ReLU, PReLU（Parametric Relu）, RReLU（Random ReLU）\n为了防止模型的‘Dead’情况，后人将x&lt;0部分并没有直接置为0，而是给了一个很小的负数梯度值α。\nLeaky ReLU中的α为常数，一般设置 0.01。这个函数通常比 Relu 激活函数效果要好，但是效果不是很稳定，所以在实际中 Leaky ReLu 使用的并不多。\nPRelu（参数化修正线性单元）中的α作为一个可学习的参数，会在训练的过程中进行更新。\nRReLU（随机纠正线性单元）也是Leaky ReLU的一个变体。在RReLU中，负值的斜率在训练中是随机的，在之后的测试中就变成了固定的了。RReLU的亮点在于，在训练环节中，是从一个均匀的分布U(I,u)中随机抽取的数值。\n\n\n(来源见水印)\n\n3.5 tanh\ntanh为双曲正切函数。tanh和 sigmoid 相似，都属于饱和激活函数，区别在于输出值范围由 (0,1) 变为了 (-1,1)，可以把 tanh 函数看做是 sigmoid 向下平移和拉伸后的结果。\n3.4.1 公式及导数\n 公式1  公式2  公式3\n从公式2中，可以更加清晰看出tanh与sigmoid函数的关系（平移+拉伸）\n3.4.2 曲线\n\n\n(来源见水印)\n\n3.4.3 特点\n相比Sigmoid函数， 1. tanh的输出范围时(-1, 1)，解决了Sigmoid函数的不是zero-centered输出问题； 2. 幂运算的问题仍然存在； 3. tanh导数范围在(0, 1)之间，相比sigmoid的(0, 0.25)，梯度消失（gradient vanishing）问题会得到缓解，但仍然还会存在。\n3.6 softplus\nSoftplus函数是Logistic-Sigmoid函数原函数,  加了1是为了保证非负性。Softplus可以看作是平滑版的relu。红色的即为ReLU。\n\n3.7 Swish, hard-Swish\nSwish:β是个常数或者可以训练的参数。 和 ReLU 一样，Swish 无上界有下界。与 ReLU 不同的是，Swish 是平滑且非单调的函数。事实上，Swish 的非单调特性把它与大多数常见的激活函数区别开来。\nf(x) = x*sigmoid(βx) f'(x) = sigmoid(βx) + x(β*sigmoid(βx))\n\n\n\n\n\n\n\n\n\n\nSwish函数可以看做是介于线性函数与ReLU函数之间的平滑函数.\nhard-Swish(MobileNetV3): 虽然这种Swish非线性提高了精度，但是在嵌入式环境中，他的成本是非零的，因为在移动设备上计算sigmoid函数代价要大得多。 因此作者使用hard-Swish和hard-Sigmoid替换了ReLU6和SE-block中的Sigmoid层，但是只是在网络的后半段才将ReLU6替换为h-Swish，因为作者发现Swish函数只有在更深的网络层使用才能体现其优势。 首先是肯定了Swish的重要性，然后指出在量化模式下，Sigmoid函数比ReLU6的计算代价大的多，所以才有了这个ReLU6版本的h-Swish。\n\n\n\n\n\n\n\n\n\n\n\nwhy hard？relu6有上界。如果对relu6再除6再向左平移三个单位，就会得到一条类似于sigmoid函数 \n3.8 Mish\nMish: y=*tanh(ln(1+exp(x)))\nMish和Swish中参数=1的曲线对比：（第一张是原始函数，第二张是导数）\n\n\n\n\n\n\n\n\n\n\n以上无边界(即正值可以达到任何高度)避免了由于封顶而导致的饱和。理论上对负值的轻微允许允许更好的梯度流，而不是像ReLU中那样的硬零边界。最后，可能也是最重要的，目前的想法是，平滑的激活函数允许更好的信息深入神经网络，从而得到更好的准确性和泛化。要区别可能是Mish函数在曲线上几乎所有点上的平滑度.\n3.9 神经网络中的激活函数为什么都是平滑或近似平滑的？\n\n激活值不存在像感知机那样的阶跃现象，比较容易收敛\n很多平滑函数的引入，使得模型有了非线性因素，因此可以识别更加复杂的模式\n平滑函数是可导的，这便于梯度的计算与更新。而对像感知机这样的激活函数，梯度的更新非常困难；如下图所示，两个激活值相同的点，从同一个点到另外一个点，似乎是没有很好的策略来更新梯度\n可导，因此可以指定更加灵活的梯度更新规则，加速模型训练\n\n4. 正则化相关\n4.1 正则化的本质\n知乎回答\n重点记录下： 如何去防止过拟合，首先想到的就是控制N的数量吧，即让N最小化吧，而让N最小化，其实就是让W向量中项的个数最小化，其中。 so,如何求解“让W向量中项的个数最小化\"? 没错，这就是0范数的概念！什么是范数，这里只是给出个0-2范数定义: 0范数，向量中非零元素的个数。 1范数，为绝对值之和。 2范数，就是通常意义上的模。\n\n\n\n\n\n\n\n\n\n求解“让W向量中项的个数最小化”吗？怎么与0范数的定义有点不一样，一句话，向量中0元素，对应的x样本中的项我们是不需要考虑的，可以砍掉。因为0*没有啥意义，说明项没有任何权重\n所以为了防止过拟合，除了需要前面的相加项最小，即最小，我们还需要让最小，所以，为了同时满足两项都最小化，咱们可以求解让和r(d)之和最小，这样不就同时满足两者了吗？如果r(d) 过大，再小也没用；相反r(d)再小，太大也失去了问题的意义。说到这里已经回答了，那就是为什么需要有个r(d)项，为什么r(d)能够防止过拟合原因了。\n\n\n\n\n\n\n\n\n\n1范数和0范数可以实现稀疏，1因具有比L0更好的优化求解特性而被广泛应用。然后L2范数，是下面这么理解的，我就直接查别人给的解释好了，反正简单，就不自己动脑子解释了： L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的正则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这里是有很大的区别的哦；所以大家比起1范数，更钟爱2范数。\n4.2 L1和L2正则化\nL1正则化与L2正则化\n\n \n降低过拟合程度：\n正则化之所以能够降低过拟合的原因在于，正则化是结构风险最小化的一种策略实现。\n给loss function加上正则化项，能使得新得到的优化目标函数h = f+normal，需要在f和normal中做一个权衡（trade-off），如果还像原来只优化f的情况下，那可能得到一组解比较复杂，使得正则项normal比较大，那么h就不是最优的，因此可以看出加正则项能让解更加简单，符合奥卡姆剃刀理论，同时也比较符合在偏差和方差（方差表示模型的复杂度）分析中，通过降低模型复杂度，得到更小的泛化误差，降低过拟合程度。\nL1正则化和L2正则化：\nL1正则化就是在loss function后边所加正则项为L1范数，加上L1范数容易得到稀疏解（0比较多）。 L2正则化就是loss function后边所加正则项为L2范数的平方，加上L2正则相比于L1正则来说，得到的解比较平滑（不是稀疏），但是同样能够保证解中接近于0（但不是等于0，所以相对平滑）的维度比较多，降低模型的复杂度。\nl1 相比于 l2 为什么容易获得稀疏解？\n两种 regularization 能不能把最优的 x 变成 0，取决于原先的费用函数在 0 点处的导数。如果本来导数不为 0，那么施加 L2 regularization 后导数依然不为 0，最优的 x 也不会变成 0。而施加 L1 regularization 时，只要 regularization 项的系数 C 大于原先费用函数在 0 点处的导数的绝对值，x = 0 就会变成一个极小值点。上面只分析了一个参数 x。事实上 L1 regularization 会使得许多参数的最优值变成 0，这样模型就稀疏了。\nL1和L2正则化的区别 1. 从求解效率上看： L2损失函数是可导的，L2正则化也是可导的，所以L2正则化是有解析解的，求解的效率高。但是L1正则化在零点处是不可导的，所以它是没有解析解的，如果问题是一个稀疏问题（简单地说就是很多特征的系数为0），那么可以采用稀疏算法求解，如果问题不是稀疏的，那求解的效率就很低了。 2. 从解的角度看：L2正则化得到不会是稀疏性结果，但是L1正则化可能会得到稀疏结果。 3. L1正则化的优点在于它可以进行特征选择（由于其结果是稀疏的，即很多变量前的系数为0，那么这些系数为0的变量，就是被淘汰的变量）；但是L2正则化则不行。（因为L2正则化的结果不是稀疏的）。所以可以认为L1正则化是一种嵌入式的特征选择方法，其特征选择过程与模型的建立过程融为一体，同时完成。\nL1范数符合拉普拉斯分布，L2范数符合高斯分布，怎么理解？\n5 网络梯度优化下降方法\n5.1 SGD\nSGD一般都指mini-batch gradient descent。 SGD就是每一次迭代计算mini-batch的梯度，然后对参数进行更新，是最常见的优化方法了。即： \n其中，，是梯度 SGD完全依赖于当前batch的梯度，所以\n缺点:\n\n选择合适的learning rate比较困难 - 对所有的参数更新使用同样的learning rate。对于稀疏数据或者特征，有时我们可能想更新快一些对于不经常出现的特征，对于常出现的特征更新慢一些，这时候SGD就不太能满足要求了\nSGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点;\n\n5.2 Momentum\nmomentum是模拟物理里动量的概念，积累之前的动量来替代真正的梯度。公式如下：\n\n特点： 下降初期时，使用上一次参数更新，下降方向一致，乘上较大的μ能够进行很好的加速 下降中后期时，在局部最小值来回震荡的时候，gradient-&gt;0，μ使得更新幅度增大，跳出陷阱 在梯度改变方向的时候，μ能够减少更新 总而言之，momentum项能够在相关方向加速SGD，抑制振荡，从而加快收敛\n5.3 Adam\nAdam(Adaptive Moment Estimation)本质上是带有动量项的RMSprop，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。公式如下：\n\n特点：\n结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点 对内存需求较小 为不同的参数计算不同的自适应学习率 也适用于大多非凸优化 - 适用于大数据集和高维空间\n5.4 经验之谈\n对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值 SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠 如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。 Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。 在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果\n5.5 一些问题\nL2正则=Weight Decay？并不是这样\n使用Adam优化带L2正则的损失并不有效。如果引入L2正则项，在计算梯度的时候会加上对正则项求梯度的结果。那么如果本身比较大的一些权重对应的梯度也会比较大，由于Adam计算步骤中减去项会有除以梯度平方的累积，使得减去项偏小。按常理说，越大的权重应该惩罚越大，但是在Adam并不是这样。而权重衰减对所有的权重都是采用相同的系数进行更新，越大的权重显然惩罚越大。在常见的深度学习库中只提供了L2正则，并没有提供权重衰减的实现。这可能就是导致Adam跑出来的很多效果相对SGD with Momentum偏差的一个原因。\n牛顿法和梯度下降法有什么不同？\n6. 模型相关计算\n输入L* L，卷积核k* k，步长s，padding p，输出尺寸，Flops计算\nL1 = (L-k+2*p)/s + 1 flops = k * k * c1 * c2 * L1 * L1\n网络大小指标：参数量，flops，乘加数\n参考 * FLOPS: 注意全部大写 是floating point of per second的缩写，意指每秒浮点运算次数。可以理解为计算速度，用来衡量硬件的性能。 * FLOPs: 是floating point of operations的缩写，是浮点运算次数，理解为计算量，可以用来衡量算法/模型复杂度。\n卷积层:\n +1表示bias  表示一次卷积操作中的加法运算量，+ 1 表示bias， 上面是乘运算和加运算的总和，将一次乘运算或加运算都视作一次浮点运算。 在计算机视觉论文中，常常将一个'乘-加'组合视为一次浮点运算，英文表述为'Multi-Add'，运算量正好是上面的算法减半，此时的运算量为： \n全连接层: params=(I+1)*O=I*O+O 每一层神经元(O这一层)的权重数为I×O，bias数量为O。 FLOPs=[I+(I-1)+1]*O=2*I*O 第一个I表示乘法运算量， I-1表示加法运算量，+1表示bias， *O表示计算O个神经元的值。\n7. Loss相关\n7.1 Cross Entropy &amp;&amp; MSE\n\n均方差损失函数（MSE） 简单来说，均方误差（MSE）的含义是求一个batch中n个样本的n个输出与期望输出的差的平方的平均值、\nCross-entropy（交叉熵损失函数) 交叉熵是用来评估当前训练得到的概率分布与真实分布的差异情况。 它刻画的是实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近。\n\n\n\n来源\n\n7.1.2 分类为何不用MSE\n分类问题，最后必须是 one hot 形式算出各 label 的概率， 然后通过 argmax 选出最终的分类。 在计算各个 label 概率的时候，用的是 softmax 函数。如果用 MSE 计算 loss， 输出的曲线是波动的，有很多局部的极值点。 即，非凸优化问题 (non-convex) cross entropy 计算 loss，则依旧是一个凸优化问题，用梯度下降求解时，凸优化问题有很好的收敛特性。 &gt; 当梯度很小的时候，应该减小步长（否则容易在最优解附近产生来回震荡），但是如果采用 MSE ，当梯度很小的时候，无法知道是离目标很远还是已经在目标附近了。（离目标很近和离目标很远，其梯度都很小）----知乎\n7.1.3 交叉熵为什么有log项\n不带log，对p的求导处处为1； log(p)，当p接近1时（接近目标），导数小，接近0时，导数大 带log可以使得优化时更偏重于离目标远的那些p，而非同等对待。\n7.1.4 熵、交叉熵、KL散度的关系\n交叉熵其实只是KL散度的一部分\n\n其中p代表label或者叫groundtruth，q代表预测值.在机器学习中，我们需要评估label和predicts之间的差距，使用KL散度刚刚好，即： \n由于KL散度中的前一部分恰巧就是p的熵，p代表label或者叫groundtruth，故−H(p(x))不变，故在优化过程中，只需要关注DKL()的后一部分：交叉熵(H(p, q)) 就可以了。\n7.2 L1 loss, L2 loss, smooth L1 loss\n出处\n\n\n\n\n\n\n\n\n\n对于大多数CNN网络，我们一般是使用L2-loss而不是L1-loss，因为L2-loss的收敛速度要比L1-loss要快得多\n对于边框预测回归问题，通常也可以选择平方损失函数（L2损失），但L2范数的缺点是当存在离群点（outliers)的时候，这些点会占loss的主要组成部分。比如说真实值为1，预测10次，有一次预测值为1000，其余次的预测值为1左右，显然loss值主要由1000主宰。所以FastRCNN采用稍微缓和一点绝对损失函数（smooth L1损失），它是随着误差线性增长，而不是平方增长。\n\n\n\n\n\n\n\n\n\n注意：smooth L1和L1-loss函数的区别在于，L1-loss在0点处导数不唯一，可能影响收敛。smooth L1的解决办法是在0点附近使用平方函数使得它更加平滑。\n\n\n\n\n\n\n\n\n\nsmooth L1 loss并不是为了解决L1-loss在0点处导数不唯一，毕竟直接定义L1 loss在0处的导数为0就行了。更像是，当预测值与标签之差小于1时，smooth l1的导数更小，使得loss收敛的更加稳定，更容易收敛到局部最优，而不会跳出局部最优。\n公式： \n\n\n\n\n来源\n\n\n\n\n\n\n\n\n\n\nsmooth L1 loss让loss对于离群点更加鲁棒，即：相比于L2损失函数，其对离群点、异常值（outlier）不敏感，梯度变化相对更小，训练时不容易跑飞。\ncos距离和l2什么情况下相等?\nL2归一化后欧拉距离的平方与cosine相似度的关系为; &gt; L2归一化就是对向量的每一个值都除以向量的平方和的开方\n\n7.3 focal loss\n来源知乎\nFocal Loss的引入主要是为了解决难易样本数量不平衡（注意，有区别于正负样本数量不平衡）的问题，实际可以使用的范围非常广泛，为了方便解释，还是拿目标检测的应用场景来说明： 单阶段的目标检测器通常会产生高达100k的候选目标，只有极少数是正样本，正负样本数量非常不平衡。我们在计算分类的时候常用的损失——交叉熵的公式如下： \n为了解决正负样本不平衡的问题，我们通常会在交叉熵损失的前面加上一个参数，即： 公式（2） \n但这并不能解决全部问题。根据正、负、难、易，样本一共可以分为以下四类：\n\n\n\n\n难\n易\n\n\n\n\n正\n正难\n正易\n\n\n负\n负难\n负易\n\n\n\n尽管平衡了正负样本，但对难易样本的不平衡没有任何帮助。而实际上，目标检测中大量的候选目标都是易分样本。 这些样本的损失很低，但是由于数量极不平衡，易分样本的数量相对来讲太多，最终主导了总的损失。而本文的作者认为，易分样本（即，置信度高的样本）对模型的提升效果非常小，模型应该主要关注与那些难分样本（这个假设是有问题的，是GHM的主要改进对象）\n这时候，Focal Loss就上场了！ 一个简单的思想：把高置信度(p)样本的损失再降低一些不就好了吗！ 公式（3） \n举个例， γ取2时，如果p=0.968, (1-0.968)^2=0.001，损失衰减了1000倍！\nFocal Loss的最终形式结合了上面的公式（2）. 这很好理解，公式(3)解决了难易样本的不平衡，公式(2)解决了正负样本的不平衡，将公式（2）与（3）结合使用，同时解决正负难易2个问题！\n最终的Focal Loss形式如下：\n\n\n\n\n\n\n\n\n\n\n实验表明α=0.25, γ=2的时候效果最佳。注意在作者的任务中，正样本是属于少数样本，也就是说，本来正样本难以“匹敌”负样本，但经过 (1−ŷ)^γ 和 ŷ^γ 的“操控”后，也许形势还逆转了，还要对正样本降权。\n7.4 人脸识别loss\n出处1\n出处2\n7.4.1 Softmax Loss\n见3.2节\n7.4.2 Center Loss\n对MNIST数据集进行分类，若损失函数采用上述介绍的Softmax Loss(因为Softmax Loss能够使特征可分)，那么最后每个类别数字学出来的特征分布下图，我们可以看出类间距离还是比较小，类内距离比较大的，虽然效果很好：\n\n如果损失函数采用Center Loss，那么特征分布如下图，我们可以看出相比于Softmax Loss, 类间距离变大了，类内距离变小了：\n\n所以我们可以看出Center Loss能够最小化类内距离的同时保证特征可分，来提高特征之间的可判别性！简单地说，给每一类(lable)定义一个类中心(Center)，同一类的数据向类中心靠近，离得远要惩罚！于是Center Loss就出现了。\n\n其中 表示这个样本所对应的第类别的特征中心， m表示每一个batch大小。上述公式的意义是：希望batch中的每个样本特征距离特征中心的距离的平方和越小越好，也就是负责类内差距。\n那么上述的每一batch怎么确定的呢？理想情况下，需要随着学习到的feature进行实时更新，也就是在每一次迭代的时候用整个数据集的feature来计算每个类的中心。但是这样时间复杂度高，于是：用 batch来更新enter，每一轮计算一下当前batch数据与center的距离，然后这个距离以梯度的形式叠加到center上。 我们下面对求导：\n\n这里因为每个 batch的数量m太小，那么每次更新center可能会引起抖动。那么梯度上面加个限制，这个值在0-1之间：\n△\n为了最小化类内，最大化类间，即满足特征可分和特征可判别，论文中将Softmax Loss和Center Loss结合。\n\n7.4.3 A-Softmax Loss\n传统的Softmax很容易优化，因为它没有尽可能的扩大类间距离，缩小类内距离。\n7.4.4 L-Softmax Loss\n7.4.5 CosFace Loss\n7.4.6 AM-Softmax Loss\n7.4.7 ArcFace/Insight Face\n7.4.8 Triplet Loss\n7.5 Gan loss\n7.5.1 JS散度\n出处\nGAN实际是通过对先验分布施加一个运算G, 来拟合一个新的分布\n\n如果从传统的判别式网络的思路出发，只要选定合适的loss，就可以使生成分布和真实分布之间的距离尽可能逼近 KL散度经常用来衡量分布之间距离:  但KL散度是不对称的。不对称意味着，对于同一个距离，观察方式不同，获取的loss也不同，那么整体loss下降的方向就会趋向于某个特定方向。这在GAN中非常容易造成模式崩塌，即生成数据的多样性不足\nJS散度在KL散度的基础上进行了修正，保证了距离的对称性： \n实际上，无论KL散度还是JS散度，在直接用作loss时，都是难以训练的：由于分布只能通过取样计算，这个loss在每次迭代时都几乎为零\n7.5.2 GAN loss\n推荐\n本质就是在做一个极大似然估计的事情，我们希望可以用某一种具体的分布形式尽可能逼真地表达分布，这样我们就相当于是得到了，并据此分布 采样（也就是做生成式的任务）：\n对于生成器G：\n\nG是一个函数，输入，输出上x~\n先验分布, 和G共同决定的分布\n\n对于判别器D:\n\nD是一个函数，输入x~，输出一个scalar\nD用于评估和之间的差异\n\n那么，GAN的最终目标--&gt;用符号化语言表示就是：\n\n我们的目标是得到使得式子maxV(G,D)最小的生成器.\n关于V:\n\n7.5.3 Wasserstein GAN\n推荐 * 判别器最后一层去掉sigmoid * 生成器和判别器的loss不取log * 每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c * 不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行\n7.5.3 WGAN-GP\n推荐\n跟WGAN不同的主要有几处：1）用gradient penalty取代weight clipping；2）在生成图像上增加高斯噪声；3）优化器用Adam取代RMSProp。\n\n\n\n\n\n\n\n\n\n这里需要注意的是，这个GP的引入，跟一般GAN、WGAN中通常需要加的Batch Normalization会起冲突。因为这个GP要求critic的一个输入对应一个输出，但是BN会将一个批次中的样本进行归一化，BN是一批输入对应一批输出，因而用BN后无法正确求出critic对于每个输入样本的梯度。\n\n\n\n\n\n\n\n\n\n对于WGAN-gp的判别器，是不能加 batchNorm 的，原因很简单， 是因为WGAN-gp的惩罚项计算中，惩罚的是单个数据的gradient norm，如果使用 batchNorm，就会扰乱这种惩罚，让这种特别的惩罚失效。 当然你可以绕过 batchNorm, 使用 layerNorm 或者 InstanceNorm。\n8. 模型结构\n8.1 为什么resnet效果会那么好\n出处\n\n使网络更容易在某些层学到恒等变换（identity mapping）。在某些层执行恒等变换是一种构造性解，使更深的模型的性能至少不低于较浅的模型。这也是作者原始论文指出的动机。Deep Residual Learning for Image Recognition\n残差网络是很多浅层网络的集成（ensemble），层数的指数级那么多。主要的实验证据是：把 ResNet 中的某些层直接删掉，模型的性能几乎不下降。 Residual Networks Behave Like Ensembles of Relatively Shallow Networks\n残差网络使信息更容易在各层之间流动，包括在前向传播时提供特征重用，在反向传播时缓解梯度信号消失。原作者在一篇后续文章中给出了讨论。Identity Mappings in Deep Residual Networks\n\n\n\n\n\n\n\n\n\n\nhigh way(当门为1的时候，全部输出原x，不用激活。) 比较: https://blog.csdn.net/qq_27009517/article/details/84028568\n8.2 mobilenet\n参考\n8.3 shuffle net\n参考\nSE模块\n9. 实际训练相关\n9.1 学习过程中学习曲线产生振荡，可能原因？\n\n训练的batch_size太小\n\nbatch数太小，而类别又比较多的时候，可能会导致loss函数震荡而不收敛，尤其是在网络比较复杂的时候。\n随着batchsize增大，处理相同的数据量的速度越快。\n随着batchsize增大，达到相同精度所需要的epoch数量越来越多。\n由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。\n过大的batchsize的结果是网络很容易收敛到一些不好的局部最优点。同样太小的batch也存在一些问题，比如训练速度很慢，训练不容易收敛等。\n体的batch size的选取和训练集的样本数目相关\n\n\n\n\n\n\n\n\n\n\n\n一定范围内，batchsize越大，其确定的下降方向就越准，引起训练震荡越小。 batchsize增大到一定的程度，其确定的下降方向已经基本不再变化。一味地增加batch size就好，太大的batch size 容易陷入sharp minima，泛化性不好\n\n学习率：学习率太大，一步前进的路程太长，会出现来回震荡的情况，但是学习率太小，收敛速度会比较慢。\n是否找到合适的loss函数：在深度学习里面，不同的loss针对的任务是有不同的，有些loss函数比较通用例如L1/L2等，而如perceptual loss则比较适合在图像恢复/生成领域的任务上。当loss出现问题的适合，想一想，是不是loss设置的有问题，别人在此领域的任务的方法是否也使用和你一样的loss。\n是否使用合适的激活函数：一般来说，都几乎使用RELU作为全局激活函数，尽可能少的使用sigmoid激活函数（激活范围太小），容易造成梯度弥散、消失\n是否选择合适的优化算法：一般来说，我都使用Adam作为优化器（默认参数）。如果经过仔细调整的SGD算法性能可能更好，但是时间上不太允许这样做。\n\n9.2 loss nan原因\n\n如果在迭代的100轮以内，出现NaN，一般情况下的原因是因为你的学习率过高，需要降低学习率。可以不断降低学习率直至不出现NaN为止，一般来说低于现有学习率1-10倍即可。\n如果当前的网络是类似于RNN的循环神经网络的话，出现NaN可能是因为梯度爆炸的原因，一个有效的方式是增加“gradient clipping”（梯度截断来解决）\n可能用0作为了除数;\n可能0或者负数作为自然对数\n需要计算loss的数组越界（尤其是自己，自定义了一个新的网络，可能出现这种情况）\n在某些涉及指数计算，可能最后算得值为INF（无穷）（比如不做其他处理的softmax中分子分母需要计算exp（x），值过大，最后可能为INF/INF，得到NaN，此时你要确认你使用的softmax中在计算exp（x）做了相关处理（比如减去最大值等等））\n\n9.3 loss不降/网络不收敛怎么办，valid loss不降（过拟合）怎么办\n训练集loss不下降：\n出处\n\n模型结构和特征工程存在问题\n权重初始化方案有问题 &gt; 建议无脑xaiver normal初始化或者 he normal\n正则化过度 L1 L2和Dropout是防止过拟合用的，当训练集loss下不来时，就要考虑一下是不是正则化过度，导致模型欠拟合了。\n选择合适的激活函数、损失函数\n选择合适的优化器和学习速率\n模型训练遇到瓶颈 这里的瓶颈一般包括：梯度消失、大量神经元失活、梯度爆炸和弥散、学习率过大或过小等。\nbatch size过大 batch size过小，会导致模型后期摇摆不定，迟迟难以收敛，而过大时，模型前期由于梯度的平均，导致收敛速度过慢。\n数据集未打乱，数据集问题，未归一化\n\nvalid loss不降（过拟合）：\n\nRegularization(正则化) 权重衰减(Weight Decay)。到训练的后期，通过衰减因子使权重的梯度下降地越来越缓。\n\n\nBatch Normalization\nDropout\nL1 , L2\n\n\n调整网络结构 过拟合很重要的一个原因也是模型的复杂度太高，除了正则化手段以外，适当减小模型的规模也是很重要的，尽量让神经网络结构的假设空间与预期目标模型需要存储的信息量相匹配。\n增大训练数据量 这是终极解决方案，深度学习就是在有大量数据的基础上发展起来的。深度学习的三件套：数据、模型和硬件。模型可以直接拿来用，硬件可以花钱买，但是数据需要一点一点去收集，而且很多问题的解决就依赖于大量的数据，没数据就没有一切。\nEarly Stopping(早停法) (详细解释)早停法将数据分成训练集和验证集，训练集用来计算梯度、更新权重和阈值，验证集用来估计误差，若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值。\n\n9.4 判断过拟合，欠拟合\n出处\n\n\n\n\n\n\n\n\n\n学习曲线就是通过画出不同训练集大小时训练集和交叉验证的准确率，可以看到模型在新数据上的表现，进而来判断模型是否方差偏高或偏差过高，以及增大训练集是否可以减小过拟合。\n\n欠拟合: 当训练集和测试集的误差收敛但却很高时，为高偏差。 左上角的偏差很高，训练集和验证集的准确率都很低，很可能是欠拟合。 我们可以增加模型参数，比如，构建更多的特征，减小正则项。 此时通过增加数据量是不起作用的。\n过拟合: 当训练集和测试集的误差之间有大的差距时，为高方差。 当训练集的准确率比其他独立数据集上的测试结果的准确率要高时，一般都是过拟合。 右上角方差很高，训练集和验证集的准确率相差太多，应该是过拟合。 我们可以增大训练集，降低模型复杂度，增大正则项，或者通过特征选择减少特征数。\n\n\n\n\n\n\n\n\n\n理想情况是是找到偏差和方差都很小的情况，即收敛且误差较小。\n9.5 梯度消失和梯度爆炸，哪些函数会导致梯度消失\n\n\n\n\n\n\n\n\n\n梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。\n其实梯度爆炸和梯度消失问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。对于更普遍的梯度消失问题，可以考虑用ReLU激活函数取代sigmoid激活函数。另外，LSTM的结构设计也可以改善RNN中的梯度消失问题。\n\n\n\n\n\n\n\n\n\n哪些函数会导致梯度消失? sigmoid\n解决方法： 1. 好的参数初始化方式，如He初始化 2. 非饱和的激活函数（如 ReLU） 3. 批量规范化（Batch Normalization） 4. 梯度截断（Gradient Clipping） 5. 更快的优化器 6. LSTM\n9.6 cnn中网络权重初始化为0有什么问题，为什么不能，全部初始化为常数呢\n\n\n\n\n\n\n\n\n\n结论：在训练神经网络的时候，权重初始化要谨慎，不能初始化为0\n参考 如果神经元的权重被初始化为0， 在第一次更新的时候，除了输出之外，所有的中间层的节点的值都为零。一般神经网络拥有对称的结构，那么在进行第一次误差反向传播时，更新后的网络参数将会相同，在下一次更新时，相同的网络参数学习提取不到有用的特征，因此深度学习模型都不会使用0初始化所有参数。\n\n模型所有权重w初始化为0，所有偏置b初始化为0 简单来说，就会出现同一隐藏层所有神经元的输出都一致，对于后期不同的batch，每一隐藏层的权重都能得到更新，但是存在每一隐藏层的隐藏神经元权重都是一致的，多个隐藏神经元的作用就如同1个神经元。\n模型所有权重w初始化为0，所有偏置b随机初始化 这种方式存在更新较慢、梯度消失、梯度爆炸等问题，在实践中，通常不会选择此方式。\n模型所有的权重w随机初始化，所有偏置b初始化为0 在反向传播的过程中所有权重的导数都不相同，所以权重和偏置b都能得到更新。\n\n9.7 初始化方法，He解决了啥\n参考\nXavier Xavier初始化将一个层的权重设置为从一个有界的随机均匀分布中选择的值\n\n其中，是传入网络连接的数量叫“扇入”，是从那层出去的网络连接的数量，也被称为“扇出”。\nHe\n\n\n\n\n\n\n\n\n\n探索如何用类relu的激活函数在网络中最好地初始化权重是kobjective He等人，提出他们自己的初始化方案的动机，这是为使用这些非对称、非线性激活的深层神经网络量身定制的。\n方法： 1. 为给定层上的权值矩阵创建一个张量，并用从标准正态分布中随机选择的数字填充它。 2. 将每个随机选择的数字乘以√2/√n，其中n是从上一层的输出(也称为“扇入”)进入给定层的连接数。 3. 偏置张量初始化为零。\n9.8 反向传播时，同一个mini-batch在共享卷积层的末端是否需要除以batch-size？\n个人觉得不需要。\n由于目前主流深度学习框架处理mini-batch的反向传播时，默认都是先将每个mini-batch中每个instance得到的loss平均化之后再反求梯度，也就是说每次反向传播的梯度是对mini-batch中每个instance的梯度平均之后的结果。\n9.9 偏置和方差区别，模型比较复杂时，偏置和方差变化：偏置变小，方差变大\n\n\n\n\n\n\n\n\n\n模型复杂度增加时，模型预测的方差会增大，偏差会减小\n出处\n\n偏差(Bias)：在不同训练集上训练得到的所有模型的平均性能和最优模型的差异，可以用来衡量模型的拟合能力。\n方差(Variance)：在不同的训练集上训练得到的模型之间的性能差异，表示数据扰动对模型性能的影响，可以用来衡量模型是否容易过拟合，即模型的泛化能力。\n\n所以，当模型的复杂度增加时，模型的拟合能力得到增强，偏差便会减小，但很有可能会由于拟合“过度”，从而对数据扰动更加敏感，导致方差增大。从模型评价上来看，模型复杂度增加后，出现验证集效果提升，但是测试集效果下降的现象。\n9.10 多卡训练，梯度为什么平均\n出处\n单GPU情况下，也要对一个 batch 中 每个 data sample 的gradient 求平均。现在回到多卡的情况，相当于把一个 batch 分到多个卡上去跑，仍然希望在这个 batch 内求平均。现在假设我们一个 batch 有 20 的 data sample，本来单卡我们可以直接对这 20 个 data sample 求平均，现在加入我们有 5 个卡，那么每个卡跑 4 个 data sample，那么我们可以先 4 个 4 个的求平均，然后把得到的 5 个均值求平均，得到的结果和直接对 20 个 data sample 求平均是一样的。\n9.11 蒸馏\n详见\n先训练好一个teacher网络，然后将teacher的网络的输出结果q作为student网络的目标，训练student网络，使得student网络的结果p接近q，因此，我们可以将损失函数写成:\n\ny是真实标签的onehot编码，q是teacher网络的输出结果，p是student网络的输出结果。\n但是，直接使用teacher网络的softmax的输出结果q，可能不大合适。因此，一个网络训练好之后，对于正确的答案会有一个很高的置信度。例如，在MNIST数据中，对于某个2的输入，对于2的预测概率会很高，而对于2类似的数字，例如3和7的预测概率为10-6和10-9。这样的话，teacher网络学到数据的相似信息（例如数字2和3，7很类似）很难传达给student网络。由于它们的概率值接近0。因此，文章提出了softmax-T，公式如下所示：\n\n这里是student网络学习的对象（soft targets），是神经网络softmax前的输出logit。如果将T取1，这个公式就是softmax，根据logit输出各个类别的概率。如果T接近于0，则最大的值会越近1，其它值会接近0，近似于onehot编码。如果T越大，则输出的结果的分布越平缓，相当于平滑的一个作用，起到保留相似信息的作用。如果T等于无穷，就是一个均匀分布。\n9.12 怎么做模型压缩？\n详见\n9.1x int8量化和卷积加速的方式\nfft，winograd，im2col+sgemm 详见\n10. 分类\n零样本分类问题，如果测试出现一个图片是训练时没有的，怎么做\n零次学习\n知乎: 不用那么麻烦，我之前做ins用户分类的时候也遇到过类似的问题不过我用的是向量化文本数据之后上lightgbm测试，训练集大概有十几个类别包括厨师，宠物，画家，摄影师等，测试的时候发现了测试集中出现了“黑人说唱歌手”这个群体训练集中没有的群体，但是因为模型已经训练出了十几个分类的结构了所以最终黑人说唱歌手被预测为宠物，后来想了一个办法就是把多分类问题转化为多个二分类问题，然后设定预测概率超过比如0.8的时候判定为某个类别否则判定为其它类，这样就可以解决问题了，黑人说唱歌手最终因为没有达到预先设定的阈值所以没有被判定为任何类别。\n11. 目标检测\n11.1 roi pooling和roi align区别，怎么做插值\n参考1参考2\n这两个都是用在rpn之后的。具体来说，从feature map上经过RPN得到一系列的proposals，大概2k个，这些bbox大小不等，如何将这些bbox的特征进行统一表示就变成了一个问题。即需要找一个办法从大小不等的框中提取特征使输出结果是等长的。\n最开始目标检测模型Faster RCNN中用了一个简单粗暴的办法，叫ROI Pooling。 该方式在语义分割这种精细程度高的任务中，不够精准。RoI Align 在 Mask RCNN 中被首次提出。针对RoI Pooling在语义分割等精细度任务中精确度的问题提出的改进方案。\n11.1.1 ROI Pooling\n\n假如现在有一个8x8的feature map，现在希望得到2x2的输出，有一个bbox坐标为[0,3,7,8]。 这个bbox的w=7，h=5，如果要等分成四块是做不到的，因此在ROI Pooling中会进行取整。就有了上图看到的h被分割为2,3，w被分割成3,4。这样之后在每一块(称为bin)中做max pooling，可以得到下图的结果。\n\n这样就可以将任意大小bbox转成2x2表示的feature。 ROI Pooling需要取整，这样的取整操作进行了两次，一次是得到bbox在feature map上的坐标时。\n例如：原图上的bbox大小为665x665，经backbone后，spatial scale=1/32。因此bbox也相应应该缩小为665/32=20.78，但是这并不是一个真实的pixel所在的位置，因此这一步会取为20。0.78的差距反馈到原图就是0.78x32=25个像素的差距。如果是大目标这25的差距可能看不出来，但对于小目标而言差距就比较巨大了。\n\n两次量化 以输出目标特征图尺寸大小为2x2x512进行说明 1. 对齐到网格单元（snap to grid cell） 首先将一个浮点数RoI量化为特征映射的离散粒度。表现为RoI对应的特征图的与原始特征图的网格单元对齐。这里为第一次量化操作。 下图中绿色框为RoI对应的实际区域（由于经过特征尺度变换，导致RoI的坐标会可能会落到特征图的单元之间）， 蓝色框代表量化(网格对齐)后的RoI所对应的特征图。（得到到量化特征图尺寸为5x7x512） \n\n划分网格为子区域（bin） 粗略地将网格分为H*W（Fast RCNN 中设为7x7）个子网格区域。将上一步得到的量化RoI 特征进一步细分为量化的空间单元(bin)。这里进行了第二次量化操作。 为了得到输出的特征图为2x2x512，这里的量化操作就是将上一步的到量化特征图划分为2x2个特征单元。如果无法通过直接均分得到量化的子区域，通过分别采取向上取整（ceil）和向下取整（floor）的到对应的单元尺寸大小。以当前4x5尺寸的特征图为例，对于宽度方向4/2=2，但是对于高度方向由于5/2=2.5， 通过向上和向下取整整，确定高度方向特征子区域的大小分别为2和3。 \n\n缺点 每一次量化操作都会对应着轻微的区域特征错位（misaligned）， 这些量化操作在RoI和提取到的特征之间引入了偏差。这些量化可能不会影响对分类任务，但它对预测像素精度掩模有很大的负面影响。\n11.1.2 ROI Align\n因此有人提出不需要进行取整操作，如果计算得到小数，也就是没有落到真实的pixel上，那么就用最近的pixel对这一点虚拟pixel进行双线性插值，得到这个“pixel”的值。 具体做法如下图所示： \n\n来源见水印\n\n\n将bbox区域按输出要求的size进行等分，很可能等分后各顶点落不到真实的像素点上\n没关系，在每个bin中再取固定的4个点(作者实验后发现取4效果较好)，也就是图二右侧的蓝色点\n针对每一个蓝点，距离它最近的4个真实像素点的值加权(双线性插值)，求得这个蓝点的值\n一个bin内会算出4个新值，在这些新值中取max，作为这个bin的输出值\n最后就能得到2x2的输出\n\n​ 下面以输出目标特征图尺寸大小为2x2x512进行说明 1. 遍历候选每个候选区域，保持浮点数边界不做量化（不对齐网格单元）；同时平均分网格分为HxW（这里为2x2）个子网格区域，每个单元的边界也不做量化。 \n\n对于每个区域选择4个规则采样点（分别对应将区域进一步平均分为四个区域，取每个子区域的中点）。 \n利用双线性插值计算得到四个采用点的像素值大小。下图为一个规则采样点所对应的邻近区域示意图。 \n利用最大池化（max pooling）或平均池化(average pooling)分别对每个子区域执行聚合操作，得到最终的特征图。 \n\n11.2 OHEM\nTraining Region-based Object Detectors with Online Hard Example Mining\n\n\n\n\n\n\n\n\n\n需要注意的是，这个OHEM适合于batch size（images）较少，但每张image的examples很多的情况。\nOHEM算法的核心是选择一些hard example作为训练的样本从而改善网络参数效果，hard example指的是有多样性和高损失的样本。 hard example是根据每个ROI的损失来选择的，选择损失最大的一些ROI。但是这里有一个问题：重合率比较大的ROI之间的损失也比较相似。因此这里作者采用NMS（non-maximum suppresison）去除重合率较大的ROI，这里作者给的阈值是当IOU大于0.7就认为重合率较高，需去除。\n注意，这里作者没有采用设定背景和目标样本数的比例方式处理数据的类别不平衡问题。因为如果哪个类别不平衡，那么这个类别的损失就会比较大，这样被采样的可能性也比较大。 论文中把OHEM应用在Fast R-CNN中具体参看博文，是因为Fast R-CNN相当于目标检测各大框架的母体，很多框架都是它的变形，所以作者在Fast R-CNN上应用很有说明性。\n\n作者将OHEM应用在Fast RCNN的网络结构，如上图。这里包含两个ROI network，上面一个ROI network是只读的，为所有的ROI在前向传递的时候分配空间。下面一个ROI network则同时为前向和后向分配空间。\n首先，ROI经过ROI plooling层生成feature map，然后进入只读的ROI network得到所有ROI的loss；然后是hard ROI sampler结构根据损失排序选出hard example，并把这些hard example作为下面那个ROI network的输入。\n实际训练的时候，每个mini-batch包含N个图像，共|R|个ROI，也就是每张图像包含|R|/N个ROI。经过hard ROI sampler筛选后得到B个hard example。作者在文中采用N=2，|R|=4000，B=128。\n另外关于正负样本的选择：当一个ROI和一个ground truth的IOU大于0.5，则为正样本；当一个ROI和所有ground truth的IOU的最大值小于0.5时为负样本。\n总结来说，对于给定图像，经过selective search RoIs，同样计算出卷积特征图。但是在绿色部分的（a）中，一个只读的RoI网络对特征图和所有RoI进行前向传播，然后Hard RoI module利用这些RoI的loss选择B个样本。在红色部分（b）中，这些选择出的样本（hard examples）进入RoI网络，进一步进行前向和后向传播。\n\n\n\n\n\n\n\n\n\n论文提及到可以用一种简单的方式来完成hard mining：在原有的Fast-RCNN里的loss layer里面对所有的props计算其loss，根据loss对其进行排序，（这里可以选用NMS），选出K KK个hard examples（即props）。反向传播时，只对这K KK个props的梯度/残差回传，而其他的props的梯度/残差设为0 00即可。由于这样做，容易导致显存显著增加，迭代时间增加，\nFocal-Loss与OHEM的关系 OHEM是只取3:1的负样本去计算loss，之外的负样本权重置零，而focal loss取了所有负样本，根据难度给了不同的权重。 focal loss相比OHEM的提升点在于，3:1的比例比较粗暴，那些有些难度的负样本可能游离于3:1之外。之前实验中曾经调整过OHEM这个比例，发现是有好处的，现在可以试试focal loss了。\n11.3 正负样本比例\n一把情况下，negativa default box &gt;&gt; positive default box数量，直接训练会导致网络过于重视负样本，从而loss不稳定。所以，SSD在臭氧是按照置信度误差（预测背景的置信度越小，误差越大）进行降序排列，选取误差较大的top-k作为训练的负样本，控制positive：negtive=1:3。作者发现这可导致模型更快的优化和更稳定的训练。\n11.4 样本不均衡\n检测\n分类\n处理不平衡问题的loss\n不平衡数据集处理方法\n11.5 nms时间复杂度\n假如一张图片中有n个检测框，由于顺序处理的原因，某一个框与其他框计算IoU，最少一次，最多有n-1次。再加上顺序迭代抑制，NMS算法在计算IoU方面，共需要计算IoU至少n-1次，最多(n-1)+(n-2)+...+1=次。\n11.6 小问题\n11.6.1 目标检测有一个类别ap很低\n\n低AP数据类别增强\n低AP loss权重\n\n11.6.2 检测出现一半的物体处理方式\n最好要标注遮挡的程度。这个不仅对训练有帮助，可以因为可以选择训练的样本。但是更重要的是对测试的帮助。如果有了遮挡的标注，就可以把测试数据集分为不同的集合，比如说无遮挡集合，中度遮挡集合和重度遮挡集合。这样可以得到更加细致和精确的测试结果，有利于问题的分析和解决。\n11.6.3 小目标\n参考 1. 传统的图像金字塔和多尺度滑动窗口检测，如MTCNN 2. FPN 3. Data Augmentation\n11.6.4 如何解决类内检测/重叠\n\nRepulsion loss，参见CVPR 2018论文 repulsion loss，用作拥挤人群检测；\nSoft NMS，降低confidence而不是直接去掉；\nNMS的改进 Acquisition of Localization Confidence for Accurate Object Detection\nPart based，分解物体为多个部位的组合，是一个思路；如OR-CNN\n融合其他信息，如分割的信息，语义信息；\n\n11.6.5 检测的框偏移45度，怎么处理\n参考\n11.7 soft-nms\n出处\nNMS缺点 1、NMS算法中的最大问题就是它将相邻检测框的分数均强制归零(即将重叠部分大于重叠阈值Nt的检测框移除)。在这种情况下，如果一个真实物体在重叠区域出现，则将导致对该物体的检测失败并降低了算法的平均检测率。 2、NMS的阈值也不太容易确定，设置过小会出现误删，设置过高又容易增大误检。\nNMS算法是略显粗暴，因为NMS直接将删除所有IoU大于阈值的框。soft-NMS吸取了NMS的教训，在算法执行过程中不是简单的对IoU大于阈值的检测框删除，而是降低得分。算法流程同NMS相同，但是对原置信度得分使用函数运算，目标是降低置信度得分.\n\n\n来源见水印\n\n为待处理BBox框，B为待处理BBox框集合，是框更新得分，是NMS的阈值，D集合用来放最终的BBox，f是置信度得分的重置函数。 和M的IOU越大，的得分就下降的越厉害。\n经典的NMS算法将IOU大于阈值的窗口的得分全部置为0，可表述如下：\n\n论文置信度重置函数有两种形式改进，一种是线性加权的：\n 一种是高斯加权形式： \n具体代码实现： if method==1: # linear\n    if ov&gt;Nt:\n        weight=1-ov\n    else:\n        weight=1\nelif method==2: # Gaussian\n    weight=p.exp(-(ov*ov)/sigma)\nelse:          # Original NMS\n    if ov&gt;Nt:\n        weight=0\n    else:\n        weight=1\n优点：\n1、Soft-NMS可以很方便地引入到object detection算法中，不需要重新训练原有的模型、代码容易实现，不增加计算量（计算量相比整个object detection算法可忽略）。并且很容易集成到目前所有使用NMS的目标检测算法。 2、soft-NMS在训练中采用传统的NMS方法，仅在推断代码中实现soft-NMS。作者应该做过对比试验，在训练过程中采用soft-NMS没有显著提高。 3、NMS是Soft-NMS特殊形式，当得分重置函数采用二值化函数时，Soft-NMS和NMS是相同的。soft-NMS算法是一种更加通用的非最大抑制算法。\n缺点： soft-NMS也是一种贪心算法，并不能保证找到全局最优的检测框分数重置。除了以上这两种分数重置函数，我们也可以考虑开发其他包含更多参数的分数重置函数，比如Gompertz函数等。但是它们在完成分数重置的过程中增加了额外的参数。\nsofter-nms\n11.8 几种iou\n见\n11.9 小插件\n\nSPP\n\n\n\n\n\n\n\n\n\n\nSPP的本质就是多层maxpool\n\nSPP优点:\n（1）对于不同尺寸的CNN_Pre输出能够输出固定大小的向量。 （2）可以提取不同尺寸的空间特征信息，可以提升模型对于空间布局和物体变性的鲁棒性。 （3）可以避免将图片resize、crop成固定大小输入模型的弊端。\n\nFPN\n\n1. 不同深度的 feature map 为什么可以经过 upsample 后直接相加？\nA：作者解释说这个原因在于我们做了 end-to-end 的 training，因为不同层的参数不是固定的，不同层同时给监督做 end-to-end training，所以相加训练出来的东西能够更有效地融合浅层和深层的信息。 #### 2. 为什么 FPN 相比去掉深层特征 upsample(bottom-up pyramid) 对于小物体检测提升明显？（RPN 步骤 AR 从 30.5 到 44.9，Fast RCNN 步骤 AP 从 24.9 到 33.9） A：对于小物体，一方面它提高了小目标的分辨率信息； 另一方面，如图中的挎包一样，从上到下传递过来的更全局的情景信息可以更准确判断挎包的存在及位置。 \n\nPAN\n\n\nFPN加入top-down的旁路连接，能给feature增加high-level语义,有利于分类。但是PAN论文作者觉得low-level的feature很有利于定位，虽然FPN中P5也间接融合了low-level的特征，但是信息流动路线太长了如红色虚线所示，其中会经过超多conv操作，本文在FPN的P2-P5又加了low-level的特征，最底层的特征流动到N2-N5只需要经过很少的层如绿色需要所示，主要目的是加速信息融合,缩短底层特征和高层特征之间的信息路径.\n11.10 经典算法\nFaster RCNN maskrcnn， cascade rcnn\nFaster RCNN\nanchor box与bbox 正负样本选取的问题\nMask RCNN\n参考1 参考2\ncascade rcnn\n参考1 参考2\nyolo系列\nfcos\ncenter net\nmismatch问题\n\n\n来源见水印\n\n一张图说明问题，在上面这张图中，把RPN提出的Proposals的大致分布画了下，横轴表示Proposals和gt之间的iou值，纵轴表示满足当前iou值的Proposals数量。\n\n在training阶段，由于我们知道gt，所以可以很自然的把与gt的iou大于threshold（0.5）的Proposals作为正样本，这些正样本参与之后的bbox回归学习。\n在inference阶段，由于我们不知道gt，所以只能把所有的proposal都当做正样本，让后面的bbox回归器回归坐标。\n\n我们可以明显的看到training阶段和inference阶段，bbox回归器的输入分布是不一样的，training阶段的输入proposals质量更高(被采样过，IoU&gt;threshold)，inference阶段的输入proposals质量相对较差(没有被采样过，可能包括很多IoU &lt; threshold的)，这就是论文中提到mismatch问题，这个问题是固有存在的，通常threshold取0.5时，mismatch问题还不会很严重。\n12. 其他\n12.1 注意力机制\n来源 github\nSE,SK,CBAM\nattention怎么做，self-attention怎么做，原理是什么，为什么有效，multi attention（多头），attention里面的QKV是啥\n图解Self-Attention\n计算 attention 的过程，即使用 一个 Q(uery)，计算它和每个K(ey)的相似度作为权重,对所有的 V(alue)进行加权求和。\nencoder-decoder中，如果decoder基于attention应该怎么做\nPCA\n参考\n","slug":"CV面试基础总结","date":"2021-10-12T07:57:22.000Z","categories_index":"","tags_index":"interview summary","author_index":"Hulk Wang"},{"id":"06a101cf6ba6fad0463c3458164de1c3","title":"Faster RCNN 记录","content":"\n\n\n\n\n\n\n\n\n参考来源： https://zhuanlan.zhihu.com/p/31426458 https://zhuanlan.zhihu.com/p/86403390\n\n\nFaster RCNN基本结构\n\nFaster RCNN其实可以分为4个主要内容：\n\n特征提取：Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。\nRPN：RPN网络用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。\nRoi Pooling：该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。\nClassification：利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。\n\n\n\nFaster RCNN(VGG16)网络结构\n\n特征提取\n没啥可说的，特征提取，输入MxN，VGG16为例，4个pooling层，得到的feature map分辨率为：（M/16）x（N/16）\nRPN\n\n\nRPN结构\n\nRPN网络实际分为2条线，上面一条通过softmax分类anchors获得positive和negative分类，下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合positive anchors和对应bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。\nanchors\n以tf代码为例： def _anchor_component(self):  #获得锚的数量和位置\n    with tf.variable_scope('ANCHOR_' + 'default'):\n        # just to get the shape right  只是为了让形状正确\n        height = tf.to_int32(tf.ceil(self._im_info[0, 0] / np.float32(self._feat_stride[0]))) #高度为图片高/16,，就是特征图的高，tf.ceil向上取整\n        width = tf.to_int32(tf.ceil(self._im_info[0, 1] / np.float32(self._feat_stride[0]))) #宽度为图片宽/16，为特征图的宽\n        anchors, anchor_length = tf.py_func(generate_anchors_pre,\n                                            [height, width,                                        self._feat_stride,self._anchor_scales, self._anchor_ratios],\n                                            [tf.float32, tf.int32], name=\"generate_anchors\")  #构建生成锚的py函数，这个锚有9*（50*38）个，anchor_length是锚的个数\n        anchors.set_shape([None, 4])  #锚定义为4列\n        anchor_length.set_shape([]) #行向量，length为锚的个数\n        self._anchors = anchors\n        self._anchor_length = anchor_length  #length为特征图面积*9 函数generate_anchors_pre来生成anchor：\ndef generate_anchors_pre(height, width, feat_stride, anchor_scales=(8, 16, 32), anchor_ratios=(0.5, 1, 2)):\n    \"\"\" A wrapper function to generate anchors given different scales\n      Also return the number of anchors in variable 'length' 给定不同比例生成锚点的包装函数也返回可变“长度”的锚点数量\n    \"\"\"\n    anchors = generate_anchors(ratios=np.array(anchor_ratios), scales=np.array(anchor_scales))\n    A = anchors.shape[0] #anchor的数量，为9\n    shift_x = np.arange(0, width) * feat_stride  #将特征图的宽度进行16倍延伸至原图，以width=4为例子，则shfit_x=[0,16,32,48]\n    shift_y = np.arange(0, height) * feat_stride  #将特征图的高度进行16倍衍生至原图\n    shift_x, shift_y = np.meshgrid(shift_x, shift_y)  #生成原图的网格点\n    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel())).transpose()  #若width=50，height=38，生成（50*38）*4的数组\n    #如 [[0,0,0,0],[16,0,16,0],[32,0,32,0].......]，shift中的前两个坐标和后两个一样（保持右下和左上的坐标一样），是从左到右，从上到下的坐标点（映射到原图）\n    K = shifts.shape[0]  #k=50*38\n    # width changes faster, so here it is H, W, C\n    anchors = anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2))  #列数相同的list相加就是简单的添加，而数组不一样，1*9*4和（50*38）*1*4进行相加，生成了（50*38）*9*4的数组\n    #其实意思就是右下角坐标和左上角的左边都加上同一个变换坐标\n    anchors = anchors.reshape((K * A, 4)).astype(np.float32, copy=False) #三维变两维，（50*38*9，4），此处就是将特征层的anchor坐标转到原图上的区域\n    length = np.int32(anchors.shape[0])  #length=50*38*9\n\n    return anchors, length\n观察第一行代码，即 anchors = generate_anchors(ratios=np.array(anchor_ratios),scales=np.array(anchor_scales)) 这里是3x3=9个基础anchor的生成，我们进入函数generate_anchors中，发现其实生成就是一个数组，这个数组是： # array([[ -83.,  -39.,  100.,   56.],\n#       [-175.,  -87.,  192.,  104.],\n#       [-359., -183.,  376.,  200.],\n#       [ -55.,  -55.,   72.,   72.],\n#       [-119., -119.,  136.,  136.],\n#       [-247., -247.,  264.,  264.],\n#       [ -35.,  -79.,   52.,   96.],\n#       [ -79., -167.,   96.,  184.],\n#       [-167., -343.,  184.,  360.]]) 其中每行的4个值（x1，y1, x2, y2）表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为{1:1, 1:2, 2:1}三种，实际上通过anchors就引入了检测中常用到的多尺度方法。\n生成这个数组的代码是： def generate_anchors(base_size=16, ratios=[0.5, 1, 2],\n                     scales=2 ** np.arange(3, 6)):\n    \"\"\"\n    Generate anchor (reference) windows by enumerating aspect ratios X\n    scales wrt a reference (0, 0, 15, 15) window.  通过枚举参考(0，0，15，15)窗口的长宽比来生成锚(参考)窗口。\n    \"\"\"\n\n    base_anchor = np.array([1, 1, base_size, base_size]) - 1  #生成一个base_anchor = [0, 0, 15, 15]，其中(0, 0)是anchor左上点的坐标\n    # (15, 15)是anchor右下点的坐标，那么这个anchor的中心点的坐标是(7.5, 7.5)\n    ratio_anchors = _ratio_enum(base_anchor, ratios)#然后产生ratio_anchors，就是将base_anchor和ratios[0.5, 1, 2],ratio_anchors生成三个anchors\n    # 传入到_ratio_enum()函数，ratios代表的是三种宽高比。\n    anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)  #在刚刚3个anchor基础上继续生成anchor\n                         for i in range(ratio_anchors.shape[0])])\n    return anchors 我们发现这个数组中的每行数据（如第一行：[ -83., -39., 100., 56.] )，它们的中心位置都为（7.5，7.5），即（0，0，15，15）的中心（因为通过代码也可以得知，这9个基础框的生成也是以（0，0，15，15）为基础的，代码中base_anchor就是（0，0，15，15））。\n通过3种anchor ratio和3种anchor scale生成的9个数组，这9个数组在坐标图上如图2所示。 \n\nanchor\n\n\n\n\n\n\n\n\n\n\n为什么这样设计呢？我们知道，其实一张图片通过特征提取网路VGG16后，长宽比都缩小了16倍得到了特征图。比如原先的800*600的原图通过VGG16后得到了50*38的特征图（以上先不考虑通道数），我们就假设，特征图上的每一个点（大小为1*1），和原图16*16区域对应（这里记住，对应不是指代感受野，只是便于理解).这里，使用对应的好处，就是特征图上的每个点（大小为1*1）负责由原图对应区域(大小为16*16）中心生成的9个anchor的训练和学习。所以Faster RCNN共产生50x38x9=17100个anchor，基本覆盖了全图各个区域。\n看generate_anchors_pre函数，shifts就是对（shift_x, shift_y）进行组合，其中shift_x是对x坐标进行移动，shift_y是对y坐标进行移动，综合起来就是将基础的中心为（7.5，7.5）的9个anchor平移到全图上，覆盖所有可能的区域。 anchors = anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)) #列数相同的list相加就是简单的添加，而数组不一样，1*9*4和（50*38）*1*4进行相加，生成了（50*38）*9*4的数组\n#其实意思就是右下角坐标和左上角的左边都加上同一个变换坐标\nanchors = anchors.reshape((K * A, 4)).astype(np.float32, copy=False) #三维变两维，（50*38*9，4），此处就是将特征层的anchor坐标转到原图上的区域\nlength = np.int32(anchors.shape[0]) #length=50*38*9 上述代码就是完成了9个base anchor 的移动，输出结果就是50*38*9个anchor。那么到此，所有的anchor都生成了，当然了，所有的anchor也和特征图产生了一一对应的关系了。\n\n\n\n\n\n\n\n\n\nanchor是辅助模型进行训练的，能让模型对物体的大小和形状有个大致的认知，也算是人为添加的先验知识了。\nRPN最终就是在原图尺度上，设置了密密麻麻的候选Anchor。然后用cnn去判断哪些Anchor是里面有目标的positive anchor，哪些是没目标的negative anchor。所以，仅仅是个二分类而已！\n\n解释一下上面这张图的数字。\n\n在原文中使用的是ZF model中，其Conv Layers中最后的conv5层num_output=256，对应生成256张特征图，所以相当于feature map每个点都是256-dimensions\n在conv5之后，做了rpn_conv/3x3卷积且num_output=256，相当于每个点又融合了周围3x3的空间信息\n假设在conv5 feature map中每个点上有k个anchor（默认k=9），而每个anhcor要分positive和negative，所以每个点由256d feature转化为cls=2•k scores；而每个anchor都有(x, y, w, h)对应4个偏移量，所以reg=4•k coordinates\n全部anchors拿去训练太多了，训练程序会在合适的anchors中随机选取128个postive anchors+128个negative anchors进行训练\n\nrpn中的二分类\n一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设 W=M/16，H=N/16。在进入reshape与softmax之前，先做了1x1卷积（此时anchor已经生成完毕），如图9：\n\n可以看到其num_output=18，也就是经过该卷积的输出图像为WxHx18大小,这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是positive和negative，所有这些信息都保存WxHx(9*2)大小的矩阵。\n\n\n\n\n\n\n\n\n\n综上所述，RPN网络中利用anchors和softmax初步提取出positive anchors作为候选区域（另外也有实现用sigmoid代替softmax，输出[1, 1, 9xH, W]后接sigmoid进行positive/negative二分类，原理一样）。\nbounding box regression\n\n\n\n\n\n\n\n\n\n在RPN网络中，进行bbox regression得到的是每个anchor的偏移量。再与anchor的坐标进行调整以后，得到proposal的坐标，经过一系列后处理，比如NMS，top-K操作以后，得到得分最高的前N个proposal传入分类网络。\n如图所示绿色框为飞机的Ground Truth(GT)，红色为提取的positive anchors，即便红色的框被分类器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机。所以我们希望采用一种方法对红色的框进行微调，使得positive anchors和GT更加接近。\n\n对于窗口一般使用四维向量 (x, y, w, h)表示，分别表示窗口的中心点坐标和宽高。对于下图，红色的框A代表原始的positive Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G'，即：\n\n\n给定anchor A=(, , , )和GT=[, , , ]\n寻找一种变换F，使得：F(, , , )= (, , , ),其中(, , , )≈(, , , )\n\n比较简单的思路就是:\n\n观察上面4个公式发现，需要学习的是,,, 这四个变换。当输入的anchor A与GT相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调（注意，只有当anchors A和GT比较接近时，才能使用线性回归模型，否则就是复杂的非线性问题了）。\n接下来的问题就是如何通过线性回归获,,, 了。线性回归就是给定输入的特征向量X, 学习一组参数W, 使得经过线性回归后的值跟真实值Y非常接近，即Y=WX.\n对于该问题，输入X是cnn feature map，定义为Φ；同时还有训练传入A与GT之间的变换量，即,,,。输出是,,,四个变换。那么目标函数可以表示为:\n\n所以在RPN中，bounding box regression通过第二条线完成：\n\n\nRPN中的bbox reg\n\n可以看到经过该卷积输出图像为WxHx36，在caffe blob存储为[1, 4x9, H, W]，这里相当于feature maps每个点都有9个anchors，每个anchors又都有4个用于回归的\nVGG输出(M/16)*(N/16)*256的特征，对应设置 (M/16)*(N/16)*k 个anchors，而RPN输出：\n\n大小为(M/16)*(N/16)*2k的positive/negative softmax分类特征矩阵\n大小为(M/16)*(N/16)*4k的regression坐标回归特征矩阵\n\n恰好满足RPN完成positive/negative分类+bounding box regression坐标回归.\nProposal Layer\nProposal Layer负责综合所有 ,,, 变换量和positive anchors，计算出精准的proposal，送入后续RoI Pooling Layer。\nProposal Layer有3个输入： 1. 分类器结果（positive vs negative anchorsrpn_cls_prob_reshape) 2. bbox reg的 ,,,变换量rpn_bbox_pred 3. im_info； 4. 另外还有参数feat_stride=16，这和图4是对应的。\n首先解释im_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定MxN，im_info=[M, N, scale_factor]则保存了此次缩放的所有信息。然后经过Conv Layers，经过4次pooling变为WxH=(M/16)x(N/16)大小，其中feature_stride=16则保存了该信息，用于计算anchor偏移量。\nProposal Layer forward按照以下顺序依次处理： 1.生成anchors，利用,,, 对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致） 2. 按照输入的positive softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的positive anchors 3. 限定超出图像边界的positive anchors为图像边界，防止后续roi pooling时proposal超出图像边界 4. 剔除尺寸非常小的positive anchors 5. 对剩余的positive anchors进行NMS（nonmaximum suppression） 6. Proposal Layer有3个输入：positive和negative anchors分类器结果rpn_cls_prob_reshape，对应的bbox reg的(e.g. 300)结果作为proposal输出\n之后输出proposal=[x1, y1, x2, y2]，注意，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应MxN输入图像尺度的，这点在后续网络中有用。\n\n\n\n\n\n\n\n\n\nRPN网络结构就介绍到这里，总结起来就是： 生成anchors -&gt; softmax分类器提取positvie anchors -&gt; bbox reg回归positive anchors -&gt; Proposal Layer生成proposals\nRPN生成RoIs\nRPN在自身训练的同时，还会提供RoIs（region of interests）给Fast RCNN（RoIHead）作为训练样本。RPN生成RoIs的过程(ProposalCreator)如下：\n\n对于每张图片，利用它的feature map， 计算 (H/16)× (W/16)×9（大概20000）个anchor属于前景的概率，以及对应的位置参数。\n选取概率较大的12000个anchor\n利用回归的位置参数，修正这12000个anchor的位置，得到RoIs\n利用非极大值（(Non-maximum suppression, NMS）抑制，选出概率最大的2000个RoIs\n\n注意：在inference的时候，为了提高处理速度，12000和2000分别变为6000和300.\n注意：这部分的操作不需要进行反向传播，因此可以利用numpy/tensor实现。\nROI pooling\nRoI Pooling层则负责收集proposal，并计算出proposal feature maps，送入后续网络。从图2中可以看到Rol pooling层有2个输入：\n\n原始的feature maps\nRPN输出的proposal boxes（大小各不相同）\n\n为何需要RoI Pooling\n当网络训练好后输入的图像尺寸必须是固定值，同时网络输出也是固定大小的vector or matrix。如果输入图像大小不定，这个问题就变得比较麻烦。有2种解决办法：\n\n从图像中crop一部分传入网络\n将图像warp成需要的大小后传入网络\n\n无论采取那种办法都不好，要么crop后破坏了图像的完整结构，要么warp破坏了图像原始形状信息。\nRPN网络生成的proposals的方法：对positive anchors进行bounding box regression，那么这样获得的proposals也是大小形状各不相同，即也存在上述问题。所以Faster R-CNN中提出了RoI Pooling解决这个问题.\nRoI Pooling layer forward\n\n由于proposal是对应MxN尺度的，所以首先使用spatial_scale参数将其映射回(M/16)x(N/16)大小的feature map尺度；\n再将每个proposal对应的feature map区域水平分为 pooled_w*_h的网格；\n对网格的每一份都进行max pooling处理。\n\n这样处理后，即使大小不同的proposal输出结果都是pooled_w*_h固定大小，实现了固定长度输出。\n为什么要pooling成7×7的尺度？\n是为了能够共享权重。在之前讲过，除了用到VGG前几层的卷积之外，最后的全连接层也可以继续利用。当所有的RoIs都被pooling成（512×7×7）的feature map后，将它reshape 成一个一维的向量，就可以利用VGG16预训练的权重，初始化前两层全连接。最后再接两个全连接层，分别是：\nFC 21 用来分类，预测RoIs属于哪个类别（20个类+背景） FC 84 用来回归位置（21个类，每个类都有4个位置参数）\nClassification\nClassification部分利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出cls_prob概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量bbox_pred，用于回归更加精确的目标检测框。Classification部分网络结构如图:\n\n\nClassification部分网络结构\n\n从RoI Pooling获取到7x7=49大小的proposal feature maps后，送入后续网络，可以看到做了如下2件事：\n\n通过全连接和softmax对proposals进行分类，这实际上已经是识别的范畴了\n再次对proposals进行bounding box regression，获取更高精度的rect box\n\nAnchor到底与网络输出如何对应\nVGG输出 50*38*512的特征，对应设置 50*38*k个anchors，而RPN输出50*38*2k的分类特征矩阵和50*38*4k的坐标回归特征矩阵。\n\n其实在实现过程中，每个点的2k个分类特征与 4k回归特征，与 k个anchor逐个对应即可，这实际是一种“人为设置的逻辑映射”。当然，也可以不这样设置，但是无论如何都需要保证在训练和测试过程中映射方式必须一致。\nLoss\n\n上述公式中 i 表示anchors index， 表示positive softmax probability，代表对应的GT predict概率（即当第i个anchor与GT间IoU&gt;0.7，认为是该anchor是positive，=1 ；反之IoU&lt;0.3时，认为是该anchor是negative，=0 ；至于那些0.3&lt;IoU&lt;0.7的anchor则不参与训练）；t 代表predict bounding box，代表对应positive anchor对应的GT box。可以看到，整个Loss分为2部分：\n\n在训练Faster RCNN的时候有四个损失：\n\nRPN 分类损失：anchor是否为前景（二分类）\nRPN位置回归损失：anchor位置微调\nRoI 分类损失：RoI所属类别（21分类，多了一个类作为背景）\nRoI位置回归损失：继续对RoI位置微调\n\n四个损失相加作为最后的损失，反向传播，更新参数。\n### 注意\n1\n\n在RPN的时候，已经对anchor做了一遍NMS，在RCNN测试的时候，还要再做一遍\n在RPN的时候，已经对anchor的位置做了回归调整，在RCNN阶段还要对RoI再做一遍\n在RPN阶段分类是二分类，而Fast RCNN阶段是21分类(voc)\n\n2\nRPN会产生大约2000个RoIs，这2000个RoIs不是都拿去训练，而是选择128个RoIs用以训练。选择的规则如下：\n\nRoIs和gt_bboxes 的IoU大于0.5的，选择一些（比如32个）\n选择 RoIs和gt_bboxes的IoU小于等于0（或者0.1）的选择一些（比如 128-32=96个）作为负样本\n\n为了便于训练，对选择出的128个RoIs，还对他们的gt_roi_loc 进行标准化处理（减去均值除以标准差）\n","slug":"Faster-RCNN-记录","date":"2021-10-11T07:47:29.000Z","categories_index":"","tags_index":"detection","author_index":"Hulk Wang"},{"id":"7f399d762217f0410e81da356daa491f","title":"FCOS学习笔记","content":"FCOS学习笔记\n\n\n\n\n\n\n\n\n\n笔记来源：https://blog.csdn.net/WZZ18191171661/article/details/89258086 https://zhuanlan.zhihu.com/p/339023466\nFCOS是一个基于FCN的per-pixel、anchor free的one-stage目标检测算法，论文全:《FCOS: Fully Convolutional One-Stage Object Detection》\nAnchor-based不足：\n\nanchor会引入很多需要优化的超参数， 比如anchor number、anchor size、anchor ratio等；\n为了保证算法效果，需要很多的anchors，存在正负样本类别不均衡问题；\n在训练的时候，需要计算所有anchor box同ground truth boxes的IoU，计算量较大；\n\nFCOS优势： 1. 因为输出是pixel-based预测，所以可以复用semantic segmentation方向的相关tricks； 2. 可以修改FCOS的输出分支，用于解决instance segmentation和keypoint detection任务；\n\n\nFCOS网络结构\n\n实现细节\n与Anchor Base对比\n对于基于anchors的目标检测算法而言，我们将输入的图片送入backbone网络之后，会获得最终的feature_map，比如说是17x17x256；然后我们会在该feature_map上的每一位置上使用预先定义好的anchors。而FCOS的改动点就在这里，它是直接在feature_map上的每一点进行回归操作。\n具体的实施思路如下所示： 1. 我们可以将feature_map中的每一个点(x,y)映射回原始的输入图片中: (⌊s/2⌋ + xs, ⌊s/2⌋ + ys) 其中: s为步长，(x,y)为改点对应feature map上的坐标.\n\n如果这个映射回原始输入的点在相应的GT的bbox范围之内，而且类别标签对应，我们将其作为训练的正样本块，否则将其作为正样本块；\n回归的目标是(l,t,r,b)，即中心点做bbox的left、top、right和bottom之间的距离，具体如下图所示：\n\n如果一个位置在多个bbox的内部的话，如右图，针对这样样本文中采样的方法是直接选择择面积最小的边界框作为其回归目标。由于网络中FPN的存在，导致这样的模糊样本的数量大大减少。\n如果这个位置(x,y)和一个bbox关联的话，该位置处的训练回归目标可制定为:其中(x1,y1)和(x2,y2)分别表示bbox的左上角和右下角坐标值。\n\n由于FCOS可以通过这样方式获得很多正样本块，使用这样的正样本块进行回归操作，因此获得了比较好的性能提升，而原始的基于anchor的算法需要通过计算预设的anchor和对应的GT之间的IOU值，当该IOU值大于设定的阈值时才将其看做正样本块。\n\nLoss\n\nloss函数如上图所示，包含两部分，Lcls表示分类loss，本文使用的是Focal_loss；Lreg表示回归loss，本文使用的是IOU loss。\ncenter-ness分支\nCenter-ness表示的是(x,y)距目标中心的标准化后的距离，为了制止过多的低质量离目标中心远的检测框而设计。\n\n\n如上图，红色到蓝色表示center-ness从1到0，因为center-ness是在0-1之间，所以用的BCE loss，这个loss会一起加到上面我们提到的loss function中。在测试时，检测框的排序分数由center-ness乘上分类的分数。如果还有低质量的框，最后可用NMS来剔除。\n","slug":"FCOS学习笔记","date":"2021-09-13T08:19:35.000Z","categories_index":"","tags_index":"detection","author_index":"Hulk Wang"},{"id":"57d3aa7cdcc56700d35ee7aa4e8c6558","title":"YOLO-V5学习笔记","content":"YOLO-V5学习笔记\n\n\n\n\n\n\n\n\n\n知识点来源于网络，仅记录学习 来源：https://zhuanlan.zhihu.com/p/172121380\n网络结构\n\n\nYolov5s网络结构(来源见水印)\n\n如上图为yolov5的整体网络结构，跟yolov4一样，分别按input、backbone、Neck以及Prediction四部分来理解。\n\n\n\n\n\n\n\n\n\nYolov5官方代码中，一共有4个版本，分别是Yolov5s、Yolov5m、Yolov5l、Yolov5x四个模型。Yolov5s是Yolov5系列中深度最小，特征图的宽度最小的网络。后面的3种都是在此基础上不断加深，不断加宽。\n输入端\nMosaic数据增强\n与v4一样，采用Mosaic数据增强；\n自适应锚框计算\n将anchor初始计算(聚类)集成到训练代码中；\n自适应图片缩放\n针对inference阶段的优化\n在常用的目标检测算法中，不同的图片长宽都不相同，因此常用的方式是将原始图片统一缩放到一个标准尺寸，再送入检测网络中。\n\n\n传统方法(来源见水印)\n\n但Yolov5代码中对此进行了改进，也是Yolov5推理速度能够很快的一个不错的trick。\n作者认为，在项目实际使用时，很多图片的长宽比不同，因此缩放填充后，两端的黑边大小都不同，而如果填充的比较多，则存在信息冗余，影响推理速度。\n因此在Yolov5的代码中datasets.py的letterbox函数中进行了修改，对原始图像自适应的添加最少的黑边。\n\n\nyolov5(来源见水印)\n\n举例说明填充方法： 原始：800x600 目标：416\n\n选择小的缩放系数，短边:min(416/800, 416/600);\n得到新的尺寸(即长边resize到目标尺寸,短边按原始长宽比变换）: (416,312);\n计算pad大小(找到大于312且能被32整除的最小整数): (416 - 312) mod 32 = 8 所以pad值为8/2=4\n\nBackbone\nFocus结构\n\n\nfocus(来源见水印)\n\nyolov5中，Focus模块位于backbone前。具体操作是在一张图片中每隔一个像素拿到一个值，类似于邻近下采样，这样就拿到了四张近似下采样的图片，但是没有信息丢失。相当于w,h变为1/2，输入通道扩充了4倍，最后将得到的新图片再经过卷积操作，最终得到了没有信息丢失情况下的二倍下采样特征图。\n以yolov5s为例，原始的640 × 640 × 3的图像输入Focus结构，采用切片操作，先变成320 × 320 × 12的特征图，再经过一次卷积操作，最终变成320 × 320 × 32的特征图。\n具体代码实现：\n\n目的和作用： Focus是为了提速，和mAP无关，减少了计算量和参数量。\nThe YOLOv5 Focus layer replaces the first 3 YOLOv3 layers with a single layer:\n\n详见作者解答\nCSP结构\nYolov5与Yolov4不同点在于，Yolov4中只有主干网络使用了CSP结构。\n而Yolov5中设计了两种CSP结构，以Yolov5s网络为例，CSP1_X结构应用于Backbone主干网络，另一种CSP2_X结构则应用于Neck中。\n\nNeck\n\nYolov5现在的Neck和Yolov4中一样，都采用FPN+PAN的结构.\n如CSP结构中讲到，Yolov5和Yolov4的不同点在于，Yolov4的Neck结构中，采用的都是普通的卷积操作。而Yolov5的Neck结构中，采用借鉴CSPnet设计的CSP2结构，加强网络特征融合的能力。\n\n\nPrediction\nBounding box损失函数\nYolov5: GIOU_Loss Yolov4: CIOU_Loss\nnms非极大值抑制\nYolov4: DIOU_nms Yolov5: 加权nms\n\n\n\n\n\n\n\n\n\nWeighted NMS出现于ICME Workshop 2017《Inception Single Shot MultiBox Detector for object detection》一文中。论文认为Traditional NMS每次迭代所选出的最大得分框未必是精确定位的，冗余框也有可能是定位良好的。那么与直接剔除机制不同，Weighted NMS顾名思义是对坐标加权平均，加权平均的对象包括M自身以及IoU≥NMS阈值的相邻框。  加权的权重为 :   ，表示得分与IoU的乘积。 优点： Weighted NMS通常能够获得更高的Precision和Recall，只要NMS阈值选取得当，Weighted NMS均能稳定提高AP与AR，无论是AP50还是AP75，也不论所使用的检测模型是什么。 缺点： 顺序处理模式，且运算效率比Traditional NMS更低。加权因子是IoU与得分，前者只考虑两个框的重叠面积，这对描述box重叠关系或许不够全面；而后者受到定位与得分不一致问题的限制。\n","slug":"YOLO-V5学习笔记","date":"2021-09-09T07:42:37.000Z","categories_index":"","tags_index":"detection","author_index":"Hulk Wang"},{"id":"f45a749a54e27d0a1f22b1f23eadc80a","title":"Pixel-Level Domain Transfer论文复现","content":"Pixel-Level Domain Transfer论文复现\n博客迁移，原文链接\n\n\n\n\n\n\n\n\n\nAbstract.：We present an image-conditional image generation model. The model transfers an input domain to a target domain in semantic level, and generates the target image in pixel level. To generate realistic target images, we employ the real/fake-discriminator as in Generative Adversarial Nets, but also introduce a novel domain-discriminator to make the generated image relevant to the input image. We verify our model through a challenging task of generating a piece of clothing from an input image of a dressed person. We present a high quality clothing dataset containing the two domains, and succeed in demonstrating decent results.\n论文简述\n整篇论文比较容易懂，主要内容就是把输入domain转换到目标domain，输入一张模特图片，得到上衣图片，如下：\n\n文章主要贡献主要在两个方面：\nLookBook数据集\n下载地址（uj3j）\n\n基于Gan的转换框架\n网络结构如下：\n\n生成网络是encoder-decoder结构，判别网络有两个：Dr和Da。\nDr就是一个基本的Gan的判别网络，判别fake或real；Da主要用来判断生成图像与输入是否配对，所以Dr输入是生成网络的输入和输出的concat.\n整个过程很容易懂，细节看原文即可.\n论文复现\nGenerator：\n输入64x64x3图像，输出64x64x3生成图像\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n \n        def conv_block(in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, bn=True, a_func='lrelu'):\n \n            block = nn.ModuleList()\n            block.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func == 'lrelu':\n                block.append(nn.LeakyReLU(0.2))\n            elif a_func == 'relu':\n                block.append(nn.ReLU())\n            else:\n                pass\n \n            return block\n \n        def convTranspose_block(in_channels, out_channels, kernel_size, stride=2,\n                 padding=0, output_padding=0, bn=True, a_func='relu'):\n            '''\n            H_out = (H_in - 1) * stride - 2 * padding + kernel_size + output_padding\n            :param in_channels:\n            :param out_channels:\n            :param kernel_size:\n            :param stride:\n            :param padding:\n            :param output_padding:\n            :param bn:\n            :param a_func:\n            :return:\n            '''\n            block = nn.ModuleList()\n            block.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride,\n                 padding, output_padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func == 'lrelu':\n                block.append(nn.LeakyReLU(0.2))\n            elif a_func == 'relu':\n                block.append(nn.ReLU())\n            else:\n                pass\n \n            return block\n \n \n        def encoder():\n            conv_layer = nn.ModuleList()\n            conv_layer += conv_block(3, 128, 5, 2, 2, False)    # 32x32x128\n            conv_layer += conv_block(128, 256, 5, 2, 2)        # 16x16x256\n            conv_layer += conv_block(256, 512, 5, 2, 2)         # 8x8x512\n            conv_layer += conv_block(512, 1024, 5, 2, 2)       # 4x4x1024\n            conv_layer += conv_block(1024, 64, 4, 1)          # 1x1x64\n            return conv_layer\n \n        def decoder():\n            conv_layer = nn.ModuleList()\n            conv_layer += conv_block(64, 4 * 4 * 1024, 1, a_func='relu')\n            conv_layer.append(Reshape((1024, 4, 4)))                            # 4x4x1024\n            conv_layer += convTranspose_block(1024, 512, 4, 2, 1)               # 8x8x512\n            conv_layer += convTranspose_block(512, 256, 4, 2, 1)                # 16x16x256\n            conv_layer += convTranspose_block(256, 128, 4, 2, 1)                # 32x32x128\n            conv_layer += convTranspose_block(128, 3, 4, 2, 1, bn=False, a_func='')     # 64x64x3\n            conv_layer.append(nn.Tanh())\n            return conv_layer\n \n        self.net = nn.Sequential(\n            *encoder(),\n            *decoder(),\n        )\n \n    def forward(self, input):\n        out = self.net(input)\n        return out\nDiscriminatorR\n输入64x64x3图像，输出real or fake；\nclass DiscriminatorR(nn.Module):\n    def __init__(self):\n        super(DiscriminatorR, self).__init__()\n \n        def conv_block(in_channels, out_channels, kernel_size, stride=1,\n                       padding=0, bn=True, a_func=True):\n \n            block = nn.ModuleList()\n            block.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func:\n                block.append(nn.LeakyReLU(0.2))\n \n            return block\n \n \n        self.net = nn.Sequential(\n            *conv_block(3, 128, 5, 2, 2, False),                            # 32x32x128\n            *conv_block(128, 256, 5, 2, 2),                                 # 16x16x256\n            *conv_block(256, 512, 5, 2, 2),                                 # 8x8x512\n            *conv_block(512, 1024, 5, 2, 2),                                # 4x4x1024\n            *conv_block(1024, 1, 4, bn=False, a_func=False),                # 1x1x1\n            nn.Sigmoid(),\n        )\n \n    def forward(self, img):\n        out = self.net(img)\n        return out\nDiscriminatorA\n输入64x64x6的concat图像，输出real or fake；\nclass DiscriminatorA(nn.Module):\n    def __init__(self):\n        super(DiscriminatorA, self).__init__()\n \n        def conv_block(in_channels, out_channels, kernel_size, stride=1,\n                       padding=0, bn=True, a_func=True):\n \n            block = nn.ModuleList()\n            block.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func:\n                block.append(nn.LeakyReLU(0.2))\n \n            return block\n \n        self.net = nn.Sequential(\n            *conv_block(6, 128, 5, 2, 2, False),                # 32x32x128\n            *conv_block(128, 256, 5, 2, 2),                     # 16x16x256\n            *conv_block(256, 512, 5, 2, 2),                     # 8x8x512\n            *conv_block(512, 1024, 5, 2, 2),                    # 4x4x1024\n            *conv_block(1024, 1, 4, bn=False, a_func=False),    # 1x1x1\n            nn.Sigmoid(),\n        )\n \n    def forward(self, img):\n        out = self.net(img)\n        return out\nloss\n与原文不同，在生成损失上加了mse\ngen_loss_d = self.adversarial_loss(torch.squeeze(gen_output), real_label)\ngen_loss_a = self.adversarial_loss(torch.squeeze(gen_output_a), real_label)\nmse_loss = self.mse_loss(gen_target_batch, target_batch)\n完整训练测试代码：GitHub\n结果\ntensorboard\n\n训练过程可视化\n\n验证集\n\n","slug":"Pixel-Level-Domain-Transfer论文复现","date":"2021-09-06T11:51:46.000Z","categories_index":"","tags_index":"csdn迁移","author_index":"Hulk Wang"},{"id":"260edd383abab8c5258536a42ddb3e2a","title":"YOLO-V4学习笔记","content":"YOLO-V4学习笔记\n\n\n\n\n\n\n\n\n\n知识点来源于网络，仅记录学习 来源：https://zhuanlan.zhihu.com/p/143747206\nyolov4-TT100k\n\n使用yolov4训练的交通标志检测\n\n网络结构\n\n\n网络结构(来源见水印)\n\n五个基本组件: 1. CBM：Yolov4网络结构中的最小组件，由Conv+Bn+Mish激活函数三者组成。 2. CBL：由Conv+Bn+Leaky_relu激活函数三者组成。 3. Res unit：借鉴Resnet网络中的残差结构，让网络可以构建的更深。 4. CSPX：借鉴CSPNet网络结构，由卷积层和X个Res unint模块Concate组成。 5. SPP：采用1×1，5×5，9×9，13×13的最大池化的方式，进行多尺度融合。\n\n\nObject detector\n\n如上图，大致分为四个阶段理解yolov4，分别为输入端、backbone、Neck以及Prediction。\n输入端\nMosaic数据增强\nYolov4中使用的Mosaic是参考2019年底提出的CutMix数据增强的方式，但CutMix只使用了两张图片进行拼接，而Mosaic数据增强则采用了4张图片，随机缩放、随机裁剪、随机排布的方式进行拼接。\n\n为什么要进行Mosaic数据增强?\n\n\n\n\n\n\n\n\n\n在平时项目训练时，小目标的AP一般比中目标和大目标低很多。而Coco数据集中也包含大量的小目标，但比较麻烦的是小目标的分布并不均匀。\n首先看下小、中、大目标的定义： 2019年发布的论文《Augmentation for small object detection》对此进行了区分:\n\n\n\n\n最小矩形区域面积\n最大矩形区域面积\n\n\n\n\n小目标\n0 * 0\n32 * 32\n\n\n中目标\n32 * 32\n96 * 96\n\n\n大目标\n96 * 96\n∞ * ∞\n\n\n\n\n\n\n\n\n\n\n\n\n可以看到小目标的定义是目标框的长宽0×0~32×32之间的物体。\n\n\n\n\n小\n中\n大\n\n\n\n\n数据集中小目标占比\n41.4%\n34.3%\n24.3%\n\n\n数据集图片包含占比\n52.3%\n70.7%\n83.0%\n\n\n\n但在整体的数据集中，小、中、大目标的占比并不均衡。 如上表所示，Coco数据集中小目标占比达到41.4%，数量比中目标和大目标都要多。\n但在所有的训练集图片中，只有52.3%的图片有小目标，而中目标和大目标的分布相对来说更加均匀一些。\n针对这种状况，Yolov4的作者采用了Mosaic数据增强的方式。\n主要有几个优点：\n\n丰富数据集：随机使用4张图片，随机缩放，再随机分布进行拼接，丰富了图片的背景，大大丰富了检测数据集，特别是随机缩放增加了很多小目标，让网络的鲁棒性更好。\n减少GPU：四张图片拼接在一起变相地提高了batch_size，在进行batch normalization的时候也会计算四张图片，所以对本身batch_size不是很依赖，单块GPU就可以训练YOLOV4。\n\n\n\n\n\n\n\n\n\n\n最后说明一下对于标签框的处理，当进行裁剪的时候，如果裁剪了样本当中的标签框的部分区域，则将其舍弃，保留裁剪之后还完整的标签框。\n参考\n与其他增强方法对比\n\nMixup:将随机的两张样本按比例混合，分类的结果按比例分配；\nCutout:随机的将样本中的部分区域cut掉，并且填充0像素值，分类的结果不变；\nCutMix:就是将一部分区域cut掉但不填充0像素而是随机填充训练集中的其他数据的区域像素值，分类结果按一定的比例分配\n\n\n上述三种数据增强的区别： 1. cutout和cutmix就是填充区域像素值的区别; 2. mixup和cutmix是混合两种样本方式上的区别; 3. mixup是将两张图按比例进行插值来混合样本，cutmix是采用cut部分区域再补丁的形式去混合图像，不会有图像混合后不自然的情形。\n参考\nBackBone\nCSPDarknet53\n\n\n\n\n\n\n\n\n\nCSPDarknet53是在Yolov3主干网络Darknet53的基础上，借鉴2019年CSPNet的经验，产生的Backbone结构。因为Backbone有5个CSP模块（见网络结构），输入图像是608 * 608，所以特征图变化的规律是：608-&gt;304-&gt;152-&gt;76-&gt;38-&gt;19。经过5次CSP模块后得到19*19大小的特征图。 而且作者只在Backbone中采用了Mish激活函数，网络后面仍然采用Leaky_relu激活函数。\nCSPNet\n论文地址\nCSPNet全称是Cross Stage Paritial Network，主要从网络结构设计的角度解决推理中从计算量很大的问题。\n设计CSPNet的主要目的是使该体系结构能够实现更丰富的梯度组合信息，同时减少计算量。 通过将基础层的特征图划分为两个部分，然后通过提出的跨阶段层次结构将它们合并，可以实现此目标。\nCSPNet的作者认为推理计算过高的问题是由于网络优化中的梯度信息重复导致的。\n因此采用CSP模块先将基础层的特征映射划分为两部分，然后通过跨阶段层次结构将它们合并，在减少了计算量的同时可以保证准确率。\n因此Yolov4在主干网络Backbone采用CSPDarknet53网络结构，主要有三个方面的优点：\n\n增强CNN的学习能力，使得在轻量化的同时保持准确性\n降低计算瓶颈\n降低内存成本\n\n论文方法\n\n如上图(a), DenseNet的每个阶段均包含一个dense block(密集连接层)和transition layer(过渡层)，同时，每个dense block由K个密集层连接。第个密集层的输出将会同第个密集层的输入相连接，同时拼接后的输出结果作为个密集层的输入，等式表示如下所示：\n\n如果使用反向传播算法更新权重，则权重更新方程可写为：\n\n其中f是权重更新的函数，表示传播到第个密集层的梯度。 作者发现大量重复的梯度信息被用来更新不同密集层的权重。 这将导致不同的密集层重复学习复制的梯度信息。\n如上图(b),改进点在于CSPNet将浅层特征映射为两个部分，一部分经过Dense模块（图中的Partial Dense Block）,另一部分直接与Partial Dense Block输出进行concate。CSPDenseNet的前馈传递和权重更新的方程式分别显示如下：\n\n\n总体而言，CSPDenseNet保留了DenseNet的特征重用特性的优点，但同时通过截断梯度流，防止了过多的梯度信息重复。 通过设计分层特征融合策略来实现此思想，并将其用于部分过渡层。\n作者也设计了几种特征融合的策略，如下图所示： \nFustion First的方式是对两个分支的feature map先进行concatenation操作，这样梯度信息可以被重用。 Fusion Last的方式是对Dense Block所在分支先进性transition操作，然后再进行concatenation， 梯度信息将被截断，因此不会重复使用梯度信息 。\n\n\n\n\n\n\n\n\n\n所以CSP-DarkNet到底是怎么借鉴CSPNet的？ 如果按照CSPNet的思想，那特征输入应该按一定比例分为两路，分别经过Part1和Part2后concat，比如下图这样：\n\n\n来源见水印\n\n\n\n\n\n\n\n\n\n\n但实际CSP-DarkNet没有做split操作，Part1和Part2输入的是全部特征，如下图：\n\n\n来源见水印\n\n\n\n\n\n\n\n\n\n\n直接用两路的1x1卷积将输入特征进行变换。 可以理解的是，将全部的输入特征利用两路1x1进行transition，比直接划分通道能够进一步提高特征的重用性，并且在输入到resiudal block之前也确实通道减半，减少了计算量。\n参考来源 ##### Mish激活函数\n论文地址\n\n\nMish曲线\n\ny = x * tanh(ln(1+exp(x)))\n\n\n\n\n\n\n\n\n\n一种自正则的非单调神经激活函数，平滑的激活函数允许更好的信息深入神经网络，从而得到更好的准确性和泛化。论文中提出，相比Swish有0.494%的提升，相比ReLU有1.671%的提升。\n\n\n\n\n\n\n\n\n\nYolov4的Backbone中都使用了Mish激活函数，而后面的网络则还是使用leaky_relu函数。\n优点：\n\n以上无边界(即正值可以达到任何高度)避免了由于封顶而导致的饱和。2. 理论上对负值的轻微允许可以产生更好的梯度流，而不是像ReLU中那样的硬零边界。\n平滑的激活函数允许更好的信息深入神经网络，从而得到更好的准确性和泛化。\n\n更平滑的激活函数允许信息更深入地流动\n缺点：\n\n计算量肯定比relu大，占用的内存也多了不少；\n\npytorch实现\n\nDropblock\n论文地址\n\n\n\n\n\n\n\n\n\nYolov4中使用的Dropblock，其实和常见网络中的Dropout功能类似，也是缓解过拟合的一种正则化方式。\ndropout方法多是作用在全连接层上，在卷积层应用dropout方法意义不大。文章认为是因为每个feature map的位置都有一个感受野范围，仅仅对单个像素位置进行dropout并不能降低feature map学习的特征范围，也就是说网络仍可以通过该位置的相邻位置元素去学习对应的语义信息，也就不会促使网络去学习更加鲁棒的特征。 既然单独的对每个位置进行dropout并不能提高网络的泛化能力，那么很自然的，如果我们按照一块一块的去dropout，就自然可以促使网络去学习更加鲁棒的特征。思路很简单，就是在feature map上去一块一块的找，进行归零操作，类似于dropout，叫做dropblock。\n\n\n绿色阴影区域是语义特征，b图是模拟dropout的做法，随机丢弃一些位置的特征,(c)是dropblock\n\n\ndropblock有三个比较重要的参数，一个是block_size，用来控制进行归零的block大小；一个是γ，用来控制每个卷积结果中，到底有多少个channel要进行dropblock；最后一个是keep_prob，作用和dropout里的参数一样。\nM大小和输出特征图大小一致，非0即1，为了保证训练和测试一致，需要和dropout一样，进行rescale。\n上述是理论分析，在做实验时候发现，**block_size控制为7*7效果最好**，对于所有的feature map都一样，γ通过一个公式来控制，keep_prob则是一个线性衰减过程，从最初的1到设定的阈值(具体实现是dropout率从0增加到指定值为止)，论文通过实验表明这种方法效果最好。如果固定prob效果好像不好。 实践中，并没有显式的设置γ的值，而是根据keep_prob(具体实现是反的，是丢弃概率)来调整。\nNeck\n\n\n\n\n\n\n\n\n\n在目标检测领域，为了更好的提取融合特征，通常在Backbone和输出层，会插入一些层，这个部分称为Neck。相当于目标检测网络的颈部，也是非常关键的。\nYolov4的Neck结构主要采用了SPP模块、FPN+PAN的方式。\nSPP模块\n\n作者在SPP模块中，使用k={1 * 1,5 * 5,9 * 9,13 * 13}的最大池化的方式，再将不同尺度的特征图进行Concat操作。\n\n\n\n\n\n\n\n\n\n采用SPP模块的方式，比单纯的使用k*k最大池化的方式，更有效的增加主干特征的接收范围，显著的分离了最重要的上下文特征\nFPN+PAN\n论文地址\n\n\n\n\n\n\n\n\n\nPath Aggregation Network(PANet)，旨在提升基于侯选区域的实例分割框架内的信息流传播。具体来讲，通过自下向上(bottom-up)的路径增强在较低层(lower layer)中准确的定位信息流，建立底层特征和高层特征之间的信息路径，从而增强整个特征层次架构。\nyolo-v3中使用FPN具体如下：\n\n\n来源见水印\n\nFPN是自顶向下的，将高层的特征信息通过上采样的方式进行传递融合，得到进行预测的特征图。\n在yolo_v4中，在FPN的基础上增加PAN，具体结构如下：\n\n\n来源见水印\n\n如上图，紫色箭头处分别是三个中间feature map，分辨率为76 * 76、38 * 38、19 * 19\n\n\n来源见水印\n\n这样结合操作，FPN层自顶向下传达强语义特征，而特征金字塔则自底向上传达强定位特征，两两联手，从不同的主干层对不同的检测层进行参数聚合。\n\n\nyolo_v4中使用的是修改的PAN\n\nPrediction\n目标检测任务的损失函数一般由Classificition Loss（分类损失函数）和Bounding Box Regeression Loss（回归损失函数）两部分构成。\nBounding Box Regeression的Loss近些年的发展过程是：Smooth L1 Loss-&gt; IoU Loss（2016）-&gt; GIoU Loss（2019）-&gt; DIoU Loss（2020）-&gt;CIoU Loss（2020）\n我们从最常用的IOU_Loss开始，进行对比拆解分析，看下Yolov4为啥要选择CIOU_Loss。\n\n\n\n\n\n\n\n\n\n记住一点：好的目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比。\nIOU_Loss\n\nIOU的loss其实很简单，主要是交集/并集，但其实也存在两个问题。\n\n\n即状态1的情况，当预测框和目标框不相交时，IOU=0，无法反应两个框距离的远近，此时损失函数不可导，IOU_Loss无法优化两个框不相交的情况。\n即状态2和状态3的情况，当两个预测框大小相同，两个IOU也相同，IOU_Loss无法区分两者相交情况的不同。\n\n因此2019年出现了GIOU_Loss来进行改进。\nGIOU_Loss\n\n可以看到上图GIOU_Loss中，增加了相交尺度的衡量方式，缓解了单纯IOU_Loss时的尴尬。 但还有一种不足，如下：\n\n问题：状态1、2、3都是预测框在目标框内部且预测框大小一致的情况，这时预测框和目标框的差集都是相同的，因此这三种状态的GIOU值也都是相同的，这时GIOU退化成了IOU，无法区分相对位置关系。 基于这个问题，2020年的AAAI又提出了DIOU_Loss。\nDIOU_Loss\n好的目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比。\n针对IOU和GIOU存在的问题，作者从两个方面进行考虑\n一：如何最小化预测框和目标框之间的归一化距离？ 二：如何在预测框和目标框重叠时，回归的更准确？\n针对第一个问题，提出了DIOU_Loss（Distance_IOU_Loss）\n\nDIOU_Loss考虑了重叠面积和中心点距离，当目标框包裹预测框的时候，直接度量2个框的距离，因此DIOU_Loss收敛的更快。 但就像前面好的目标框回归函数所说的，没有考虑到长宽比。\n\n比如上面三种情况，目标框包裹预测框，本来DIOU_Loss可以起作用。 但预测框的中心点的位置都是一样的，因此按照DIOU_Loss的计算公式，三者的值都是相同的。\nCIOU_Loss\nCIOU_Loss和DIOU_Loss前面的公式都是一样的，不过在此基础上还增加了一个影响因子，将预测框和目标框的长宽比都考虑了进去。\n\n其中v是衡量长宽比一致性的参数，我们也可以定义为：\n\n这样CIOU_Loss就将目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比全都考虑进去了。\n对比\n再来综合的看下各个Loss函数的不同点：\nIOU_Loss：主要考虑检测框和目标框重叠面积。 GIOU_Loss：在IOU的基础上，解决边界框不重合时的问题。 DIOU_Loss：在IOU和GIOU的基础上，考虑边界框中心点距离的信息。 CIOU_Loss：在DIOU的基础上，考虑边界框宽高比的尺度信息。\nYolov4中采用了CIOU_Loss的回归方式，使得预测框回归的速度和精度更高一些。\nDIOU_nms\nDIoU用作 NMS 的一个因子。该方法在抑制冗余的边界框时会使用 IoU 和两个边界框的中心点之间的距离。这能使得模型能更加稳健地应对有遮挡的情况。 在传统NMS中，IoU指标常用于抑制冗余bbox，其中重叠区域是唯一因素，对于遮挡情况经常产生错误抑制。 DIoU-NMS将DIoU作为NMS的准则，因为在抑制准则中不仅应考虑重叠区域，而且还应考虑两个box之间的中心点距离，而DIoU就是同时考虑了重叠区域和两个box的中心距离。\nDIoU-NMS建议两个中心点较远的box可能位于不同的对象上，不应将其删除(这就是DIoU-NMS的与NMS的最大不同之处)。\n\n在上图重叠的摩托车检测中，中间的摩托车因为考虑边界框中心点的位置信息，也可以回归出来。\n因此在重叠目标的检测中，DIOU_nms的效果优于传统的nms。\n\n\n\n\n\n\n\n\n\n这里为什么不用CIOU_nms，而用DIOU_nms?\n答：因为前面讲到的CIOU_loss，是在DIOU_loss的基础上，添加的影响因子，包含groundtruth标注框的信息，在训练时用于回归。 但在测试过程中，并没有groundtruth的信息，不用考虑影响因子，因此直接用DIOU_nms即可。\n","slug":"YOLO-V4学习笔记","date":"2021-09-06T10:10:36.000Z","categories_index":"","tags_index":"detection","author_index":"Hulk Wang"},{"id":"82f698705681b0bacc9c6cad3db6d88e","title":"YOLO-V3学习笔记","content":"YOLO-V3学习笔记\n\n\n\n\n\n\n\n\n\n知识点来源于论文和网络，仅记录学习\nyolov3-TT100k\n\n使用yolov3训练的交通标志检测\n\n网络结构\nBackbone\n\n整个v3结构没有池化层和全连接层\n输出特征图缩小到输入的1/32。所以，通常都要求输入图片是32的倍数\n\n\nDBL:代码中的Darknetconv2d_BN_Leaky，是yolo_v3的基本组件。就是卷积+BN+Leaky relu。 resn:n代表数字，有res1，res2, … ,res8等等，表示这个res_block里含有多少个res_unit。 concat:张量拼接。将darknet中间层和后面的某一层的上采样进行拼接。拼接的操作和残差层add的操作是不一样的，拼接会扩充张量的维度，而add只是直接相加不会导致张量维度的改变。\nOutput\nyolo v3输出了3个不同尺度的feature map，如上图所示的y1, y2, y3。借鉴了FPN(feature pyramid networks)，采用多尺度来对不同size的目标进行检测，越精细的grid cell就可以检测出越精细的物体(大分辨率y3更能检测小物体，小分辨率y1更能检测大物体)。\ny1,y2和y3的深度都是255，边长分别为13:26:52。 对于COCO类别而言，有80个种类，所以每个box应该对每个种类都输出一个概率。\nyolo v3设定的是每个网格单元预测3个box，所以每个box需要有(x, y, w, h, confidence)五个基本参数，然后还要有80个类别的概率。所以3*(5 + 80) = 255。这个255就是这么来的。 v3用上采样的方法来实现这种多尺度的feature map，concat连接的两个张量是具有一样尺度的(两处拼接分别是26x26尺度拼接和52x52尺度拼接，通过(2, 2)上采样来保证concat拼接的张量尺度相同)。作者并没有像SSD那样直接采用backbone中间层的处理结果作为feature map的输出，而是和后面网络层的上采样结果进行一个拼接之后的处理结果作为feature map。\nBounding Box\n在Yolov1中，网络直接回归检测框的宽、高，这样效果有限。所以在Yolov2中，改为了回归基于先验框的变化值，这样网络的学习难度降低，整体精度提升不小。Yolov3沿用了Yolov2中关于先验框的技巧，并且使用k-means对数据集中的标签框进行聚类，得到类别中心点的9个框，作为先验框。在COCO数据集中（原始图片全部resize为416 × 416），九个框分别是 (10×13)，(16×30)，(33×23)，(30×61)，(62×45)，(59× 119)， (116 × 90)， (156 × 198)，(373 × 326) ，顺序为w × h。\nfeature map中的每一个cell都会预测3个边界框（bounding box） ，每个bounding box都会预测三个东西： 1. 每个框的位置（4个值，中心坐标tx和ty，框的高度bh和宽度bw） 2. 一个objectness prediction 3. N个类别\n三个output，每个对应的感受野不同，32倍降采样的感受野最大，适合检测大的目标，所以在输入为416×416时，每个cell的三个anchor box为(116 ,90); (156 ,198); (373 ,326)。16倍适合一般大小的物体，anchor box为(30,61); (62,45); (59,119)。8倍的感受野最小，适合检测小目标，因此anchor box为(10,13); (16,30); (33,23)。所以当输入为416×416时，实际总共有（52×52+26×26+13×13）×3=10647个proposal box。\n\n\n\n\n\n\n\n\n\n特征图\n13x13\n26x26\n52x52\n\n\n\n\n感受野\n大\n中\n小\n\n\n先验框\n(116 ,90)(156 ,198)(373 ,326)\n(30,61) (62,45)(59,119)\n(10,13)(16,30)(33,23)\n\n\n\n\n9种尺寸的先验框，图中蓝色框为聚类得到的先验框。黄色框式ground truth，红框是对象中心点所在的网格(来源见水印)\n\n\n\n\n\n\n\n\n\n\n这里注意bounding box 与anchor box的区别： Bounding box它输出的是框的位置（中心坐标与宽高），confidence以及N个类别。anchor box只是一个尺度即只有宽高。\nOutput Decode\nBounding box decode\n如上一节所说，v2开始，回归基于先验框的变化值，因此可以通过以下公式解码检测框的x，y，w，h.\n\n如下图，、是基于矩形框中心点左上角格点坐标的偏移量, 是激活函数，论文中作者使用sigmoid, 是先验框的宽、高，通过上述公式，计算出实际预测框的宽高 .\n\n\n\n\n\n\n\n\n\n\n得到对应的后, 还需要乘以特征图对应的的采样率(32,16,8)，得到真实的检测框x,y\nobjectness score decode\n物体的检测置信度，在Yolo设计中非常重要，关系到算法的检测正确率与召回率。 置信度在输出85维中占固定一位，由sigmoid函数解码即可，解码之后数值区间在[0，1]中。\nlogistic回归用于对anchor包围的部分进行一个目标性评分(objectness score)，即这块位置是目标的可能性有多大。这一步是在predict之前进行的，可以去掉不必要anchor，可以减少计算量。作者在论文种的描述如下:\n\n\n\n\n\n\n\n\n\nIf the bounding box prior is not the best but does overlap a ground truth object by more than some threshold we ignore the prediction, following[17]. We use the threshold of 0.5. Unlike [17] our system only assigns one bounding box prior for each ground truth object.\n如果模板框不是最佳的即使它超过我们设定的阈值，我们还是不会对它进行predict。不同于faster R-CNN的是，yolo_v3只会对1个prior进行操作，也就是那个最佳prior。而logistic回归就是用来从9个anchor priors中找到objectness score(目标存在可能性得分)最高的那一个。logistic回归就是用曲线对prior相对于 objectness score映射关系的线性建模。\nClass Prediction decode\nCOCO数据集有80个类别，所以类别数在85维输出中占了80维，每一维独立代表一个类别的置信度。使用sigmoid激活函数替代了Yolov2中的softmax，取消了类别之间的互斥，可以使网络更加灵活。\n总结\n\n9个anchor会被三个输出张量平分的。根据大中小三种size各自取自己的anchor。\n作者使用了logistic回归来对每个anchor包围的内容进行了一个目标性评分(objectness score)。 根据目标性评分来选择anchor prior进行predict，而不是所有anchor prior都会有输出。\n\n训练策略\n\n\n\n\n\n\n\n\n\nYOLOv3 predicts an objectness score for each bounding box using logistic regression. This should be 1 if the bounding box prior overlaps a ground truth object by more than any other bounding box prior. If the bounding box prior is not the best but does overlap a ground truth object by more than some threshold we ignore the prediction, following [17]. We use the threshold of .5. Unlike [17] our system only assigns one bounding box prior for each ground truth object. If a bounding box prior is not assigned to a ground truth object it incurs no loss for coordinate or class predictions, only objectness.\n预测框一共分为三种情况：正例（positive）、负例（negative）、忽略样例（ignore）。\n正例：任取一个ground truth，与4032个框全部计算IOU，IOU最大的预测框，即为正例。并且一个预测框，只能分配给一个ground truth。例如第一个ground truth已经匹配了一个正例检测框，那么下一个ground truth，就在余下的4031个检测框中，寻找IOU最大的检测框作为正例。ground truth的先后顺序可忽略。正例产生置信度loss、检测框loss、类别loss。预测框为对应的ground truth box标签（需要反向编码，使用真实的x、y、w、h计算出 ）；类别标签对应类别为1，其余为0；置信度标签为1。\n忽略样例：正例除外，与任意一个ground truth的IOU大于阈值（论文中使用0.5），则为忽略样例。忽略样例不产生任何loss。\n负例：正例除外（与ground truth计算后IOU最大的检测框，但是IOU小于阈值，仍为正例），与全部ground truth的IOU都小于阈值（0.5），则为负例。负例只有置信度产生loss，置信度标签为0。\nLoss\nYolov3 Loss为三个特征图Loss之和：\n\n\n\n为权重常数，控制检测框Loss、obj置信度Loss、noobj置信度Loss之间的比例，通常负例的个数是正例的几十倍以上，可以通过权重超参控制检测效果;\n 若是正例则输出1，否则为0； ,若是负例则输出1，否则为0；忽略样例都输出0;\nx、y、w、h使用MSE作为损失函数，也可以使用smooth L1 loss（出自Faster R-CNN）作为损失函数。smooth L1可以使训练更加平滑。置信度、类别标签由于是0，1二分类，所以使用交叉熵作为损失函数。\n\n其他\n\nground truth为什么不按照中心点分配对应的预测box？ &gt;在Yolov3的训练策略中，不再像Yolov1那样，每个cell负责中心落在该cell中的ground truth。原因是Yolov3一共产生3个特征图，3个特征图上的cell，中心是有重合的。训练时，可能最契合的是特征图1的第3个box，但是推理的时候特征图2的第1个box置信度最高。所以Yolov3的训练，不再按照ground truth中心点，严格分配指定cell，而是根据预测值寻找IOU最大的预测框作为正例。\n为什么有忽略样例？\n\n忽略样例是Yolov3中的点睛之笔。由于Yolov3使用了多尺度特征图，不同尺度的特征图之间会有重合检测部分。比如有一个真实物体，在训练时被分配到的检测框是特征图1的第三个box，IOU达0.98，此时恰好特征图2的第一个box与该ground truth的IOU达0.95，也检测到了该ground truth，如果此时给其置信度强行打0的标签，网络学习效果会不理想。\n本身正负样本比例就不均衡（负例&gt;正例），如果强行标为0，会使不均衡更严重。\n\n\n","slug":"YOLO-V3学习笔记","date":"2021-08-30T11:59:09.000Z","categories_index":"","tags_index":"detection","author_index":"Hulk Wang"},{"id":"2a4b32b81021e06bffb6e540079ceefc","title":"hexo+github 搭建个人博客","content":"hexo+github搭建个人博客\n\n\n\n\n\n\n\n\n\n搭建环境:macOs 11.4 环境依赖:\n&gt; * git\n&gt; * npm\n&gt; * node\n&gt; * hexo\nhexo安装\n\n安装node brew install node\n安装hexo npm install -g hexo-cli\n查看hexo版本 hexo -v \n\n建站\n\n\n\n\n\n\n\n\n\n安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件\n\n初始化hexo框架 hexo init &lt;folder&gt;\n移动到目标目录 cd &lt;folder&gt;\n安装依赖组件 npm install\n生成静态文件 hexo g\n开启本地服务器 hexo s\n\n\n\n\n\n\n\n\n\n\n在浏览器中输入 http://localhost:4000 回车就可以预览效果了\n更换主题\n\n\n\n\n\n\n\n\n\n此时博客是hexo默认主题，比较普通，这里推荐一个主题：Aurora\n安装教程\n效果预览\n配置github\n\n建立respository repository名称为username.github.io\n修改配置文件 &gt; _config.yml文件\n\ndeploy:  \n\ttype: git \n\trepository: https:&#x2F;&#x2F;github.com&#x2F;username&#x2F;username.github.io.git\n\tbranch: master\n\n安装一个部署插件 npm install hexo-deployer-git --save\n重新生成部署 hexo g -d\n\n\n\n\n\n\n\n\n\n\n此时可通过 https://username.github.io 访问博客\n配置个性域名\n\n\n\n\n\n\n\n\n\n这里我购买了腾讯云的域名: hulk.show\n\n配置域名 &gt; 进入域名管理界面，选择解析，添加两条解析： \n配置git &gt; 你的项目-&gt;Setting-&gt;Pages-&gt;Custom domain,添加你的域名： \n\n\n\n\n\n\n\n\n\n\n可能需要等几分钟,即可通过购买的域名访问博客： www.hulk.show\n","slug":"hexo-github-搭建个人博客","date":"2021-08-29T10:50:28.000Z","categories_index":"","tags_index":"config","author_index":"Hulk Wang"}]