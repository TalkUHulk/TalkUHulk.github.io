{"title":"妆容迁移总结","uid":"40ac598c9729991661764e8c3d733bc9","slug":"妆容迁移总结","date":"2021-11-02T15:22:22.000Z","updated":"2021-11-03T09:55:29.475Z","comments":true,"path":"api/articles/妆容迁移总结.json","keywords":null,"cover":[],"content":"<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>先上一个自己简单优化的效果,对非正脸/五官有遮挡的图效果待优化,在真实图片上测试，比几个开源模型效果好</p></blockquote>\n<p><img src=\"/images/beauty/selfresult.jpg\" width=\"50%\" height=\"50%\"></p>\n<h3 id=\"beautygan\">BeautyGan</h3>\n<p><a href=\"http://colalab.org/media/paper/BeautyGAN-camera-ready.pdf\">BeautyGAN: Instance-level Facial Makeup Transfer with Deep Generative Adversarial Network</a></p>\n<ul>\n<li><p>化妆风格是因人而异的，需要在instance-level进行迁移。然而GAN网络主要是应用于域级的迁移，比如CycleGAN，强调域间差异而忽略域内差异。 &gt; For instance, CycleGAN realizes image-to-image translation between two collections (e.g., horses and zebras), and emphasizes inter-domain differences while omits intra-domain differences</p></li>\n<li><p>妆容风格不仅是全局的风格，还包括了每种妆独立的风格。因此，我们想extract出整体的妆容风格并且同时保留每种妆的特点是很难的。</p></li>\n</ul>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Makeup style is beyond a global style and includes independent local styles. Therefore, it is difficult to extract makeup style as a whole while preserving particular traits of various cosmetics</p></blockquote>\n<p>为了解决以上问题，作者提出了一个基于双重GAN网络的BeautyGAN模型。 &gt; To address the above issues, we propose a novel dual input/output generative adversarial network called BeautyGAN, to realize makeup style transfer in an unified framework.</p>\n<h4 id=\"framework\">Framework</h4>\n<p><img src=\"/images/beauty/beautygan_framework.jpg\" width=\"75%\" height=\"75%\"></p>\n<p>如图，主要包括一个生成器和两个判别器。</p>\n<h4 id=\"loss\">Loss</h4>\n<p>主要包括4中Loss:</p>\n<h5 id=\"adversarial-loss\">1. Adversarial Loss</h5>\n<ul>\n<li><p><span class=\"math inline\">\\(D_A\\)</span>用来区分生成的去妆容图片和真实的未化妆图片： <span class=\"math display\">\\[L_{D_A}=E_{I_{src}}[logD_A(I_{src})]+E_{I_{src},I_{ref}}[log(1-D_A(I_{src}^A))]\\]</span></p></li>\n<li><p><span class=\"math inline\">\\(D_B\\)</span>用来区分生成的化妆图片和真实的化妆图片： <span class=\"math display\">\\[L_{D_B}=E_{I_{ref}}[logD_B(I_{ref})]+E_{I_{src},I_{ref}}[log(1-D_B(I_{src}^B))]\\]</span></p></li>\n</ul>\n<p>所以:</p>\n<p><span class=\"math display\">\\[L_{adv}=L_{D_A}+L_{D_B}\\]</span></p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>对抗loss的作用是使得生成的图片更加形象逼真</p></blockquote>\n<h5 id=\"perceptual-loss\">2. perceptual Loss</h5>\n<p><span class=\"math display\">\\[L_{per}=\\frac{1}{C_l*H_l*W_l}*\\sum_{ijk}{E_l}\\]</span> <span class=\"math display\">\\[E_l=[F_l(I_{src})-F_l(I_{src}^B]_{ijk}^2+[F_l(I_{ref})-F_l(I_{ref}^A]_{ijk}^2\\]</span></p>\n<p>用<span class=\"math inline\">\\(F_l(x)\\)</span>来表示x图片在VGG网络中第l层特征,<span class=\"math inline\">\\(C_l,H_l,W_l\\)</span>分别表示feature map的通道，高度，宽度。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>perceptual loss的作用是保持原图像面部特征</p></blockquote>\n<h5 id=\"cycle-consistency-loss\">3. cycle consistency Loss</h5>\n<p>前向路径: <span class=\"math display\">\\[(I_{src},I_{ref}){\\rightarrow}G(I_{src},I_{ref}){\\rightarrow}G(G(I_{src},I_{ref})){\\approx}(I_{src},I_{ref})\\]</span></p>\n<p><span class=\"math display\">\\[L_{cyc}=E_{I_{src},I_{ref}}[dist(I_{src}^{ref},I_{src})+dist(I_{ref}^{src},I_{ref})]\\]</span></p>\n<p>The distance function dist(·)could be chosen as L1 norm, L2 norm or other metrics.</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>作用:保证妆容迁移前后图片的背景信息保持一致</p></blockquote>\n<h5 id=\"makeup-loss\">4. makeup Loss</h5>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>本文创新的地方</p></blockquote>\n<p>在妆容迁移前后要保证将颜色和面部的形态区分开，只迁移颜色而不改变形态。该模型在pixel-level上使用直方图匹配的方法(Histogram Matching)来保证输出图片与参考图片的妆容特征完全相同.</p>\n<p>首先进行Face parsing，利用PSPNet对<span class=\"math inline\">\\(I_{src},I_{ref},I_{src}^B\\)</span>图片进行分割，得到面部不同位置的mask，找到face,lip,eye(化妆位置)处理成binary mask得到<span class=\"math inline\">\\(M_{face},M_{lip},M_{eye}\\)</span>。通过mask与原图像相乘提取出图像中对于mask的区域，做基于颜色的<a href=\"https://zhuanlan.zhihu.com/p/78172123\">Histogram Matching(原理参考)</a>，得到具有相同颜色分布的H(x,y) ,其保留了图像x的内容信息以及图像y的颜色分布。最后求原图上妆后的图与直方图匹配函数的MSE即为makeup-loss。</p>\n<p><img src=\"/images/beauty/makeuploss.jpg\" width=\"50%\" height=\"50%\"></p>\n<h5 id=\"total-loss\">Total Loss</h5>\n<p><span class=\"math display\">\\[L_G={\\alpha}L_{adv}+{\\beta}L_{cyc}+{\\gamma}L_{per}+L_{makeup}\\]</span></p>\n<h4 id=\"迁移效果\">迁移效果</h4>\n<p><img src=\"/images/beauty/beautygan_result.jpg\" width=\"50%\" height=\"50%\"></p>\n<h3 id=\"psgan\">PSGAN</h3>\n<p><a href=\"https://arxiv.org/abs/1909.06956\">PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable Makeup Transfer</a></p>\n<p>实现了对不同角度和表情的妆容迁移，且可以做到控制妆容迁移的程度(浓淡)，以及控制只对特定的部位进行妆容迁移.</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Our PSGAN not only achieves state-of-the-art re- sults even when large pose and expression differences exist but also is able to perform partial and shade-controllable makeup transfer.</p></blockquote>\n<h4 id=\"framework-1\">Framework</h4>\n<p><img src=\"/images/beauty/psgan_framework.jpg\" width=\"75%\" height=\"75%\"></p>\n<p>如图，主要包括以下三个模块</p>\n<h4 id=\"mdnetmakeup-distillation-network\">MDNet(Makeup Distillation Network)</h4>\n<p>该模块可以从参考图像上提炼出两个妆容矩阵<span class=\"math inline\">\\(\\alpha\\)</span>和<span class=\"math inline\">\\(\\beta\\)</span>，其具有与原图像一样的维度。妆容矩阵<span class=\"math inline\">\\(\\alpha\\)</span>和<span class=\"math inline\">\\(\\beta\\)</span>分别是两个<span class=\"math inline\">\\(R^{1xWxH}\\)</span>的feature map, 他们由reference image分别经过1x1的卷积层产生。</p>\n<h4 id=\"ammattentive-makeup-morphing-module\">AMM(Attentive makeup morphing module)</h4>\n<p>这个模块是核心模块，该模块接收MDNet产生的妆容矩阵，将其转化为source image上对应位置的妆容矩阵<span class=\"math inline\">\\(\\alpha\\)</span>'和<span class=\"math inline\">\\(\\beta\\)</span>'(因为原来的<span class=\"math inline\">\\(\\alpha\\)</span>和<span class=\"math inline\">\\(\\beta\\)</span>是对应reference image的妆容矩阵)。这个模块解决了因source image和reference image的姿态差异而导致的不对齐问题，即实现了(pose-robust)不同角度，不同表情的妆容迁移。</p>\n<p>因为source image 和 reference image 可能会有不同的姿态和表情, 所以MDNet生成的妆容矩阵是不能直接用于source image的。因此AMM模块计算一个attentive矩阵A(如上图)， 其中<span class=\"math inline\">\\(A_{i,j}\\)</span>表示的是图x的第i个pixel <span class=\"math inline\">\\(x_i\\)</span> 与图y的第j个pixel <span class=\"math inline\">\\(y_i\\)</span> 的attentive value。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>该模块还使用68人脸关键点作为anchor points，相关看论文</p></blockquote>\n<h4 id=\"manetmakeup-apply-network\">MANet(Makeup apply network)</h4>\n<p>典型的encode-bottleneck-decode模型。该模块先对source image进行卷积做encode, 而后在bottleneck部分应用AMM产生的妆容矩阵<span class=\"math inline\">\\(\\alpha\\)</span>'和<span class=\"math inline\">\\(\\beta\\)</span>'做pixel-level的weighted multiplication 和 addition。通过调整权值与[0,1]之间可以控制妆容迁移的程度。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>the encoder part of MANet shares the same architecture with MDNet, but they do not share parameters</p></blockquote>\n<p>在bottleneck部分应用从AMM中得到的morph makeup tensor Γ' 和 B'：</p>\n<p><span class=\"math display\">\\[V_x&#39;=Γ&#39;V_x+B&#39;\\]</span></p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>这个过程类似于进行Instance Normalization（在encoder时每一层都有IN，除了最后一层是只有norm没有尺度和位移因子，因此可以直接使用scaling Γ' 和 shift B' 形成一个完整的IN）</p></blockquote>\n<h4 id=\"loss-1\">Loss</h4>\n<p>同BeautyGan</p>\n<h4 id=\"迁移效果-1\">迁移效果</h4>\n<p><img src=\"/images/beauty/psgan_result.jpg\" width=\"75%\" height=\"75%\"></p>\n<h3 id=\"cpm\">CPM</h3>\n<p><a href=\"https://arxiv.org/abs/2104.01867\">Lipstick ain’t enough: Beyond Color Matching for In-the-Wild Makeup Transfer</a></p>\n<p>不仅考虑颜色迁移，同时增加了图案迁移 &gt; In this work, we consider makeup as a combination of color transformation and pattern addition.</p>\n<p>而且引入3D人脸中的UV map，网络输入UV map.</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>\"UV\"这里是指u,v纹理贴图坐标的简称(它和空间模型的X, Y, Z轴是类似的)。 它定义了图片上每个点的位置的信息。这些点与3D模型是相互联系的, 以决定表面纹理贴图的位置。 UV就是将图像上每一个点精确对应到模型物体的表面. 在点与点之间的间隙位置由软件进行图像光滑插值处理。这就是所谓的UV贴图。</p></blockquote>\n<h4 id=\"framework-2\">Framework</h4>\n<p><img src=\"/images/beauty/CPM.jpg\" width=\"75%\" height=\"75%\"></p>\n<p>如上图，整个网络分为颜色和图案样式两个branch；</p>\n<h4 id=\"loss-2\">Loss</h4>\n<p>颜色branch与之前一样： <img src=\"/images/beauty/cpm_loss.jpg\" width=\"50%\" height=\"50%\"></p>\n<p>样式branch： <img src=\"/images/beauty/cpm_loss2.jpg\" width=\"50%\" height=\"50%\"> 如论文中所述，就是计算一个二值图的IOU；</p>\n<h4 id=\"迁移效果-2\">迁移效果</h4>\n<p><img src=\"/images/beauty/cpm_result.jpg\" width=\"50%\" height=\"50%\"></p>\n<h3 id=\"ipm\">IPM</h3>\n<p><a href=\"https://www.ijcai.org/proceedings/2020/0091.pdf\">Real-World Automatic Makeup via Identity Preservation Makeup Net</a></p>\n<p>基于MUNIT 来disentangle 不同的特征，减少了妆容迁移中对背景造成的变化，同时加入了identity-aware的loss，用的不是vgg这种general的特征提取器，而是用了person recognition中用的vgg-face网络。 解决迁移前后复杂背景和身份信息不一致的问题；</p>\n<h4 id=\"framework-3\">Framework</h4>\n<p>看着很头大的一个Framework:</p>\n<p><img src=\"/images/beauty/ipm_framework.jpg\" width=\"50%\" height=\"50%\"></p>\n<p>三个branch,上面branch送入目标妆容图片，编码得到妆容风格code;中间一个输入待上妆图片和其对应的纹理和面部mask，编码得到identity content code;最下面branch和最上面branch一样，编码得到未化妆风格code。 然后就是identity content code和style code组合，送入生成器获取对应图片了。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>如图b，inference 阶段 只需要执行前两个branch</p></blockquote>\n<h4 id=\"loss-3\">Loss</h4>\n<p>很多loss组合，对抗loss，前背景loss（根据mask），id loss（face roi），重建loss（像素级），perpetual loss...大杂烩，详细看论文代码吧。</p>\n<h4 id=\"迁移效果-3\">迁移效果</h4>\n<p>使用官方预处理的图片测试</p>\n<p><img src=\"/images/beauty/ipm_result.jpg\" width=\"50%\" height=\"50%\"></p>\n","feature":true,"text":" 先上一个自己简单优化的效果,对非正脸/五官有遮挡的图效果待优化,在真实图片上测试，比几个开源模型效果好 BeautyGan BeautyGAN: Instance-level Facial Makeup Transfer with Deep Generative Adversa...","link":"","photos":[],"count_time":{"symbolsCount":"5k","symbolsTime":"5 mins."},"categories":[],"tags":[{"name":"work summary","slug":"work-summary","count":3,"path":"api/tags/work-summary.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#beautygan\"><span class=\"toc-text\">BeautyGan</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#framework\"><span class=\"toc-text\">Framework</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#loss\"><span class=\"toc-text\">Loss</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#adversarial-loss\"><span class=\"toc-text\">1. Adversarial Loss</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#perceptual-loss\"><span class=\"toc-text\">2. perceptual Loss</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#cycle-consistency-loss\"><span class=\"toc-text\">3. cycle consistency Loss</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#makeup-loss\"><span class=\"toc-text\">4. makeup Loss</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#total-loss\"><span class=\"toc-text\">Total Loss</span></a></li></ol></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%BF%81%E7%A7%BB%E6%95%88%E6%9E%9C\"><span class=\"toc-text\">迁移效果</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#psgan\"><span class=\"toc-text\">PSGAN</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#framework-1\"><span class=\"toc-text\">Framework</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#mdnetmakeup-distillation-network\"><span class=\"toc-text\">MDNet(Makeup Distillation Network)</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#ammattentive-makeup-morphing-module\"><span class=\"toc-text\">AMM(Attentive makeup morphing module)</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#manetmakeup-apply-network\"><span class=\"toc-text\">MANet(Makeup apply network)</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#loss-1\"><span class=\"toc-text\">Loss</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%BF%81%E7%A7%BB%E6%95%88%E6%9E%9C-1\"><span class=\"toc-text\">迁移效果</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#cpm\"><span class=\"toc-text\">CPM</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#framework-2\"><span class=\"toc-text\">Framework</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#loss-2\"><span class=\"toc-text\">Loss</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%BF%81%E7%A7%BB%E6%95%88%E6%9E%9C-2\"><span class=\"toc-text\">迁移效果</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#ipm\"><span class=\"toc-text\">IPM</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#framework-3\"><span class=\"toc-text\">Framework</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#loss-3\"><span class=\"toc-text\">Loss</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%BF%81%E7%A7%BB%E6%95%88%E6%9E%9C-3\"><span class=\"toc-text\">迁移效果</span></a></li></ol></li></ol>","author":{"name":"Hulk Wang","slug":"blog-author","avatar":"/images/avatar_small.jpg","link":"https://github.com/TalkUHulk","description":"学习、记录、总结<br />kill拖延症<br /> 不生产知识，只是知识的搬运工<br /> <img src=\"/images/funny.gif\" height=\"240\" width=\"360\"/>","socials":{"github":"https://github.com/TalkUHulk","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/311127773","zhihu":"https://www.zhihu.com/people/MisterAntebellum","csdn":"https://blog.csdn.net/hyqwmxsh","juejin":"","customs":{}}},"mapped":true,"prev_post":{},"next_post":{"title":"无监督对比学习(Contrastive LearningOC)","uid":"84b6109cde20e5bfc02c6af7734d88cb","slug":"无监督对比学习-Contrastive-LearningOC","date":"2021-11-02T03:39:11.000Z","updated":"2021-11-02T12:13:26.073Z","comments":true,"path":"api/articles/无监督对比学习-Contrastive-LearningOC.json","keywords":null,"cover":[],"text":" 推荐阅读 对比学习 原理: 输入N个图片，用不同的数据增强方法为每个图片生成两个view，分别对它们编码得到y和y'。我们对上下两批表示两两计算cosine，得到NxN的矩阵，每一行的对角线位置代表y和y'的相似度，其余代表y和N-1个负例的相似度。 对每一行做softmax分...","link":"","photos":[],"count_time":{"symbolsCount":"2.8k","symbolsTime":"3 mins."},"categories":[],"tags":[{"name":"work summary","slug":"work-summary","count":3,"path":"api/tags/work-summary.json"}],"author":{"name":"Hulk Wang","slug":"blog-author","avatar":"/images/avatar_small.jpg","link":"https://github.com/TalkUHulk","description":"学习、记录、总结<br />kill拖延症<br /> 不生产知识，只是知识的搬运工<br /> <img src=\"/images/funny.gif\" height=\"240\" width=\"360\"/>","socials":{"github":"https://github.com/TalkUHulk","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/311127773","zhihu":"https://www.zhihu.com/people/MisterAntebellum","csdn":"https://blog.csdn.net/hyqwmxsh","juejin":"","customs":{}}},"feature":true}}