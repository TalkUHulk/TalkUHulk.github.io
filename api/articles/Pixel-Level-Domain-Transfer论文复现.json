{"title":"Pixel-Level Domain Transfer论文复现","uid":"f45a749a54e27d0a1f22b1f23eadc80a","slug":"Pixel-Level-Domain-Transfer论文复现","date":"2021-09-06T11:51:46.000Z","updated":"2021-09-06T15:35:42.197Z","comments":true,"path":"api/articles/Pixel-Level-Domain-Transfer论文复现.json","keywords":null,"cover":[],"content":"<h3 id=\"Pixel-Level-Domain-Transfer论文复现\"><a href=\"#Pixel-Level-Domain-Transfer论文复现\" class=\"headerlink\" title=\"Pixel-Level Domain Transfer论文复现\"></a>Pixel-Level Domain Transfer论文复现</h3><p><a href=\"https://blog.csdn.net/hyqwmxsh/article/details/103717306?spm=1001.2014.3001.5501\">博客迁移，原文链接</a></p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Abstract.：We present an image-conditional image generation model. The model transfers an input domain to a target domain in semantic level, and generates the target image in pixel level. To generate realistic target images, we employ the real/fake-discriminator as in Generative Adversarial Nets, but also introduce a novel domain-discriminator to make the generated image relevant to the input image. We verify our model through a challenging task of generating a piece of clothing from an input image of a dressed person. We present a high quality clothing dataset containing the two domains, and succeed in demonstrating decent results.</p></blockquote>\n<h4 id=\"论文简述\"><a href=\"#论文简述\" class=\"headerlink\" title=\"论文简述\"></a>论文简述</h4><p>  整篇论文比较容易懂，主要内容就是把输入domain转换到目标domain，输入一张模特图片，得到上衣图片，如下：<br><img src=\"/images/DTGan/task.jpg\" width=\"75%\" height=\"75%\"></p>\n<p>文章主要贡献主要在两个方面：</p>\n<h5 id=\"LookBook数据集\"><a href=\"#LookBook数据集\" class=\"headerlink\" title=\"LookBook数据集\"></a>LookBook数据集</h5><p><a href=\"https://pan.baidu.com/s/15fW_mvICv--_ydCyEyn-Qg\">下载地址（uj3j）</a></p>\n<img src=\"/images/DTGan/datasets.jpg\" width=\"75%\" height=\"75%\">\n\n<h5 id=\"基于Gan的转换框架\"><a href=\"#基于Gan的转换框架\" class=\"headerlink\" title=\"基于Gan的转换框架\"></a>基于Gan的转换框架</h5><p>网络结构如下：</p>\n<img src=\"/images/DTGan/arch.jpg\" width=\"75%\" height=\"75%\">\n\n<p>生成网络是encoder-decoder结构，判别网络有两个：Dr和Da。</p>\n<p>Dr就是一个基本的Gan的判别网络，判别fake或real；Da主要用来判断生成图像与输入是否配对，所以Dr输入是生成网络的输入和输出的concat.</p>\n<p>整个过程很容易懂，细节看<a href=\"https://blog.csdn.net/hyqwmxsh/article/details/103717306?spm=1001.2014.3001.5501\">原文</a>即可.</p>\n<h4 id=\"论文复现\"><a href=\"#论文复现\" class=\"headerlink\" title=\"论文复现\"></a>论文复现</h4><h5 id=\"Generator：\"><a href=\"#Generator：\" class=\"headerlink\" title=\"Generator：\"></a>Generator：</h5><p>输入64x64x3图像，输出64x64x3生成图像</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n \n        def conv_block(in_channels, out_channels, kernel_size, stride&#x3D;1,\n                 padding&#x3D;0, bn&#x3D;True, a_func&#x3D;&#39;lrelu&#39;):\n \n            block &#x3D; nn.ModuleList()\n            block.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func &#x3D;&#x3D; &#39;lrelu&#39;:\n                block.append(nn.LeakyReLU(0.2))\n            elif a_func &#x3D;&#x3D; &#39;relu&#39;:\n                block.append(nn.ReLU())\n            else:\n                pass\n \n            return block\n \n        def convTranspose_block(in_channels, out_channels, kernel_size, stride&#x3D;2,\n                 padding&#x3D;0, output_padding&#x3D;0, bn&#x3D;True, a_func&#x3D;&#39;relu&#39;):\n            &#39;&#39;&#39;\n            H_out &#x3D; (H_in - 1) * stride - 2 * padding + kernel_size + output_padding\n            :param in_channels:\n            :param out_channels:\n            :param kernel_size:\n            :param stride:\n            :param padding:\n            :param output_padding:\n            :param bn:\n            :param a_func:\n            :return:\n            &#39;&#39;&#39;\n            block &#x3D; nn.ModuleList()\n            block.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride,\n                 padding, output_padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func &#x3D;&#x3D; &#39;lrelu&#39;:\n                block.append(nn.LeakyReLU(0.2))\n            elif a_func &#x3D;&#x3D; &#39;relu&#39;:\n                block.append(nn.ReLU())\n            else:\n                pass\n \n            return block\n \n \n        def encoder():\n            conv_layer &#x3D; nn.ModuleList()\n            conv_layer +&#x3D; conv_block(3, 128, 5, 2, 2, False)    # 32x32x128\n            conv_layer +&#x3D; conv_block(128, 256, 5, 2, 2)        # 16x16x256\n            conv_layer +&#x3D; conv_block(256, 512, 5, 2, 2)         # 8x8x512\n            conv_layer +&#x3D; conv_block(512, 1024, 5, 2, 2)       # 4x4x1024\n            conv_layer +&#x3D; conv_block(1024, 64, 4, 1)          # 1x1x64\n            return conv_layer\n \n        def decoder():\n            conv_layer &#x3D; nn.ModuleList()\n            conv_layer +&#x3D; conv_block(64, 4 * 4 * 1024, 1, a_func&#x3D;&#39;relu&#39;)\n            conv_layer.append(Reshape((1024, 4, 4)))                            # 4x4x1024\n            conv_layer +&#x3D; convTranspose_block(1024, 512, 4, 2, 1)               # 8x8x512\n            conv_layer +&#x3D; convTranspose_block(512, 256, 4, 2, 1)                # 16x16x256\n            conv_layer +&#x3D; convTranspose_block(256, 128, 4, 2, 1)                # 32x32x128\n            conv_layer +&#x3D; convTranspose_block(128, 3, 4, 2, 1, bn&#x3D;False, a_func&#x3D;&#39;&#39;)     # 64x64x3\n            conv_layer.append(nn.Tanh())\n            return conv_layer\n \n        self.net &#x3D; nn.Sequential(\n            *encoder(),\n            *decoder(),\n        )\n \n    def forward(self, input):\n        out &#x3D; self.net(input)\n        return out<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<h5 id=\"DiscriminatorR\"><a href=\"#DiscriminatorR\" class=\"headerlink\" title=\"DiscriminatorR\"></a>DiscriminatorR</h5><p>输入64x64x3图像，输出real or fake；</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">class DiscriminatorR(nn.Module):\n    def __init__(self):\n        super(DiscriminatorR, self).__init__()\n \n        def conv_block(in_channels, out_channels, kernel_size, stride&#x3D;1,\n                       padding&#x3D;0, bn&#x3D;True, a_func&#x3D;True):\n \n            block &#x3D; nn.ModuleList()\n            block.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func:\n                block.append(nn.LeakyReLU(0.2))\n \n            return block\n \n \n        self.net &#x3D; nn.Sequential(\n            *conv_block(3, 128, 5, 2, 2, False),                            # 32x32x128\n            *conv_block(128, 256, 5, 2, 2),                                 # 16x16x256\n            *conv_block(256, 512, 5, 2, 2),                                 # 8x8x512\n            *conv_block(512, 1024, 5, 2, 2),                                # 4x4x1024\n            *conv_block(1024, 1, 4, bn&#x3D;False, a_func&#x3D;False),                # 1x1x1\n            nn.Sigmoid(),\n        )\n \n    def forward(self, img):\n        out &#x3D; self.net(img)\n        return out<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<h5 id=\"DiscriminatorA\"><a href=\"#DiscriminatorA\" class=\"headerlink\" title=\"DiscriminatorA\"></a>DiscriminatorA</h5><p>输入64x64x6的concat图像，输出real or fake；</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">class DiscriminatorA(nn.Module):\n    def __init__(self):\n        super(DiscriminatorA, self).__init__()\n \n        def conv_block(in_channels, out_channels, kernel_size, stride&#x3D;1,\n                       padding&#x3D;0, bn&#x3D;True, a_func&#x3D;True):\n \n            block &#x3D; nn.ModuleList()\n            block.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func:\n                block.append(nn.LeakyReLU(0.2))\n \n            return block\n \n        self.net &#x3D; nn.Sequential(\n            *conv_block(6, 128, 5, 2, 2, False),                # 32x32x128\n            *conv_block(128, 256, 5, 2, 2),                     # 16x16x256\n            *conv_block(256, 512, 5, 2, 2),                     # 8x8x512\n            *conv_block(512, 1024, 5, 2, 2),                    # 4x4x1024\n            *conv_block(1024, 1, 4, bn&#x3D;False, a_func&#x3D;False),    # 1x1x1\n            nn.Sigmoid(),\n        )\n \n    def forward(self, img):\n        out &#x3D; self.net(img)\n        return out<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<h5 id=\"loss\"><a href=\"#loss\" class=\"headerlink\" title=\"loss\"></a>loss</h5><p>与原文不同，在生成损失上加了mse</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">gen_loss_d &#x3D; self.adversarial_loss(torch.squeeze(gen_output), real_label)\ngen_loss_a &#x3D; self.adversarial_loss(torch.squeeze(gen_output_a), real_label)\nmse_loss &#x3D; self.mse_loss(gen_target_batch, target_batch)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span></span></code></pre>\n\n<p>完整训练测试代码：<a href=\"https://github.com/TalkUHulk/PixelDTGan-pytorch\">GitHub</a></p>\n<h4 id=\"结果\"><a href=\"#结果\" class=\"headerlink\" title=\"结果\"></a>结果</h4><h6 id=\"tensorboard\"><a href=\"#tensorboard\" class=\"headerlink\" title=\"tensorboard\"></a>tensorboard</h6><img src=\"/images/DTGan/tensorboard.jpg\" width=\"50%\" height=\"50%\">\n\n<h6 id=\"训练过程可视化\"><a href=\"#训练过程可视化\" class=\"headerlink\" title=\"训练过程可视化\"></a>训练过程可视化</h6><img src=\"/images/DTGan/proc.gif\" width=\"50%\" height=\"50%\">\n\n<h6 id=\"验证集\"><a href=\"#验证集\" class=\"headerlink\" title=\"验证集\"></a>验证集</h6><img src=\"/images/DTGan/validation.png\" width=\"50%\" height=\"50%\">\n\n\n\n","feature":true,"text":"Pixel-Level Domain Transfer论文复现博客迁移，原文链接 Abstract.：We present an image-conditional image generation model. The model transfers an input doma...","link":"","photos":[],"count_time":{"symbolsCount":"6.9k","symbolsTime":"6 mins."},"categories":[],"tags":[{"name":"csdn迁移","slug":"csdn迁移","count":1,"path":"api/tags/csdn迁移.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Pixel-Level-Domain-Transfer%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0\"><span class=\"toc-text\">Pixel-Level Domain Transfer论文复现</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%AE%BA%E6%96%87%E7%AE%80%E8%BF%B0\"><span class=\"toc-text\">论文简述</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#LookBook%E6%95%B0%E6%8D%AE%E9%9B%86\"><span class=\"toc-text\">LookBook数据集</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#%E5%9F%BA%E4%BA%8EGan%E7%9A%84%E8%BD%AC%E6%8D%A2%E6%A1%86%E6%9E%B6\"><span class=\"toc-text\">基于Gan的转换框架</span></a></li></ol></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0\"><span class=\"toc-text\">论文复现</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#Generator%EF%BC%9A\"><span class=\"toc-text\">Generator：</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#DiscriminatorR\"><span class=\"toc-text\">DiscriminatorR</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#DiscriminatorA\"><span class=\"toc-text\">DiscriminatorA</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#loss\"><span class=\"toc-text\">loss</span></a></li></ol></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E7%BB%93%E6%9E%9C\"><span class=\"toc-text\">结果</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-6\"><a class=\"toc-link\" href=\"#tensorboard\"><span class=\"toc-text\">tensorboard</span></a></li><li class=\"toc-item toc-level-6\"><a class=\"toc-link\" href=\"#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96\"><span class=\"toc-text\">训练过程可视化</span></a></li><li class=\"toc-item toc-level-6\"><a class=\"toc-link\" href=\"#%E9%AA%8C%E8%AF%81%E9%9B%86\"><span class=\"toc-text\">验证集</span></a></li></ol></li></ol></li></ol></li></ol>","author":{"name":"Hulk Wang","slug":"blog-author","avatar":"/images/avatar_small.jpg","link":"https://github.com/TalkUHulk","description":"I'm 浩克，CV算法工程师，热衷于各种有趣的技术，此博客主要用来做学习总结，杀死拖延症。<br /> <img src=\"/images/funny.gif\" height=\"240\" width=\"360\"/>","socials":{"github":"https://github.com/TalkUHulk","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/311127773","zhihu":"https://www.zhihu.com/people/MisterAntebellum","csdn":"https://blog.csdn.net/hyqwmxsh","juejin":"","customs":{}}},"mapped":true,"prev_post":{},"next_post":{"title":"YOLO-V4学习笔记","uid":"260edd383abab8c5258536a42ddb3e2a","slug":"YOLO-V4学习笔记","date":"2021-09-06T10:10:36.000Z","updated":"2021-09-07T02:36:10.394Z","comments":true,"path":"api/articles/YOLO-V4学习笔记.json","keywords":null,"cover":[],"text":"YOLO-V4学习笔记 知识点来源于网络，仅记录学习来源：https://zhuanlan.zhihu.com/p/143747206 网络结构 网络结构(来源见水印) 五个基本组件: CBM：Yolov4网络结构中的最小组件，由Conv+Bn+Mish激活函数三者组成。 CBL...","link":"","photos":[],"count_time":{"symbolsCount":"6.9k","symbolsTime":"6 mins."},"categories":[],"tags":[{"name":"detection","slug":"detection","count":2,"path":"api/tags/detection.json"}],"author":{"name":"Hulk Wang","slug":"blog-author","avatar":"/images/avatar_small.jpg","link":"https://github.com/TalkUHulk","description":"I'm 浩克，CV算法工程师，热衷于各种有趣的技术，此博客主要用来做学习总结，杀死拖延症。<br /> <img src=\"/images/funny.gif\" height=\"240\" width=\"360\"/>","socials":{"github":"https://github.com/TalkUHulk","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/311127773","zhihu":"https://www.zhihu.com/people/MisterAntebellum","csdn":"https://blog.csdn.net/hyqwmxsh","juejin":"","customs":{}}},"feature":true}}