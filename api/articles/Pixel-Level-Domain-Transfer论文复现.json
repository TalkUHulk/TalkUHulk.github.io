{"title":"Pixel-Level Domain Transfer论文复现","uid":"f45a749a54e27d0a1f22b1f23eadc80a","slug":"Pixel-Level-Domain-Transfer论文复现","date":"2021-09-06T11:51:46.000Z","updated":"2021-11-03T10:59:59.413Z","comments":true,"path":"api/articles/Pixel-Level-Domain-Transfer论文复现.json","keywords":null,"cover":"/images/DTGan/arch.jpg","content":"<h3 id=\"pixel-level-domain-transfer论文复现\">Pixel-Level Domain Transfer论文复现</h3>\n<p><a href=\"https://blog.csdn.net/hyqwmxsh/article/details/103717306?spm=1001.2014.3001.5501\">博客迁移，原文链接</a></p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Abstract.：We present an image-conditional image generation model. The model transfers an input domain to a target domain in semantic level, and generates the target image in pixel level. To generate realistic target images, we employ the real/fake-discriminator as in Generative Adversarial Nets, but also introduce a novel domain-discriminator to make the generated image relevant to the input image. We verify our model through a challenging task of generating a piece of clothing from an input image of a dressed person. We present a high quality clothing dataset containing the two domains, and succeed in demonstrating decent results.</p></blockquote>\n<h4 id=\"论文简述\">论文简述</h4>\n<p>整篇论文比较容易懂，主要内容就是把输入domain转换到目标domain，输入一张模特图片，得到上衣图片，如下：</p>\n<p><img src=\"/images/DTGan/task.jpg\" width=\"75%\" height=\"75%\"></p>\n<p>文章主要贡献主要在两个方面：</p>\n<h5 id=\"lookbook数据集\">LookBook数据集</h5>\n<p><a href=\"https://pan.baidu.com/s/15fW_mvICv--_ydCyEyn-Qg\">下载地址（uj3j）</a></p>\n<p><img src=\"/images/DTGan/datasets.jpg\" width=\"75%\" height=\"75%\"></p>\n<h5 id=\"基于gan的转换框架\">基于Gan的转换框架</h5>\n<p>网络结构如下：</p>\n<p><img src=\"/images/DTGan/arch.jpg\" width=\"75%\" height=\"75%\"></p>\n<p>生成网络是encoder-decoder结构，判别网络有两个：Dr和Da。</p>\n<p>Dr就是一个基本的Gan的判别网络，判别fake或real；Da主要用来判断生成图像与输入是否配对，所以Dr输入是生成网络的输入和输出的concat.</p>\n<p>整个过程很容易懂，细节看<a href=\"https://blog.csdn.net/hyqwmxsh/article/details/103717306?spm=1001.2014.3001.5501\">原文</a>即可.</p>\n<h4 id=\"论文复现\">论文复现</h4>\n<h5 id=\"generator\">Generator：</h5>\n<p>输入64x64x3图像，输出64x64x3生成图像</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n \n        def conv_block(in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, bn=True, a_func='lrelu'):\n \n            block = nn.ModuleList()\n            block.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func == 'lrelu':\n                block.append(nn.LeakyReLU(0.2))\n            elif a_func == 'relu':\n                block.append(nn.ReLU())\n            else:\n                pass\n \n            return block\n \n        def convTranspose_block(in_channels, out_channels, kernel_size, stride=2,\n                 padding=0, output_padding=0, bn=True, a_func='relu'):\n            '''\n            H_out = (H_in - 1) * stride - 2 * padding + kernel_size + output_padding\n            :param in_channels:\n            :param out_channels:\n            :param kernel_size:\n            :param stride:\n            :param padding:\n            :param output_padding:\n            :param bn:\n            :param a_func:\n            :return:\n            '''\n            block = nn.ModuleList()\n            block.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride,\n                 padding, output_padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func == 'lrelu':\n                block.append(nn.LeakyReLU(0.2))\n            elif a_func == 'relu':\n                block.append(nn.ReLU())\n            else:\n                pass\n \n            return block\n \n \n        def encoder():\n            conv_layer = nn.ModuleList()\n            conv_layer += conv_block(3, 128, 5, 2, 2, False)    # 32x32x128\n            conv_layer += conv_block(128, 256, 5, 2, 2)        # 16x16x256\n            conv_layer += conv_block(256, 512, 5, 2, 2)         # 8x8x512\n            conv_layer += conv_block(512, 1024, 5, 2, 2)       # 4x4x1024\n            conv_layer += conv_block(1024, 64, 4, 1)          # 1x1x64\n            return conv_layer\n \n        def decoder():\n            conv_layer = nn.ModuleList()\n            conv_layer += conv_block(64, 4 * 4 * 1024, 1, a_func='relu')\n            conv_layer.append(Reshape((1024, 4, 4)))                            # 4x4x1024\n            conv_layer += convTranspose_block(1024, 512, 4, 2, 1)               # 8x8x512\n            conv_layer += convTranspose_block(512, 256, 4, 2, 1)                # 16x16x256\n            conv_layer += convTranspose_block(256, 128, 4, 2, 1)                # 32x32x128\n            conv_layer += convTranspose_block(128, 3, 4, 2, 1, bn=False, a_func='')     # 64x64x3\n            conv_layer.append(nn.Tanh())\n            return conv_layer\n \n        self.net = nn.Sequential(\n            *encoder(),\n            *decoder(),\n        )\n \n    def forward(self, input):\n        out = self.net(input)\n        return out<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h5 id=\"discriminatorr\">DiscriminatorR</h5>\n<p>输入64x64x3图像，输出real or fake；</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">class DiscriminatorR(nn.Module):\n    def __init__(self):\n        super(DiscriminatorR, self).__init__()\n \n        def conv_block(in_channels, out_channels, kernel_size, stride=1,\n                       padding=0, bn=True, a_func=True):\n \n            block = nn.ModuleList()\n            block.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func:\n                block.append(nn.LeakyReLU(0.2))\n \n            return block\n \n \n        self.net = nn.Sequential(\n            *conv_block(3, 128, 5, 2, 2, False),                            # 32x32x128\n            *conv_block(128, 256, 5, 2, 2),                                 # 16x16x256\n            *conv_block(256, 512, 5, 2, 2),                                 # 8x8x512\n            *conv_block(512, 1024, 5, 2, 2),                                # 4x4x1024\n            *conv_block(1024, 1, 4, bn=False, a_func=False),                # 1x1x1\n            nn.Sigmoid(),\n        )\n \n    def forward(self, img):\n        out = self.net(img)\n        return out<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h5 id=\"discriminatora\">DiscriminatorA</h5>\n<p>输入64x64x6的concat图像，输出real or fake；</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">class DiscriminatorA(nn.Module):\n    def __init__(self):\n        super(DiscriminatorA, self).__init__()\n \n        def conv_block(in_channels, out_channels, kernel_size, stride=1,\n                       padding=0, bn=True, a_func=True):\n \n            block = nn.ModuleList()\n            block.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n            if bn:\n                block.append(nn.BatchNorm2d(out_channels))\n            if a_func:\n                block.append(nn.LeakyReLU(0.2))\n \n            return block\n \n        self.net = nn.Sequential(\n            *conv_block(6, 128, 5, 2, 2, False),                # 32x32x128\n            *conv_block(128, 256, 5, 2, 2),                     # 16x16x256\n            *conv_block(256, 512, 5, 2, 2),                     # 8x8x512\n            *conv_block(512, 1024, 5, 2, 2),                    # 4x4x1024\n            *conv_block(1024, 1, 4, bn=False, a_func=False),    # 1x1x1\n            nn.Sigmoid(),\n        )\n \n    def forward(self, img):\n        out = self.net(img)\n        return out<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<h5 id=\"loss\">loss</h5>\n<p>与原文不同，在生成损失上加了mse</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">gen_loss_d = self.adversarial_loss(torch.squeeze(gen_output), real_label)\ngen_loss_a = self.adversarial_loss(torch.squeeze(gen_output_a), real_label)\nmse_loss = self.mse_loss(gen_target_batch, target_batch)<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span></span></code></pre>\n<p>完整训练测试代码：<a href=\"https://github.com/TalkUHulk/PixelDTGan-pytorch\">GitHub</a></p>\n<h4 id=\"结果\">结果</h4>\n<h6 id=\"tensorboard\">tensorboard</h6>\n<p><img src=\"/images/DTGan/tensorboard.jpg\" width=\"50%\" height=\"50%\"></p>\n<h6 id=\"训练过程可视化\">训练过程可视化</h6>\n<p><img src=\"/images/DTGan/proc.gif\" width=\"50%\" height=\"50%\"></p>\n<h6 id=\"验证集\">验证集</h6>\n<p><img src=\"/images/DTGan/validation.png\" width=\"50%\" height=\"50%\"></p>\n","text":"Pixel-Level Domain Transfer论文复现 博客迁移，原文链接 Abstract.：We present an image-conditional image generation model. The model transfers an input dom...","link":"","photos":[],"count_time":{"symbolsCount":"6.5k","symbolsTime":"6 mins."},"categories":[],"tags":[{"name":"csdn迁移","slug":"csdn迁移","count":1,"path":"api/tags/csdn迁移.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#pixel-level-domain-transfer%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0\"><span class=\"toc-text\">Pixel-Level Domain Transfer论文复现</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%AE%BA%E6%96%87%E7%AE%80%E8%BF%B0\"><span class=\"toc-text\">论文简述</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#lookbook%E6%95%B0%E6%8D%AE%E9%9B%86\"><span class=\"toc-text\">LookBook数据集</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#%E5%9F%BA%E4%BA%8Egan%E7%9A%84%E8%BD%AC%E6%8D%A2%E6%A1%86%E6%9E%B6\"><span class=\"toc-text\">基于Gan的转换框架</span></a></li></ol></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0\"><span class=\"toc-text\">论文复现</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#generator\"><span class=\"toc-text\">Generator：</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#discriminatorr\"><span class=\"toc-text\">DiscriminatorR</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#discriminatora\"><span class=\"toc-text\">DiscriminatorA</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#loss\"><span class=\"toc-text\">loss</span></a></li></ol></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E7%BB%93%E6%9E%9C\"><span class=\"toc-text\">结果</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-6\"><a class=\"toc-link\" href=\"#tensorboard\"><span class=\"toc-text\">tensorboard</span></a></li><li class=\"toc-item toc-level-6\"><a class=\"toc-link\" href=\"#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96\"><span class=\"toc-text\">训练过程可视化</span></a></li><li class=\"toc-item toc-level-6\"><a class=\"toc-link\" href=\"#%E9%AA%8C%E8%AF%81%E9%9B%86\"><span class=\"toc-text\">验证集</span></a></li></ol></li></ol></li></ol></li></ol>","author":{"name":"Hulk Wang","slug":"blog-author","avatar":"/images/avatar_small.jpg","link":"https://github.com/TalkUHulk","description":"学习、记录、总结<br />kill拖延症<br /> 不生产知识，只是知识的搬运工<br /> <img src=\"/images/funny.gif\" height=\"240\" width=\"360\"/>","socials":{"github":"https://github.com/TalkUHulk","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/311127773","zhihu":"https://www.zhihu.com/people/MisterAntebellum","csdn":"https://blog.csdn.net/hyqwmxsh","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"YOLO-V5学习笔记","uid":"57d3aa7cdcc56700d35ee7aa4e8c6558","slug":"YOLO-V5学习笔记","date":"2021-09-09T07:42:37.000Z","updated":"2021-11-15T08:43:40.519Z","comments":true,"path":"api/articles/YOLO-V5学习笔记.json","keywords":null,"cover":"https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fuser-images.githubusercontent.com%2F26833433%2F98699617-a1595a00-2377-11eb-8145-fc674eb9b1a7.jpg&refer=http%3A%2F%2Fuser-images.githubusercontent.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1639557802&t=bf12f6b55521be8949bcb635defaedc5","text":"YOLO-V5学习笔记 知识点来源于网络，仅记录学习 来源：https://zhuanlan.zhihu.com/p/172121380 网络结构 Yolov5s网络结构(来源见水印) 如上图为yolov5的整体网络结构，跟yolov4一样，分别按input、backbone、N...","link":"","photos":[],"count_time":{"symbolsCount":"3.2k","symbolsTime":"3 mins."},"categories":[],"tags":[{"name":"detection","slug":"detection","count":6,"path":"api/tags/detection.json"}],"author":{"name":"Hulk Wang","slug":"blog-author","avatar":"/images/avatar_small.jpg","link":"https://github.com/TalkUHulk","description":"学习、记录、总结<br />kill拖延症<br /> 不生产知识，只是知识的搬运工<br /> <img src=\"/images/funny.gif\" height=\"240\" width=\"360\"/>","socials":{"github":"https://github.com/TalkUHulk","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/311127773","zhihu":"https://www.zhihu.com/people/MisterAntebellum","csdn":"https://blog.csdn.net/hyqwmxsh","juejin":"","customs":{}}}},"next_post":{"title":"YOLO-V4学习笔记","uid":"260edd383abab8c5258536a42ddb3e2a","slug":"YOLO-V4学习笔记","date":"2021-09-06T10:10:36.000Z","updated":"2021-11-11T14:23:56.241Z","comments":true,"path":"api/articles/YOLO-V4学习笔记.json","keywords":null,"cover":"https://github.com/TalkUHulk/yolov4-TT100k/blob/main/results/result.png?raw=true","text":"YOLO-V4学习笔记 知识点来源于网络，仅记录学习 来源：https://zhuanlan.zhihu.com/p/143747206 yolov4-TT100k 使用yolov4训练的交通标志检测 网络结构 网络结构(来源见水印) 五个基本组件: CBM：Yolov4网络结构...","link":"","photos":[],"count_time":{"symbolsCount":"7.3k","symbolsTime":"7 mins."},"categories":[],"tags":[{"name":"detection","slug":"detection","count":6,"path":"api/tags/detection.json"}],"author":{"name":"Hulk Wang","slug":"blog-author","avatar":"/images/avatar_small.jpg","link":"https://github.com/TalkUHulk","description":"学习、记录、总结<br />kill拖延症<br /> 不生产知识，只是知识的搬运工<br /> <img src=\"/images/funny.gif\" height=\"240\" width=\"360\"/>","socials":{"github":"https://github.com/TalkUHulk","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/311127773","zhihu":"https://www.zhihu.com/people/MisterAntebellum","csdn":"https://blog.csdn.net/hyqwmxsh","juejin":"","customs":{}}}}}