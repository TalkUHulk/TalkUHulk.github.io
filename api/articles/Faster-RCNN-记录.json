{"title":"Faster RCNN 记录","uid":"06a101cf6ba6fad0463c3458164de1c3","slug":"Faster-RCNN-记录","date":"2021-10-11T07:47:29.000Z","updated":"2021-10-12T03:02:37.814Z","comments":true,"path":"api/articles/Faster-RCNN-记录.json","keywords":null,"cover":[],"content":"<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>参考来源：<br><a href=\"https://zhuanlan.zhihu.com/p/31426458\">https://zhuanlan.zhihu.com/p/31426458</a><br><a href=\"https://zhuanlan.zhihu.com/p/86403390\">https://zhuanlan.zhihu.com/p/86403390</a></p></blockquote>\n<img src=\"/images/rcnn/arch.jpg\" width=\"75%\" height=\"75%\">\n<p align=\"center\"> Faster RCNN基本结构</p>\n\n<p>Faster RCNN其实可以分为4个主要内容：</p>\n<ol>\n<li>特征提取：Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。</li>\n<li>RPN：RPN网络用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。</li>\n<li>Roi Pooling：该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。</li>\n<li>Classification：利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。</li>\n</ol>\n<img src=\"/images/rcnn/arch2.jpg\" width=\"75%\" height=\"75%\">\n<p align=\"center\"> Faster RCNN(VGG16)网络结构</p>\n\n<h3 id=\"特征提取\"><a href=\"#特征提取\" class=\"headerlink\" title=\"特征提取\"></a>特征提取</h3><p>没啥可说的，特征提取，输入MxN，VGG16为例，4个pooling层，得到的feature map分辨率为：（M/16）x（N/16）</p>\n<h3 id=\"RPN\"><a href=\"#RPN\" class=\"headerlink\" title=\"RPN\"></a>RPN</h3><img src=\"/images/rcnn/rpn.jpg\" width=\"75%\" height=\"75%\">\n<p align=\"center\"> RPN结构</p>\n\n<p>RPN网络实际分为2条线，上面一条通过softmax分类anchors获得positive和negative分类，下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合positive anchors和对应bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。</p>\n<h4 id=\"anchors\"><a href=\"#anchors\" class=\"headerlink\" title=\"anchors\"></a>anchors</h4><p>以tf代码为例：</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">def _anchor_component(self):  #获得锚的数量和位置\n    with tf.variable_scope(&#39;ANCHOR_&#39; + &#39;default&#39;):\n        # just to get the shape right  只是为了让形状正确\n        height &#x3D; tf.to_int32(tf.ceil(self._im_info[0, 0] &#x2F; np.float32(self._feat_stride[0]))) #高度为图片高&#x2F;16,，就是特征图的高，tf.ceil向上取整\n        width &#x3D; tf.to_int32(tf.ceil(self._im_info[0, 1] &#x2F; np.float32(self._feat_stride[0]))) #宽度为图片宽&#x2F;16，为特征图的宽\n        anchors, anchor_length &#x3D; tf.py_func(generate_anchors_pre,\n                                            [height, width,                                        self._feat_stride,self._anchor_scales, self._anchor_ratios],\n                                            [tf.float32, tf.int32], name&#x3D;&quot;generate_anchors&quot;)  #构建生成锚的py函数，这个锚有9*（50*38）个，anchor_length是锚的个数\n        anchors.set_shape([None, 4])  #锚定义为4列\n        anchor_length.set_shape([]) #行向量，length为锚的个数\n        self._anchors &#x3D; anchors\n        self._anchor_length &#x3D; anchor_length  #length为特征图面积*9<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p> 函数generate_anchors_pre来生成anchor：</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">def generate_anchors_pre(height, width, feat_stride, anchor_scales&#x3D;(8, 16, 32), anchor_ratios&#x3D;(0.5, 1, 2)):\n    &quot;&quot;&quot; A wrapper function to generate anchors given different scales\n      Also return the number of anchors in variable &#39;length&#39; 给定不同比例生成锚点的包装函数也返回可变“长度”的锚点数量\n    &quot;&quot;&quot;\n    anchors &#x3D; generate_anchors(ratios&#x3D;np.array(anchor_ratios), scales&#x3D;np.array(anchor_scales))\n    A &#x3D; anchors.shape[0] #anchor的数量，为9\n    shift_x &#x3D; np.arange(0, width) * feat_stride  #将特征图的宽度进行16倍延伸至原图，以width&#x3D;4为例子，则shfit_x&#x3D;[0,16,32,48]\n    shift_y &#x3D; np.arange(0, height) * feat_stride  #将特征图的高度进行16倍衍生至原图\n    shift_x, shift_y &#x3D; np.meshgrid(shift_x, shift_y)  #生成原图的网格点\n    shifts &#x3D; np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel())).transpose()  #若width&#x3D;50，height&#x3D;38，生成（50*38）*4的数组\n    #如 [[0,0,0,0],[16,0,16,0],[32,0,32,0].......]，shift中的前两个坐标和后两个一样（保持右下和左上的坐标一样），是从左到右，从上到下的坐标点（映射到原图）\n    K &#x3D; shifts.shape[0]  #k&#x3D;50*38\n    # width changes faster, so here it is H, W, C\n    anchors &#x3D; anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2))  #列数相同的list相加就是简单的添加，而数组不一样，1*9*4和（50*38）*1*4进行相加，生成了（50*38）*9*4的数组\n    #其实意思就是右下角坐标和左上角的左边都加上同一个变换坐标\n    anchors &#x3D; anchors.reshape((K * A, 4)).astype(np.float32, copy&#x3D;False) #三维变两维，（50*38*9，4），此处就是将特征层的anchor坐标转到原图上的区域\n    length &#x3D; np.int32(anchors.shape[0])  #length&#x3D;50*38*9\n\n    return anchors, length<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<p>观察第一行代码，即</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">anchors &#x3D; generate_anchors(ratios&#x3D;np.array(anchor_ratios),scales&#x3D;np.array(anchor_scales))<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span></span></code></pre>\n<p> 这里是3x3=9个基础anchor的生成，我们进入函数generate_anchors中，发现其实生成就是一个数组，这个数组是：</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\"># array([[ -83.,  -39.,  100.,   56.],\n#       [-175.,  -87.,  192.,  104.],\n#       [-359., -183.,  376.,  200.],\n#       [ -55.,  -55.,   72.,   72.],\n#       [-119., -119.,  136.,  136.],\n#       [-247., -247.,  264.,  264.],\n#       [ -35.,  -79.,   52.,   96.],\n#       [ -79., -167.,   96.,  184.],\n#       [-167., -343.,  184.,  360.]])<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>其中每行的4个值（x1，y1, x2, y2）表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为{1:1, 1:2, 2:1}三种，<u><strong>实际上通过anchors就引入了检测中常用到的多尺度方法。</strong></u></p>\n<p>生成这个数组的代码是：</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">def generate_anchors(base_size&#x3D;16, ratios&#x3D;[0.5, 1, 2],\n                     scales&#x3D;2 ** np.arange(3, 6)):\n    &quot;&quot;&quot;\n    Generate anchor (reference) windows by enumerating aspect ratios X\n    scales wrt a reference (0, 0, 15, 15) window.  通过枚举参考(0，0，15，15)窗口的长宽比来生成锚(参考)窗口。\n    &quot;&quot;&quot;\n\n    base_anchor &#x3D; np.array([1, 1, base_size, base_size]) - 1  #生成一个base_anchor &#x3D; [0, 0, 15, 15]，其中(0, 0)是anchor左上点的坐标\n    # (15, 15)是anchor右下点的坐标，那么这个anchor的中心点的坐标是(7.5, 7.5)\n    ratio_anchors &#x3D; _ratio_enum(base_anchor, ratios)#然后产生ratio_anchors，就是将base_anchor和ratios[0.5, 1, 2],ratio_anchors生成三个anchors\n    # 传入到_ratio_enum()函数，ratios代表的是三种宽高比。\n    anchors &#x3D; np.vstack([_scale_enum(ratio_anchors[i, :], scales)  #在刚刚3个anchor基础上继续生成anchor\n                         for i in range(ratio_anchors.shape[0])])\n    return anchors<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>我们发现这个数组中的每行数据（如第一行：[ -83., -39., 100., 56.] )，它们的中心位置都为（7.5，7.5），即（0，0，15，15）的中心（因为通过代码也可以得知，这9个基础框的生成也是以（0，0，15，15）为基础的，代码中base_anchor就是（0，0，15，15））。</p>\n<p>通过3种anchor ratio和3种anchor scale生成的9个数组，这9个数组在坐标图上如图2所示。<br><img src=\"/images/rcnn/anchor.jpg\" width=\"75%\" height=\"75%\"></p>\n<p align=\"center\"> anchor</p>\n\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>为什么这样设计呢？我们知道，其实一张图片通过特征提取网路VGG16后，长宽比都缩小了16倍得到了特征图。比如原先的800*600的原图通过VGG16后得到了50*38的特征图（以上先不考虑通道数），我们就假设，特征图上的每一个点（大小为1*1），和原图16*16区域对应（这里记住，对应不是指代感受野，只是便于理解).这里，使用对应的好处，就是特征图上的每个点（大小为1*1）负责由原图对应区域(大小为16*16）中心生成的9个anchor的训练和学习。所以Faster RCNN共产生<u><strong>50x38x9=17100个anchor</strong></u>，基本覆盖了全图各个区域。</p></blockquote>\n<p>看generate_anchors_pre函数，shifts就是对（shift_x, shift_y）进行组合，其中shift_x是对x坐标进行移动，shift_y是对y坐标进行移动，综合起来就是将基础的中心为（7.5，7.5）的9个anchor平移到全图上，覆盖所有可能的区域。</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\">anchors &#x3D; anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)) #列数相同的list相加就是简单的添加，而数组不一样，1*9*4和（50*38）*1*4进行相加，生成了（50*38）*9*4的数组\n#其实意思就是右下角坐标和左上角的左边都加上同一个变换坐标\nanchors &#x3D; anchors.reshape((K * A, 4)).astype(np.float32, copy&#x3D;False) #三维变两维，（50*38*9，4），此处就是将特征层的anchor坐标转到原图上的区域\nlength &#x3D; np.int32(anchors.shape[0]) #length&#x3D;50*38*9<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span></span></code></pre>\n<p>上述代码就是完成了9个base anchor 的移动，输出结果就是50<em>38</em>9个anchor。那么到此，所有的anchor都生成了，当然了，所有的anchor也和特征图产生了一一对应的关系了。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>anchor是辅助模型进行训练的，能让模型对物体的大小和形状有个大致的认知，也算是人为添加的先验知识了。</p></blockquote>\n<p>RPN最终就是在原图尺度上，设置了密密麻麻的候选Anchor。然后用cnn去判断哪些Anchor是里面有目标的positive anchor，哪些是没目标的negative anchor。所以，仅仅是个二分类而已！</p>\n<img src=\"/images/rcnn/rpn2.jpg\" width=\"75%\" height=\"75%\">\n\n<p>解释一下上面这张图的数字。</p>\n<ol>\n<li>在原文中使用的是ZF model中，其Conv Layers中最后的conv5层num_output=256，对应生成256张特征图，所以相当于feature map每个点都是256-dimensions</li>\n<li>在conv5之后，做了rpn_conv/3x3卷积且num_output=256，相当于每个点又融合了周围3x3的空间信息</li>\n<li>假设在conv5 feature map中每个点上有k个anchor（默认k=9），而每个anhcor要分positive和negative，所以每个点由256d feature转化为cls=2•k scores；而每个anchor都有(x, y, w, h)对应4个偏移量，所以reg=4•k coordinates</li>\n<li><u><strong>全部anchors拿去训练太多了，训练程序会在合适的anchors中随机选取128个postive anchors+128个negative anchors进行训练</strong></u></li>\n</ol>\n<h4 id=\"rpn中的二分类\"><a href=\"#rpn中的二分类\" class=\"headerlink\" title=\"rpn中的二分类\"></a>rpn中的二分类</h4><p>一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设 W=M/16，H=N/16。在进入reshape与softmax之前，先做了1x1卷积（此时anchor已经生成完毕），如图9：</p>\n<img src=\"/images/rcnn/二分类.jpg\" width=\"75%\" height=\"75%\">\n\n<p>可以看到其num_output=18，也就是经过该卷积的输出图像为WxHx18大小,这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是positive和negative，所有这些信息都保存WxHx(9*2)大小的矩阵。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>综上所述，RPN网络中利用anchors和softmax初步提取出positive anchors作为候选区域（另外也有实现用sigmoid代替softmax，输出[1, 1, 9xH, W]后接sigmoid进行positive/negative二分类，原理一样）。</p></blockquote>\n<h4 id=\"bounding-box-regression\"><a href=\"#bounding-box-regression\" class=\"headerlink\" title=\"bounding box regression\"></a>bounding box regression</h4><blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>在RPN网络中，进行bbox regression得到的是每个anchor的偏移量。再与anchor的坐标进行调整以后，得到proposal的坐标，经过一系列后处理，比如NMS，top-K操作以后，得到得分最高的前N个proposal传入分类网络。</p></blockquote>\n<p>如图所示绿色框为飞机的Ground Truth(GT)，红色为提取的positive anchors，即便红色的框被分类器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机。所以我们希望采用一种方法对红色的框进行微调，使得positive anchors和GT更加接近。</p>\n<img src=\"/images/rcnn/bbox_reg_1.jpg\" width=\"75%\" height=\"75%\">\n\n<p>对于窗口一般使用四维向量 (x, y, w, h)表示，分别表示窗口的中心点坐标和宽高。对于下图，红色的框A代表原始的positive Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G’，即：</p>\n<img src=\"/images/rcnn/bbox_reg_2.jpg\" width=\"75%\" height=\"75%\">\n\n<ul>\n<li>给定anchor A=($A_x$, $A_y$, $A_w$, $A_h$)和GT=[$G_x$, $G_y$, $G_w$, $G_h$]</li>\n<li>寻找一种变换F，使得：F($A_x$, $A_y$, $A_w$, $A_h$)= ($G_x’$, $G_y’$, $G_w’$, $G_h’$),其中($G_x’$, $G_y’$, $G_w’$, $G_h’$)≈($G_x$, $G_y$, $G_w$, $G_h$)</li>\n</ul>\n<p>比较简单的思路就是:</p>\n<img src=\"/images/rcnn/bbox_reg_3.jpg\" width=\"75%\" height=\"75%\">\n\n<p>观察上面4个公式发现，需要学习的是$d_x(A)$,$d_y(A)$,$d_w(A)$,$d_h(A)$ 这四个变换。当输入的anchor A与GT相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调（注意，只有当anchors A和GT比较接近时，才能使用线性回归模型，否则就是复杂的非线性问题了）。</p>\n<p>接下来的问题就是如何通过线性回归获$d_x(A)$,$d_y(A)$,$d_w(A)$,$d_h(A)$ 了。线性回归就是给定输入的特征向量X, 学习一组参数W, 使得经过线性回归后的值跟真实值Y非常接近，即Y=WX.</p>\n<p>对于该问题，输入X是cnn feature map，定义为Φ；同时还有训练传入A与GT之间的变换量，即$t_x$,$t_y$,$t_w$,$t_h$。输出是$d_x(A)$,$d_y(A)$,$d_w(A)$,$d_h(A)$四个变换。那么目标函数可以表示为:</p>\n<img src=\"/images/rcnn/bbox_reg_4.jpg\" width=\"75%\" height=\"75%\">\n\n<p>所以在RPN中，bounding box regression通过第二条线完成：</p>\n<img src=\"/images/rcnn/bbox_reg_5.jpg\" width=\"75%\" height=\"75%\">\n<p align=\"center\"> RPN中的bbox reg </p>\n\n<p>可以看到经过该卷积输出图像为WxHx36，在caffe blob存储为[1, 4x9, H, W]，这里相当于feature maps每个点都有9个anchors，每个anchors又都有4个用于回归的</p>\n<p>VGG输出(M/16)*(N/16)*256的特征，对应设置 (M/16)*(N/16)*k 个anchors，而RPN输出：</p>\n<ul>\n<li>大小为(M/16)*(N/16)*2k的positive/negative softmax分类特征矩阵</li>\n<li>大小为(M/16)*(N/16)*4k的regression坐标回归特征矩阵</li>\n</ul>\n<p>恰好满足RPN完成positive/negative分类+bounding box regression坐标回归.</p>\n<h4 id=\"Proposal-Layer\"><a href=\"#Proposal-Layer\" class=\"headerlink\" title=\"Proposal Layer\"></a>Proposal Layer</h4><p>Proposal Layer负责综合所有 $d_x(A)$,$d_y(A)$,$d_w(A)$,$d_h(A)$ 变换量和positive anchors，计算出精准的proposal，送入后续RoI Pooling Layer。</p>\n<p>Proposal Layer有3个输入：</p>\n<ol>\n<li>分类器结果（positive vs negative anchorsrpn_cls_prob_reshape)</li>\n<li>bbox reg的 $d_x(A)$,$d_y(A)$,$d_w(A)$,$d_h(A)$变换量rpn_bbox_pred</li>\n<li>im_info；</li>\n<li>另外还有参数feat_stride=16，这和图4是对应的。</li>\n</ol>\n<p>首先解释im_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定MxN，im_info=[M, N, scale_factor]则保存了此次缩放的所有信息。然后经过Conv Layers，经过4次pooling变为WxH=(M/16)x(N/16)大小，其中feature_stride=16则保存了该信息，用于计算anchor偏移量。</p>\n<p>Proposal Layer forward按照以下顺序依次处理：<br>1.生成anchors，利用$d_x(A)$,$d_y(A)$,$d_w(A)$,$d_h(A)$<br>对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致）<br>2. 按照输入的positive softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的positive anchors<br>3. 限定超出图像边界的positive anchors为图像边界，防止后续roi pooling时proposal超出图像边界<br>4. 剔除尺寸非常小的positive anchors<br>5. 对剩余的positive anchors进行NMS（nonmaximum suppression）<br>6. Proposal Layer有3个输入：positive和negative anchors分类器结果rpn_cls_prob_reshape，对应的bbox reg的(e.g. 300)结果作为proposal输出</p>\n<p>之后输出proposal=[x1, y1, x2, y2]，注意，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应MxN输入图像尺度的，这点在后续网络中有用。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>RPN网络结构就介绍到这里，总结起来就是：<br>生成anchors -&gt; softmax分类器提取positvie anchors -&gt; bbox reg回归positive anchors -&gt; Proposal Layer生成proposals</p></blockquote>\n<h4 id=\"RPN生成RoIs\"><a href=\"#RPN生成RoIs\" class=\"headerlink\" title=\"RPN生成RoIs\"></a>RPN生成RoIs</h4><p>RPN在自身训练的同时，还会提供RoIs（region of interests）给Fast RCNN（RoIHead）作为训练样本。RPN生成RoIs的过程(ProposalCreator)如下：</p>\n<ol>\n<li>对于每张图片，利用它的feature map， 计算 (H/16)× (W/16)×9（大概20000）个anchor属于前景的概率，以及对应的位置参数。</li>\n<li>选取概率较大的12000个anchor</li>\n<li>利用回归的位置参数，修正这12000个anchor的位置，得到RoIs</li>\n<li>利用非极大值（(Non-maximum suppression, NMS）抑制，选出概率最大的2000个RoIs</li>\n</ol>\n<p>注意：在inference的时候，为了提高处理速度，12000和2000分别变为6000和300.</p>\n<p>注意：这部分的操作不需要进行反向传播，因此可以利用numpy/tensor实现。</p>\n<h3 id=\"ROI-pooling\"><a href=\"#ROI-pooling\" class=\"headerlink\" title=\"ROI pooling\"></a>ROI pooling</h3><p>RoI Pooling层则负责收集proposal，并计算出proposal feature maps，送入后续网络。从图2中可以看到Rol pooling层有2个输入：</p>\n<ol>\n<li>原始的feature maps</li>\n<li>RPN输出的proposal boxes（大小各不相同）</li>\n</ol>\n<h5 id=\"为何需要RoI-Pooling\"><a href=\"#为何需要RoI-Pooling\" class=\"headerlink\" title=\"为何需要RoI Pooling\"></a>为何需要RoI Pooling</h5><p>当网络训练好后输入的图像尺寸必须是固定值，同时网络输出也是固定大小的vector or matrix。如果输入图像大小不定，这个问题就变得比较麻烦。有2种解决办法：</p>\n<ol>\n<li>从图像中crop一部分传入网络</li>\n<li>将图像warp成需要的大小后传入网络</li>\n</ol>\n<p>无论采取那种办法都不好，要么crop后破坏了图像的完整结构，要么warp破坏了图像原始形状信息。</p>\n<p>RPN网络生成的proposals的方法：对positive anchors进行bounding box regression，那么这样获得的proposals也是大小形状各不相同，即也存在上述问题。<u><strong>所以Faster R-CNN中提出了RoI Pooling解决这个问题.</strong></u></p>\n<h5 id=\"RoI-Pooling-layer-forward\"><a href=\"#RoI-Pooling-layer-forward\" class=\"headerlink\" title=\"RoI Pooling layer forward\"></a>RoI Pooling layer forward</h5><ol>\n<li>由于<u><strong>proposal是对应MxN尺度</strong></u>的，所以首先使用spatial_scale参数将其映射回(M/16)x(N/16)大小的feature map尺度；</li>\n<li>再将每个proposal对应的feature map区域水平分为 pooled_w*\\pooled_h的网格；</li>\n<li>对网格的每一份都进行max pooling处理。</li>\n</ol>\n<p>这样处理后，即使大小不同的proposal输出结果都是pooled_w*\\pooled_h固定大小，实现了固定长度输出。</p>\n<h5 id=\"为什么要pooling成7×7的尺度？\"><a href=\"#为什么要pooling成7×7的尺度？\" class=\"headerlink\" title=\"为什么要pooling成7×7的尺度？\"></a>为什么要pooling成7×7的尺度？</h5><p>是为了能够共享权重。在之前讲过，除了用到VGG前几层的卷积之外，最后的全连接层也可以继续利用。当所有的RoIs都被pooling成（512×7×7）的feature map后，将它reshape 成一个一维的向量，就可以利用VGG16预训练的权重，初始化前两层全连接。最后再接两个全连接层，分别是：</p>\n<p>FC 21 用来分类，预测RoIs属于哪个类别（20个类+背景）<br>FC 84 用来回归位置（21个类，每个类都有4个位置参数）</p>\n<h3 id=\"Classification\"><a href=\"#Classification\" class=\"headerlink\" title=\"Classification\"></a>Classification</h3><p>Classification部分利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出cls_prob概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量bbox_pred，用于回归更加精确的目标检测框。Classification部分网络结构如图:</p>\n<img src=\"/images/rcnn/classification.jpg\" width=\"75%\" height=\"75%\">\n<p align=\"center\"> Classification部分网络结构 </p>\n\n<p>从RoI Pooling获取到7x7=49大小的proposal feature maps后，送入后续网络，可以看到做了如下2件事：</p>\n<ol>\n<li>通过全连接和softmax对proposals进行分类，这实际上已经是识别的范畴了</li>\n<li>再次对proposals进行bounding box regression，获取更高精度的rect box</li>\n</ol>\n<h3 id=\"Anchor到底与网络输出如何对应\"><a href=\"#Anchor到底与网络输出如何对应\" class=\"headerlink\" title=\"Anchor到底与网络输出如何对应\"></a>Anchor到底与网络输出如何对应</h3><p>VGG输出 50*38*512的特征，对应设置 50*38*k个anchors，而RPN输出50*38*2k的分类特征矩阵和50*38*4k的坐标回归特征矩阵。</p>\n<img src=\"/images/rcnn/match.jpg\" width=\"75%\" height=\"75%\">\n\n<p>其实在实现过程中，每个点的2k个分类特征与 4k回归特征，与 k个anchor逐个对应即可，这实际是一种“人为设置的逻辑映射”。当然，也可以不这样设置，但是无论如何都需要保证在训练和测试过程中映射方式必须一致。</p>\n<h3 id=\"Loss\"><a href=\"#Loss\" class=\"headerlink\" title=\"Loss\"></a>Loss</h3><img src=\"/images/rcnn/loss.jpg\" width=\"75%\" height=\"75%\">\n\n<p>上述公式中 i 表示anchors index， $p_i$表示positive softmax probability，$p_i^*$代表对应的GT predict概率（即当第i个anchor与GT间IoU&gt;0.7，认为是该anchor是positive，$p_i^*$=1<br>  ；反之IoU&lt;0.3时，认为是该anchor是negative，$p_i^*$=0<br>  ；<u>**至于那些0.3&lt;IoU&lt;0.7的anchor则不参与训练**</u>）；t<br>  代表predict bounding box，<img src=\"evernotecid://B26882A6-9AE9-467C-A70F-24D1465FC8A1/appyinxiangcom/4753150/ENResource/p4449\" alt=\"9e87c56a005dcec5fb5e7c215cb880a9.svg+xml\"><br>  代表对应positive anchor对应的GT box。可以看到，整个Loss分为2部分：</p>\n <img src=\"/images/rcnn/loss2.jpg\" width=\"75%\" height=\"75%\">\n \n<p> 在训练Faster RCNN的时候有四个损失：</p>\n<ul>\n<li>RPN 分类损失：anchor是否为前景（二分类）</li>\n<li>RPN位置回归损失：anchor位置微调</li>\n<li>RoI 分类损失：RoI所属类别（21分类，多了一个类作为背景）</li>\n<li>RoI位置回归损失：继续对RoI位置微调</li>\n</ul>\n<p>四个损失相加作为最后的损失，反向传播，更新参数。</p>\n<h3 id=\"注意\"><a href=\"#注意\" class=\"headerlink\" title=\"注意\"></a>注意</h3><h4 id=\"1\"><a href=\"#1\" class=\"headerlink\" title=\"1\"></a>1</h4><ul>\n<li>在RPN的时候，已经对anchor做了一遍NMS，在RCNN测试的时候，还要再做一遍</li>\n<li>在RPN的时候，已经对anchor的位置做了回归调整，在RCNN阶段还要对RoI再做一遍</li>\n<li>在RPN阶段分类是二分类，而Fast RCNN阶段是21分类(voc)</li>\n</ul>\n<h4 id=\"2\"><a href=\"#2\" class=\"headerlink\" title=\"2\"></a>2</h4><p><u><strong>RPN会产生大约2000个RoIs，这2000个RoIs不是都拿去训练</strong></u>，而是选择128个RoIs用以训练。选择的规则如下：</p>\n<ol>\n<li>RoIs和gt_bboxes 的IoU大于0.5的，选择一些（比如32个）</li>\n<li>选择 RoIs和gt_bboxes的IoU小于等于0（或者0.1）的选择一些（比如 128-32=96个）作为负样本</li>\n</ol>\n<p><u><strong>为了便于训练，对选择出的128个RoIs，还对他们的gt_roi_loc 进行标准化处理（减去均值除以标准差）</strong></u></p>\n","feature":true,"text":" 参考来源：https://zhuanlan.zhihu.com/p/31426458https://zhuanlan.zhihu.com/p/86403390 Faster RCNN基本结构 Faster RCNN其实可以分为4个主要内容： 特征提取：Faster RCNN首先...","link":"","photos":[],"count_time":{"symbolsCount":"13k","symbolsTime":"11 mins."},"categories":[],"tags":[],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96\"><span class=\"toc-text\">特征提取</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#RPN\"><span class=\"toc-text\">RPN</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#anchors\"><span class=\"toc-text\">anchors</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#rpn%E4%B8%AD%E7%9A%84%E4%BA%8C%E5%88%86%E7%B1%BB\"><span class=\"toc-text\">rpn中的二分类</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#bounding-box-regression\"><span class=\"toc-text\">bounding box regression</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Proposal-Layer\"><span class=\"toc-text\">Proposal Layer</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#RPN%E7%94%9F%E6%88%90RoIs\"><span class=\"toc-text\">RPN生成RoIs</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#ROI-pooling\"><span class=\"toc-text\">ROI pooling</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#%E4%B8%BA%E4%BD%95%E9%9C%80%E8%A6%81RoI-Pooling\"><span class=\"toc-text\">为何需要RoI Pooling</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#RoI-Pooling-layer-forward\"><span class=\"toc-text\">RoI Pooling layer forward</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81pooling%E6%88%907%C3%977%E7%9A%84%E5%B0%BA%E5%BA%A6%EF%BC%9F\"><span class=\"toc-text\">为什么要pooling成7×7的尺度？</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Classification\"><span class=\"toc-text\">Classification</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Anchor%E5%88%B0%E5%BA%95%E4%B8%8E%E7%BD%91%E7%BB%9C%E8%BE%93%E5%87%BA%E5%A6%82%E4%BD%95%E5%AF%B9%E5%BA%94\"><span class=\"toc-text\">Anchor到底与网络输出如何对应</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Loss\"><span class=\"toc-text\">Loss</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%B3%A8%E6%84%8F\"><span class=\"toc-text\">注意</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#1\"><span class=\"toc-text\">1</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#2\"><span class=\"toc-text\">2</span></a></li></ol></li></ol>","author":{"name":"Hulk Wang","slug":"blog-author","avatar":"/images/avatar_small.jpg","link":"https://github.com/TalkUHulk","description":"I'm 浩克，CV算法工程师，热衷于各种有趣的技术，此博客主要用来做学习总结，杀死拖延症。<br /> <img src=\"/images/funny.gif\" height=\"240\" width=\"360\"/>","socials":{"github":"https://github.com/TalkUHulk","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/311127773","zhihu":"https://www.zhihu.com/people/MisterAntebellum","csdn":"https://blog.csdn.net/hyqwmxsh","juejin":"","customs":{}}},"mapped":true,"prev_post":{},"next_post":{"title":"FCOS学习笔记","uid":"7f399d762217f0410e81da356daa491f","slug":"FCOS学习笔记","date":"2021-09-13T08:19:35.000Z","updated":"2021-09-13T10:18:08.218Z","comments":true,"path":"api/articles/FCOS学习笔记.json","keywords":null,"cover":[],"text":"FCOS学习笔记 笔记来源：https://blog.csdn.net/WZZ18191171661/article/details/89258086https://zhuanlan.zhihu.com/p/339023466 FCOS是一个基于FCN的per-pixel、anc...","link":"","photos":[],"count_time":{"symbolsCount":"1.6k","symbolsTime":"1 mins."},"categories":[],"tags":[{"name":"detection","slug":"detection","count":4,"path":"api/tags/detection.json"}],"author":{"name":"Hulk Wang","slug":"blog-author","avatar":"/images/avatar_small.jpg","link":"https://github.com/TalkUHulk","description":"I'm 浩克，CV算法工程师，热衷于各种有趣的技术，此博客主要用来做学习总结，杀死拖延症。<br /> <img src=\"/images/funny.gif\" height=\"240\" width=\"360\"/>","socials":{"github":"https://github.com/TalkUHulk","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/311127773","zhihu":"https://www.zhihu.com/people/MisterAntebellum","csdn":"https://blog.csdn.net/hyqwmxsh","juejin":"","customs":{}}},"feature":true}}