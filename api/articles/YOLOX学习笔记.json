{"title":"YOLOX学习笔记","uid":"92efdb393a2736ad47dbfd207d7d915f","slug":"YOLOX学习笔记","date":"2022-02-08T06:21:21.000Z","updated":"2022-02-09T10:32:51.413Z","comments":true,"path":"api/articles/YOLOX学习笔记.json","keywords":null,"cover":"https://github.com/TalkUHulk/yolov3-TT100k/blob/main/results/result.png?raw=true","content":"<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>知识点来源于论文和网络，仅记录学习 来源:<a href=\"https://zhuanlan.zhihu.com/p/397993315\">深入浅出Yolo系列之Yolox核心基础完整讲解</a></p></blockquote>\n<h3 id=\"yolox-darknet53\">Yolox-Darknet53</h3>\n<ul>\n<li><p>输入端：Strong augmentation数据增强</p></li>\n<li><p>BackBone主干网络：主干网络没有什么变化，还是Darknet53。</p></li>\n<li><p>Neck：没有什么变化，Yolov3 baseline的Neck层还是FPN结构。</p></li>\n<li><p>Prediction：Decoupled Head、End-to-End YOLO、Anchor-free、Multi positives。</p></li>\n</ul>\n<h4 id=\"输入端strong-augmentation\">输入端：Strong augmentation</h4>\n<p>在网络的输入端，Yolox主要采用了Mosaic、Mixup两种数据增强方法。</p>\n<p>而采用了这两种数据增强，直接将Yolov3 baseline，提升了2.4个百分点。</p>\n<ol type=\"1\">\n<li><p>Mosaic数据增强</p></li>\n<li><p>MixUp数据增强</p></li>\n</ol>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>不过有两点需要注意： 1. 在训练的最后15个epoch，这两个数据增强会被关闭掉。而在此之前，Mosaic和Mixup数据增强，都是打开的，这个细节需要注意。<u>这是考虑到在马赛克增强的过程中，由于存在缩放和剪裁的操作，会留下一些质量很差的样本，最后的15epoch就是为了缓解这一问题。经过笔者的实测，确实能够提升性能，不过小目标的性能会略微有所损失。<u> 2. 由于采取了更强的数据增强方式，作者在研究中发现，ImageNet预训练将毫无意义，因此，所有的模型，均是从头开始训练的。</u></u></p></blockquote>\n<h4 id=\"backbone\">BackBone</h4>\n<p>Yolox-Darknet53的Backbone主干网络，和原本的Yolov3 baseline的主干网络都是一样的,都是采用Darknet53的网络结构.</p>\n<h4 id=\"neck\">Neck</h4>\n<p>在Neck结构中，Yolox-Darknet53和Yolov3 baseline的Neck结构，也是一样的，都是采用FPN的结构进行融合。<u>而在Yolov4、Yolov5、甚至后面讲到的Yolox-s、l等版本中，都是采用FPN+PAN的形式。<u></u></u></p>\n<h4 id=\"prediction\">Prediction</h4>\n<h5 id=\"decoupled-head\">Decoupled Head</h5>\n<p>Decoupled Head，目前在很多一阶段网络中都有类似应用，比如RetinaNet、FCOS等。而在Yolox中，作者增加了三个Decoupled Head，俗称“解耦头”</p>\n<p><img src=\"/images/yolox/prediction.png\" width=\"60%\" height=\"60%\" align=\"center\"></p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>为什么使用Decoupled Head？ 目前Yolo系列使用的检测头，表达能力可能有所欠缺，没有Decoupled Head的表达能力更好。</p></blockquote>\n<p>但是需要注意的是：将检测头解耦，会增加运算的复杂度。</p>\n<p>因此作者经过速度和性能上的权衡，最终使用 1个1x1 的卷积先进行降维，并在后面两个分支里，各使用了 2个3x3 卷积，最终调整到仅仅增加一点点的网络参数。</p>\n<p>而且这里解耦后，还有一个更深层次的重要性：</p>\n<p>Yolox的网络架构，可以和很多算法任务，进行一体化结合。</p>\n<p>比如：</p>\n<p>（1）YOLOX + Yolact/CondInst/SOLO ，实现端侧的实例分割。</p>\n<p>（2）YOLOX + 34 层输出，实现端侧人体的 17 个关键点检测。</p>\n<h6 id=\"decoupled-head的细节\">Decoupled Head的细节</h6>\n<p>将Yolox-Darknet53中，Decoupled Head①提取出来，经过前面的Neck层，这里Decouple Head①输入的长宽为20*20。</p>\n<p><img src=\"/images/yolox/decoupled_head_2.jpg\" width=\"60%\" height=\"60%\" align=\"center\"></p>\n<p>从图上可以看出，Concat前总共有三个分支：</p>\n<p>（1）cls_output：主要对目标框的类别，预测分数。因为COCO数据集总共有80个类别，且主要是N个二分类判断，因此经过Sigmoid激活函数处理后，变为20 * 20 * 80大小。</p>\n<p>（2）obj_output：主要判断目标框是前景还是背景，因此经过Sigmoid处理好，变为20 * 20 * 1大小。</p>\n<p>（3）reg_output：主要对目标框的坐标信息（x，y，w，h）进行预测，因此大小为20 * 20 * 4。</p>\n<p>最后三个output，经过Concat融合到一起，得到20 * 20 * 85的特征信息。</p>\n<p>当然，这只是Decoupled Head①的信息，再对Decoupled Head②和③进行处理。</p>\n<p>Decoupled Head②输出特征信息，并进行Concate，得到40 * 40 * 85特征信息。</p>\n<p>Decoupled Head③输出特征信息，并进行Concate，得到80 * 80 * 85特征信息。</p>\n<p>再对①②③三个信息，进行Reshape操作，并进行总体的Concat，得到8400*85的预测信息。</p>\n<p>并经过一次Transpose，变为85*8400大小的二维向量信息。</p>\n<p>这里的8400，指的是预测框的数量，而85是每个预测框的信息（reg，obj，cls）。</p>\n<h5 id=\"anchor-free\">Anchor Free</h5>\n<p>有了预测框的信息，下面我们再了解，如何将这些预测框和标注的框，即groundtruth进行关联，从而计算Loss函数，更新网络参数呢？</p>\n<p>这里就要引入Anchor的内容，目前行业内，主要有Anchor Based和Anchor Free两种方式。</p>\n<p>在Yolov3、Yolov4、Yolov5中，通常都是采用Anchor Based的方式，来提取目标框，进而和标注的groundtruth进行比对，判断两者的差距。</p>\n<ol type=\"1\">\n<li>Anchor Based方式</li>\n</ol>\n<p>比如输入图像，经过Backbone、Neck层，最终将特征信息，传送到输出的Feature Map中。</p>\n<p>这时，就要设置一些Anchor规则，将预测框和标注框进行关联。</p>\n<p>从而在训练中，计算两者的差距，即损失函数，再更新网络参数。</p>\n<p>比如在下图的，最后的三个Feature Map上，基于每个单元格，都有三个不同尺寸大小的锚框。</p>\n<p><img src=\"/images/yolox/anchor_based.jpg\" width=\"60%\" height=\"60%\" align=\"center\"></p>\n<p>如yolo-v3中，当输入为416 * 416时，网络最后的三个特征图大小为13 * 13，26 * 26，52 * 52。</p>\n<p><img src=\"/images/yolo-v3/proposal.jpg\" width=\"60%\" height=\"60%\" align=\"center\"></p>\n<p>我们可以看到，黄色框为小狗的Groundtruth，即标注框。</p>\n<p>而蓝色的框，为小狗中心点所在的单元格，所对应的锚框，每个单元格都有3个蓝框。</p>\n<p>当采用COCO数据集，即有80个类别时。</p>\n<p>基于每个锚框，都有x、y、w、h、obj（前景背景）、class（80个类别），共85个参数。</p>\n<p>因此会产生3<em>(13 </em> 13+26 * 26+52 * 52）* 85=904995个预测结果。</p>\n<p>如果将输入从416 * 416，变为640 * 640，最后的三个特征图大小为20 * 20,40 * 40,80*80。</p>\n<p>则会产生3<em>（20 </em> 20+40 * 40+80 * 80）* 85=2142000个预测结果。</p>\n<ol start=\"2\" type=\"1\">\n<li>Anchor Free方式</li>\n</ol>\n<p>而Yolox-Darknet53中，则采用Anchor Free的方式。</p>\n<p>我们从两个方面，来对Anchor Free进行了解。</p>\n<p>a.输出的参数量</p>\n<p>我们先计算下，当得到包含目标框所有输出信息时，所需要的参数量？</p>\n<p>这里需要注意的是：</p>\n<p>最后黄色的85 * 8400，不是类似于Yolov3中的Feature Map，而是特征向量。</p>\n<p><img src=\"/images/yolox/anchor_free.jpg\" width=\"60%\" height=\"60%\" align=\"center\"></p>\n<p>从图中可知，当输入为640 * 640时，最终输出得到的特征向量是85 * 8400。</p>\n<p>我们看下，和之前Anchor Based方式，预测结果数量相差多少?</p>\n<p>通过计算，8400 * 85=714000个预测结果，比基于Anchor Based的方式，少了2/3的参数量。</p>\n<p>b.<u><strong>Anchor框信息</strong><u></u></u></p>\n<p>在前面Anchor Based中，我们知道，每个Feature map的单元格，都有3个大小不一的锚框。</p>\n<p>那么Yolox-Darknet53就没有吗？</p>\n<p>其实并不然，这里只是巧妙的，将前面Backbone中，下采样的大小信息引入进来。</p>\n<p><img src=\"/images/yolox/anchor_free_2.jpg\" width=\"60%\" height=\"60%\" align=\"center\"></p>\n<p>比如上图中，最上面的分支，下采样了5次，2的5次方为32。</p>\n<p>并且Decoupled Head①的输出，为20 * 20 * 85大小。</p>\n<p><img src=\"/images/yolox/anchor_free_3.jpg\" width=\"60%\" height=\"60%\" align=\"center\"></p>\n<p>因此如上图所示：</p>\n<p><u>最后8400个预测框中，其中有400个框，所对应锚框的大小，为32 * 32。</u></p>\n<p>同样的原理，中间的分支，最后有1600个预测框，所对应锚框的大小，为16 * 16。</p>\n<p>最下面的分支，最后有6400个预测框，所对应锚框的大小，为8 * 8。<u></u></p>\n<p>当有了8400个预测框的信息，每张图片也有标注的目标框的信息。</p>\n<p>这时的锚框，就相当于桥梁。</p>\n<p>这时需要做的，就是将8400个锚框，和图片上所有的目标框进行关联，挑选出正样本锚框。</p>\n<p>而相应的，正样本锚框所对应的位置，就可以将正样本预测框，挑选出来。</p>\n<p>这里采用的关联方式，就是标签分配。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>所以是有anchor的，只是很巧妙的用了下采样次数来转换。</p></blockquote>\n<h5 id=\"标签分配\">标签分配</h5>\n<p>当有了8400个Anchor锚框后，这里的每一个锚框，都对应85*8400特征向量中的预测框信息。</p>\n<p>不过需要知道，这些预测框只有少部分是正样本，绝大多数是负样本。</p>\n<p>那么到底哪些是正样本呢？</p>\n<p>这里需要利用锚框和实际目标框的关系，挑选出一部分适合的正样本锚框。</p>\n<p>比如第3、10、15个锚框是正样本锚框，则对应到网络输出的8400个预测框中，第3、10、15个预测框，就是相应的正样本预测框。</p>\n<p>训练过程中，在锚框的基础上，不断的预测，然后不断的迭代，从而更新网络参数，让网络预测的越来越准。</p>\n<p>那么在Yolox中，是如何挑选正样本锚框的呢？</p>\n<p>这里就涉及到两个关键点：<strong>初步筛选、SimOTA</strong>。</p>\n<ol type=\"1\">\n<li>初步筛选</li>\n</ol>\n<p>初步筛选的方式主要有两种：根据中心点来判断、根据目标框来判断； 这部分的代码，在models/yolo_head.py的get_in_boxes_info函数中。</p>\n<ol type=\"a\">\n<li>根据中心点来判断： 规则：寻找anchor_box中心点，落在groundtruth_boxes矩形范围的所有anchors。</li>\n</ol>\n<p><img src=\"/images/yolox/cbsx_a_2.jpg\" width=\"60%\" height=\"60%\" align=\"center\"></p>\n<p>如上，寻找中心点位于groundtruth中的anchor，此时中心点到groundtruth四条边的距离都&gt;0</p>\n<p>b.根据目标框来判断： 规则：以groundtruth中心点为基准，设置边长为5的正方形，挑选在正方形内的所有锚框。 <img src=\"/images/yolox/cbsx_b_2.jpg\" width=\"60%\" height=\"60%\" align=\"center\"></p>\n<p>如上，以groundtruth中点确定边长为5的正方形，然后寻找中心点位于正方形中的anchor， 此时anchor中心点到正方形四条边的距离都&gt;0</p>\n<p>经过上面两种挑选的方式，就完成初步筛选了，挑选出一部分候选的anchor，进入下一步的精细化筛选。</p>\n<ol start=\"2\" type=\"1\">\n<li>精细化筛选</li>\n</ol>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>而在精细化筛选中，就用到论文中提到的SimOTA了,SimOTA后，AP值提升了2.3个百分点，还是非常有效的。 <a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2103.14259\">《Ota: Optimal transport assignment for object detection》。</a></p></blockquote>\n<p>整个精细化筛选流程：</p>\n<ol type=\"1\">\n<li><p>初筛正样本信息提取</p></li>\n<li><p>Loss函数计算</p></li>\n<li><p>cost成本计算</p></li>\n<li><p>SimOTA求解</p></li>\n</ol>\n<p>为了便于理解，我们假定图片上有3个目标框，即3个groundtruth。</p>\n<p>再假定目前在做的项目是对人脸和人体检测，因此检测类别是2。</p>\n<p>上一节中，我们知道有8400个锚框，但是经过初步筛选后，假定有1000个锚框是正样本锚框。</p>\n<ol type=\"1\">\n<li><strong>初筛正样本信息提取</strong></li>\n</ol>\n<p>初筛出的1000个正样本锚框的位置，我们是知道的。</p>\n<p>而所有锚框的位置，和网络最后输出的85 * 8400特征向量是一一对应。</p>\n<p>所以根据位置，可以将网络预测的候选检测框位置bboxes_preds、前景背景目标分数obj_preds、类别分数cls_preds等信息，提取出来。（代码位于yolo_head.py的get_assignments函数中）</p>\n<p>以前面的假定信息为例，代码图片中的bboxes_preds_per_image因为是候选检测框的信息，因此维度为[1000，4]。</p>\n<p>obj_preds因为是目标分数，所以维度是[1000，1]。</p>\n<p>cls_preds因为是类别分数，所以维度是[1000，2]。</p>\n<ol start=\"2\" type=\"1\">\n<li><strong>Loss函数计算</strong></li>\n</ol>\n<p>针对筛选出的1000个候选检测框，和3个groundtruth计算Loss函数。（代码:yolo_head.py的get_assignments）</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>主要是iou和cls</p></blockquote>\n<ul>\n<li><p>pair_wise_ious_loss:计算出3个目标框，和1000个候选框，每个框相互之间的iou信息pair_wise_ious，因为向量维度为[3,1000]</p></li>\n<li><p>pair_wise_cls_loss: 将类别的条件概率(前景背景)和目标的先验概率(80类)做乘积，得到目标的类别分数, 再通过F.binary_cross_entroy的处理，得到3个目标框和1000个候选框的综合loss值，即pair_wise_cls_loss，向量维度为[3，1000]。</p></li>\n</ul>\n<ol start=\"3\" type=\"1\">\n<li><strong>cost成本计算</strong></li>\n</ol>\n<p>有了reg_loss和cls_loss，就可以将两个损失函数加权相加，计算cost成本函数了。 (yolo_head.py的get_assignments函数中)</p>\n<ol start=\"4\" type=\"1\">\n<li><strong><u>SimOTA<u></u></u></strong></li>\n</ol>\n<p>有了上面的一系列信息，标签分配问题，就转换为了标准的OTA问题。</p>\n<p>但是经典的Sinkhorn-Knopp算法，需要多次迭代求得最优解。</p>\n<p>作者也提到，该算法会导致25%额外训练时间，所以采用一种<u>简化版的SimOTA方法<u>，求解近似最优解。这里对应的函数，是get_assignments函数中的self.dynamic_k_matching：</u></u></p>\n<p>其中的流程如下：</p>\n<p><strong>step1.设置候选框数量</strong></p>\n<p>首先按照cost值的大小，新建一个全0变量matching_matrix，这里是[3,1000]。 原始设置候选框数量为10。然后从前面的pair_wise_ious中，给每个目标框，挑选10个iou最大的候选框。 因为前面假定有3个目标，因此这里topk_ious的维度为[3，10]。</p>\n<p><strong>step2.通过cost挑选候选框</strong></p>\n<p>再通过topk_ious的信息，动态选择候选框，<strong>这里是个关键</strong>，代码:dynamic_k_matching</p>\n<p><strong>实例</strong> 这里的topk_ious，是3个目标框和预测框中，最大iou的10个候选框:</p>\n<p><img src=\"/images/yolox/simOTA_3.jpg\" width=\"60%\" height=\"60%\" align=\"center\"></p>\n<p>经过torch.clamp函数，得到最终右面的dynamic_ks值。</p>\n<p>我们就知道，目标框1和3，给他分配3个候选框，而目标框2，给它分配4个候选框。</p>\n<p><strong>那么基于什么标准分配呢？</strong></p>\n<p>这时就要利用前面计算的cost值，即[3,1000]的损失函数加权信息。</p>\n<p>在for循环中，<u><strong>针对每个目标框挑选，相应的cost值最低的一些候选框。</strong><u></u></u></p>\n<p><img src=\"/images/yolox/simOTA_4.jpg\" width=\"60%\" height=\"60%\" align=\"center\"></p>\n<p>比如右面的matching_matrix中，cost值最低的一些位置，数值为1，其余位置都为0。</p>\n<p>因为目标框1和3，dynamic_ks值都为3，因此matching_matrix的第一行和第三行，有3个1。</p>\n<p>而目标框2，dynamic_ks值为4，因此matching_matrix的第二行，有4个1。</p>\n<p><strong>step3.过滤共用的候选框</strong></p>\n<p>不过在分析matching_matrix时，我们发现，第5列有两个1。</p>\n<p>这也就说明，第五列所对应的候选框，被目标检测框1和2，都进行关联。</p>\n<p>因此对这两个位置，还要使用cost值进行对比，选择较小的值，再进一步筛选。</p>\n<p><img src=\"/images/yolox/simOTA_5.jpg\" width=\"60%\" height=\"60%\" align=\"center\"></p>\n<p>这里为了便于理解，还是采用图示的方式：</p>\n<p>首先第一行代码，将matching_matrix，对每一列进行相加。</p>\n<p><img src=\"/images/yolox/simOTA_6.jpg\" width=\"60%\" height=\"60%\" align=\"center\"></p>\n<p>这时anchor_matching_gt中，只要有大于1的，说明有共用的情况。</p>\n<p>上图案例中，表明第5列存在共用的情况。</p>\n<p>再利用第三行代码，将cost中，第5列的值取出，并进行比较，计算最小值所对应的行数，以及分数。</p>\n<p>我们将第5列两个位置，假设为0.4和0.3。</p>\n<p><img src=\"/images/yolox/simOTA_7.jpg\" width=\"60%\" height=\"60%\" align=\"center\"></p>\n<p>经过第三行代码，可以找到最小的值是0.3，即cost_min为0.3，所对应的行数，cost_argmin为2。</p>\n<p><img src=\"/images/yolox/simOTA_8.jpg\" width=\"60%\" height=\"60%\" align=\"center\"></p>\n<p>经过第四行代码，将matching_matrix第5列都置0。</p>\n<p>再利用第五行代码，将matching_matrix第2行，第5列的位置变为1。</p>\n<p>最终我们可以得到3个目标框，最合适的一些候选框，即matching_matrix中，所有1所对应的位置。</p>\n<h5 id=\"loss\">Loss</h5>\n<p>经过第三部分的标签分配，就可以将目标框和正样本预测框对应起来了。</p>\n<p>下面就可以计算两者的误差，即Loss函数。</p>\n<p>计算的代码，位于yolo_head.py的get_losses函数中。</p>\n<p><img src=\"/images/yolox/loss.jpg\" width=\"60%\" height=\"60%\" align=\"center\"></p>\n<p>我们可以看到：</p>\n<p>检测框位置的iou_loss，Yolox中使用传统的iou_loss，和giou_loss两种，可以进行选择。</p>\n<p>而obj_loss和cls_loss，都是采用BCE_loss的方式。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>在前面精细化筛选中，使用了reg_loss和cls_loss，筛选出和目标框所对应的预测框。因此这里的iou_loss和cls_loss，只针对目标框和筛选出的正样本预测框进行计算。而obj_loss，则还是针对8400个预测框。</p></blockquote>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>在Decoupled Head中，cls_output和obj_output使用了sigmoid函数进行归一化，但是在训练时，并没有使用sigmoid函数，原因是训练时用的nn.BCEWithLogitsLoss函数，已经包含了sigmoid操作。而在推理过程中，是使用Sigmoid函数的.</p></blockquote>\n<h3 id=\"yolox-slmx系列\">Yolox-s、l、m、x系列</h3>\n<p>在对Yolov3 baseline进行不断优化，获得不错效果的基础上。</p>\n<p>作者又对Yolov5系列，比如Yolov5s、Yolov5m、Yolov5l、Yolov5x四个网络结构，也使用一系列trick进行改进。</p>\n<p>主要对Yolov5s进行对比，下图是Yolov5s的网络结构图：</p>\n<p><img src=\"/images/yolo_v5/arch.jpg\" width=\"60%\" height=\"60%\" align=\"center\"></p>\n<p><img src=\"/images/yolox/yolox_s.jpg\" width=\"60%\" height=\"60%\" align=\"center\"></p>\n<p>由上面两张图的对比，及前面的内容可以看出，Yolov5s和Yolox-s主要区别在于：</p>\n<p>（1）输入端：在Mosa数据增强的基础上，增加了Mixup数据增强效果；</p>\n<p>（2）Backbone：激活函数采用SiLU函数；</p>\n<p>（3）Neck：激活函数采用SiLU函数；</p>\n<p>（4）输出端：检测头改为Decoupled Head、采用anchor free、multi positives、SimOTA的方式。</p>\n<p>在前面Yolov3 baseline的基础上，以上的tricks，取得了很不错的涨点。</p>\n<h3 id=\"轻量级网络\">轻量级网络</h3>\n<p>针对Yolov4-Tiny，构建了Yolox-Tiny网络结构。</p>\n<p>针对FCOS 风格的NanoDet，构建了Yolox-Nano网络结构</p>\n<h3 id=\"数据增强的优缺点\">数据增强的优缺点</h3>\n<p>在Yolox的很多对比测试中，都使用了数据增强的方式。</p>\n<p>但是不同的网络结构，有的深有的浅，网络的学习能力不同，那么无节制的数据增强是否真的更好呢？</p>\n<p>作者团队，对这个问题也进行了对比测试。</p>\n<p>① Mosaic和Mixup混合策略</p>\n<p>（1）对于轻量级网络，Yolox-nano来说，当在Mosaic基础上，增加了Mixup数据增强的方式，AP值不增反而降，从25.3降到24。</p>\n<p>（2）而对于深一些的网络，Yolox-L来说，在Mosaic基础上，增加了Mixup数据增强的方式，AP值反而有所上升，从48.6增加到49.5。</p>\n<p>（3）因此不同的网络结构，采用数据增强的策略也不同，比如Yolox-s、Yolox-m，或者Yolov4、Yolov5系列，都可以使用不同的数据增强策略进行尝试。</p>\n<p>② Scale 增强策略</p>\n<p>在Mosaic数据增强中，代码Yolox/data/data_augment.py中的random_perspective函数，生成仿射变换矩阵时，对于图片的缩放系数，会生成一个随机值。</p>\n<p>（1）对于Yolox-l来说，随机范围scale设置在[0.1，2]之间，即文章中设置的默认参数。</p>\n<p>（2）而当使用轻量级模型，比如YoloNano时，一方面只使用Mosaic数据增强，另一方面随机范围scale，设置在[0.5，1.5]之间，弱化Mosaic增广的性能。</p>\n","feature":true,"text":" 知识点来源于论文和网络，仅记录学习 来源:深入浅出Yolo系列之Yolox核心基础完整讲解 Yolox-Darknet53 输入端：Strong augmentation数据增强 BackBone主干网络：主干网络没有什么变化，还是Darknet53。 Neck：没有什么变化，...","link":"","photos":[],"count_time":{"symbolsCount":"8.1k","symbolsTime":"7 mins."},"categories":[],"tags":[{"name":"detection","slug":"detection","count":7,"path":"api/tags/detection.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#yolox-darknet53\"><span class=\"toc-text\">Yolox-Darknet53</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%BE%93%E5%85%A5%E7%AB%AFstrong-augmentation\"><span class=\"toc-text\">输入端：Strong augmentation</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#backbone\"><span class=\"toc-text\">BackBone</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#neck\"><span class=\"toc-text\">Neck</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#prediction\"><span class=\"toc-text\">Prediction</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#decoupled-head\"><span class=\"toc-text\">Decoupled Head</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-6\"><a class=\"toc-link\" href=\"#decoupled-head%E7%9A%84%E7%BB%86%E8%8A%82\"><span class=\"toc-text\">Decoupled Head的细节</span></a></li></ol></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#anchor-free\"><span class=\"toc-text\">Anchor Free</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#%E6%A0%87%E7%AD%BE%E5%88%86%E9%85%8D\"><span class=\"toc-text\">标签分配</span></a></li><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#loss\"><span class=\"toc-text\">Loss</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#yolox-slmx%E7%B3%BB%E5%88%97\"><span class=\"toc-text\">Yolox-s、l、m、x系列</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">轻量级网络</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9\"><span class=\"toc-text\">数据增强的优缺点</span></a></li></ol>","author":{"name":"Hulk Wang","slug":"blog-author","avatar":"/images/avatar_small.jpg","link":"https://github.com/TalkUHulk","description":"学习、记录、总结<br />kill拖延症<br /> 不生产知识，只是知识的搬运工<br /> <img src=\"/images/funny.gif\" height=\"240\" width=\"360\"/>","socials":{"github":"https://github.com/TalkUHulk","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/311127773","zhihu":"https://www.zhihu.com/people/MisterAntebellum","csdn":"https://blog.csdn.net/hyqwmxsh","juejin":"","customs":{}}},"mapped":true,"prev_post":{},"next_post":{"title":"妆容迁移总结","uid":"40ac598c9729991661764e8c3d733bc9","slug":"妆容迁移总结","date":"2021-11-02T15:22:22.000Z","updated":"2021-11-03T10:54:35.106Z","comments":true,"path":"api/articles/妆容迁移总结.json","keywords":null,"cover":"/images/beauty/self_result.jpg","text":" 先上一个自己简单优化的效果,对非正脸/五官有遮挡的图效果待优化,在真实图片上测试，比几个开源模型效果好 BeautyGan BeautyGAN: Instance-level Facial Makeup Transfer with Deep Generative Adversa...","link":"","photos":[],"count_time":{"symbolsCount":"4.2k","symbolsTime":"4 mins."},"categories":[],"tags":[{"name":"work summary","slug":"work-summary","count":5,"path":"api/tags/work-summary.json"}],"author":{"name":"Hulk Wang","slug":"blog-author","avatar":"/images/avatar_small.jpg","link":"https://github.com/TalkUHulk","description":"学习、记录、总结<br />kill拖延症<br /> 不生产知识，只是知识的搬运工<br /> <img src=\"/images/funny.gif\" height=\"240\" width=\"360\"/>","socials":{"github":"https://github.com/TalkUHulk","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/311127773","zhihu":"https://www.zhihu.com/people/MisterAntebellum","csdn":"https://blog.csdn.net/hyqwmxsh","juejin":"","customs":{}}},"feature":true}}